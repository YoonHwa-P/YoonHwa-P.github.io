{"pages":[],"posts":[{"title":"naver-site-verification","text":"","link":"/2021/12/06/navercf4e400c34a14efb1af882dbcc185020/"},{"title":"R for DS_01 welcome &amp; Introduction","text":"Welcome 저작권 : “R for DataScience by Hadley Wickham and Garrett Grolemund(O’Reilly). Copyright 2017 Garrett Grolemund, Hadley Wickham, 978-1-491-91039-9 Introduction Data science is an exciting discipline that allows you to turn raw data into understanding, insight, and knowledge. The goal of “R for Data Science” is to help you learn the most important tools in R that will allow you to do data science. After reading this book, you’ll have the tools to tackle a wide variety of data science challenges, using the best parts of R. R을 이용한 data science를 해 봅시다. ggplot2를 더 잘 쓰고 싶다면A Layered Grammar of Graphics을 읽어보세요. 1.1 What you will learn 1.2 How this book is organised1.3 What you won’t learn1.4 Prerequisites1.4.1 R https://cloud.r-project.org,1.4.2 RStudio http://www.rstudio.com/download. https://cran.r-project.org/bin/windows/Rtools/rtools40.html 위에 Rtools(64 bit) 실행 설치 해야 할 programs : 확인 1.4.3 The tidyverse1install.packages(&quot;tidyverse&quot;) https://cloud.r-project.org/ 123456789library(tidyverse)#&gt; ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──#&gt; ✔ ggplot2 3.3.2 ✔ purrr 0.3.4#&gt; ✔ tibble 3.0.3 ✔ dplyr 1.0.2#&gt; ✔ tidyr 1.1.2 ✔ stringr 1.4.0#&gt; ✔ readr 1.4.0 ✔ forcats 0.5.0#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──#&gt; ✖ dplyr::filter() masks stats::filter()#&gt; ✖ dplyr::lag() masks stats::lag() tidyverse : ggplot2, tibble, tidyr, readr, purrr, and dplyr packages1.4.4 Other packages1install.packages(c(&quot;nycflights13&quot;, &quot;gapminder&quot;, &quot;Lahman&quot;)) 1.5 Running R code1.6 Getting help and learning more1.7 Acknowledgements1.8 ColophonMy_rpubs ref. introduction Part 3 more","link":"/2021/11/29/R/R_R4ds_Welcom/"},{"title":"R for DS_03 ggplot2","text":"Welcome 저작권 : “R for DataScience by Hadley Wickham and Garrett Grolemund(O’Reilly). Copyright 2017 Garrett Grolemund, Hadley Wickham, 978-1-491-91039-9 Introduction how to visualise your data using ggplot2. R ggplot2는 그래프를 그려주는 프로그램ggplot2 이론배경 3.1.1 Prerequisites 12345678910111213install.packages(&quot;tidyverse&quot;)library(tidyverse)#&gt; ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──#&gt; ✔ ggplot2 3.3.2 ✔ purrr 0.3.4#&gt; ✔ tibble 3.0.3 ✔ dplyr 1.0.2#&gt; ✔ tidyr 1.1.2 ✔ stringr 1.4.0#&gt; ✔ readr 1.4.0 ✔ forcats 0.5.0#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──#&gt; ✖ dplyr::filter() masks stats::filter()#&gt; ✖ dplyr::lag() masks stats::lag()install.packages(c(&quot;nycflights13&quot;, &quot;gapminder&quot;, &quot;Lahman&quot;)) 3.2 First steps 3.2.1 The mpg data frame US Environmental Protection Agency on 38 models of car A data frame is a rectangular 1mpg displ = car’s engine size, in litres hwy = fuel efficiency in miles per gallon (mpg) 3.2.2 Creating a ggplot12ggplot(data = mpg) + geom_point(mapping = aes(x= displ, y = hwy)) ggplot(data = mpg) : 비어있는 Graph를 만들어 준다. geom_point() : Layers 추가 scatterplot mapping = aes(x= displ, y = hwy) : x와 y를 mapping 해 준다. 3.2.3 A graphing templateggplot(data = &lt;DATA&gt;) + &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;)) 이런 식으로 쓰면 된다고 함. (모형) 3.3 Aesthetic mappings aesthetic : 래전드 모양, 색 크기 value : data level : aesthetic properties size : 크기 color = colour, aesthetic의 색 alpha = shape , aesthetic의 모양 12ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, color = class)) 12ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, size = class)) colour , color : 모두 써도 됨. 12ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y= hwy, alpha = class)) 12ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, shape = class)) (수동으로 색 설정) 래전드를 생성 하지 않으면서 color만 바꿀 수 있다. 12ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy), color = &quot;blue&quot;) 12ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, color = &quot;blue&quot;)) mpg 내의 data가 color이라는 column이 있다. 그 data가 “blue”인 data들의 displ과 hwy의 Graph 아직 덜 했다 ! rmarkdown_Book R publishingRggplot","link":"/2021/11/29/R/R%20for%20Ds_03%20ggplot2/"},{"title":"Text Mining in Python","text":"data 불러오기12345678910111213141516171819202122232425# ---- 데이터 불러오기 ----library(ggplot2) # 시각화 코드# install.packages(&quot;dplyr&quot;)# install.packages(&quot;tidyr&quot;)library(dplyr) # 데이터 가공library(reshape) # 데이터 가공 &lt;-- tidyrlibrary(readr) # 파일 입출력raw_reviews = read_csv(&quot;data/Womens Clothing E-Commerce Reviews.csv&quot;) %&gt;% select(-1)# raw_reviews &lt;- raw_reviews %&gt;% select(-1)glimpse(raw_reviews)colnames(raw_reviews) &lt;- c(&quot;ID&quot;, &quot;Age&quot;, &quot;Title&quot;, &quot;Review&quot;, &quot;Rating&quot;, &quot;Recommend&quot;, &quot;Liked&quot;, &quot;Division&quot;, &quot;Dept&quot;, &quot;Class&quot;)glimpse(raw_reviews) # age 리뷰 작성한 고객의 연령# Title, Review Text 리뷰 제목, 내용# Rating: 고객이 부여한 평점# Recommend IND: 추천 여부# Positive Feedback Count: 좋아요 수치# Division, Dept, Class --&gt; 상품의 대분류 정보 data 전처리123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# ---- 데이터 전처리 ----# 결측치 확인colSums(is.na(raw_reviews))table(raw_reviews$Age)age_group = cut(as.numeric(raw_reviews$Age), breaks = seq(10, 100, by = 10), include.lowest = TRUE, right = FALSE, labels = paste0(seq(10, 90, by = 10), &quot;th&quot;))age_group[1:10]# 새로운 변수 추가raw_reviews$age_group = age_grouptable(raw_reviews$age_group)# 감성 사전 데이터셋 변환summary(raw_reviews$Liked)table(raw_reviews$Liked)# 층화추출? / 임의추출idx = sample(1:nrow(raw_reviews), nrow(raw_reviews) * 0.1, replace = FALSE)raw_reviews2 = raw_reviews[idx, ] raw_reviews2 %&gt;% mutate(pos_binary = ifelse(Liked &gt; 0, 1, 0)) %&gt;% # 이산형 변수로 변환 select(Liked, pos_binary) -&gt; pos_binary_dfpos_binary_df$pos_binary &lt;- as.factor(pos_binary_df$pos_binary)table(pos_binary_df$pos_binary) # 0 부정, 1 긍정# ---- 키워드 데이터셋 생성REVIEW_TEXT = as.character(raw_reviews2$Review)REVIEW_TEXT = tolower(raw_reviews2$Review)# 단어를 이어 붙인 후, 토큰화된 단어들로 문장 재구성library(tokenizers)TEXT_Token = c()for(i in 1:length(REVIEW_TEXT)) { token_words = unlist(tokenize_word_stems(REVIEW_TEXT[i])) Sentence = &quot;&quot; for (tw in token_words) { Sentence = paste(Sentence, tw) } TEXT_Token[i] = Sentence } Text 전처리1234567891011121314151617181920212223242526272829303132333435# ---- 텍스트 전처리library(tm)Corpus_token = Corpus(VectorSource(TEXT_Token))Corpus_tm_token = tm_map(Corpus_token, removePunctuation)Corpus_tm_token = tm_map(Corpus_token, removeNumbers)Corpus_tm_token = tm_map(Corpus_token, removeWords, c(stopwords(&quot;English&quot;)))#TDM과 DTM 의 차이 (TDM :term Document Matrix)# T=ODF . DTM = CountVectprozor(in Python)DTM_Token = DocumentTermMatrix(Corpus_tm_token)DTM_Matrix_Token = as.matrix(DTM_Token)# 상위 키워드 추출# quantile() 함수 활용top_1_pct = colSums(DTM_Matrix_Token) &gt; quantile(colSums(DTM_Matrix_Token), probs = 0.99)DTM_Matrix_Token_selected = DTM_Matrix_Token[, top_1_pct]ncol(DTM_Matrix_Token_selected)#ErrorDTM_df = as.data.frame(DTM_Matrix_Token_selected)DTM_dfpos_final_df = cbind(pos_binary_df, DTM_df)glimpse(pos_final_df)#희소행렬 문제가 나타나게 된다.ncol(pos_final_df) 훈련, 검증용 data 분류123456# ---- 훈련 검증용 데이터 분류 ----set.seed(1234)idx = sample(1:nrow(pos_final_df), nrow(pos_final_df) * 0.7, replace = FALSE)train = pos_final_df[idx, ]test = pos_final_df[-idx, ] Logistic Regression Model Develop1234567891011# --- 로지스틱 회귀 모형 개발 ---start_time = Sys.time()glm_model = step(glm(pos_binary ~ ., data = train[-1], family = binomial(link = &quot;logit&quot;)), direction = &quot;backward&quot;) # 후진소거법End_time = Sys.time()difftime(End_time, start_time, units = &quot;secs&quot;) Step: AIC=2202.56 Logistic regression 안의 평가 기준 낮을 수록 좋다. Step: AIC=2202.2pos_binary ~ love + veri + just + size + dress + fit + will + back + like + tri + flatter + top + length + realli + shirt + materi 모형 성능 측정123456# ---- 모형 성능 측정 ----# install.packages(&quot;pROC&quot;)library(pROC)preds = predict(glm_model, newdata = test, type = &quot;response&quot;)roc_glm = roc(test$pos_binary, preds)plot.roc(roc_glm, print.auc=TRUE) 정리1. 정형 데이터 가져 오기 2. 정형 데이터 가공 - 좋아요 수를 활용하여 긍정/부정 data 나눔 3. 정형 데이터 분리 : 텍스트 데이터 따로 분리 4. 텍스트 데이터 처리 (전처리, 토큰화, 코퍼스, DTM) 5. 텍스트 데이터 + 기존 data 합침 6. ML 모형 진행 (다른 모형을 진행 해도 된다. ) 하지만, 혹시 지금까지 배운 내용이 너무 어렵다면 python으로만 하는 것도나쁘지 않다.","link":"/2021/12/17/R/classfication_R/"},{"title":"loan_classification","text":"1. 병렬처리를 위한 패키지 불러오기1library(caret) # 머신러닝을 위한 패키지 1## Loading required package: ggplot2 1## Loading required package: lattice 1library(tidyverse) # 데이터 핸들링 및 시각화를 위한 패키지 1## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── 1234## ✓ tibble 3.1.6 ✓ dplyr 1.0.7## ✓ tidyr 1.1.4 ✓ stringr 1.4.0## ✓ readr 2.1.0 ✓ forcats 0.5.1## ✓ purrr 0.3.4 1234## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──## x dplyr::filter() masks stats::filter()## x dplyr::lag() masks stats::lag()## x purrr::lift() masks caret::lift() 1library(doParallel) # 병렬처리를 위한 패키지 1## Loading required package: foreach 12## ## Attaching package: 'foreach' 123## The following objects are masked from 'package:purrr':## ## accumulate, when 1## Loading required package: iterators 1## Loading required package: parallel 병렬처리 주 목적: 속도 때문에 씀 원리 및 기타 설명은 다음 링크를 참고한다. https://freshrimpsushi.tistory.com/1266 1detectCores() # 현재 자기 컴퓨터의 코어 개수를 반환한다 1## [1] 8 병렬처리에 쓸 코어를 등록한다. 보통 50% 쓰는 것을 추천한다. (이유: 모형이 개발되는 동안 다른 간단한 작업도 해야 함) 12cl &lt;- parallel::makeCluster(6, setup_timeout = 0.5)registerDoParallel(cl) 2. 데이터 가져오기 경로를 확인한 뒤 데이터를 가져온다. 12loan_data &lt;- read.csv(&quot;data/cleaned_loan_data.csv&quot;, stringsAsFactors = FALSE)dim(loan_data) 1## [1] 29091 8 3. 데이터 전처리 경로를 확인한 뒤 데이터를 가져온다. 먼저 중복값을 확인한다. 1sapply(loan_data, function(x) sum(is.na(x))) 1234## loan_status loan_amnt grade home_ownership annual_inc ## 0 0 0 0 0 ## age emp_cat ir_cat ## 0 0 0 데이터 타입을 확인한다. 1loan_data %&gt;% duplicated() %&gt;% sum() # 374개 확인 1## [1] 374 1loan_data2 &lt;- loan_data %&gt;% distinct() 데이터 타입을 확인한다. 1glimpse(loan_data2) 12345678910## Rows: 28,717## Columns: 8## $ loan_status &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0…## $ loan_amnt &lt;int&gt; 5000, 2400, 10000, 5000, 3000, 12000, 9000, 3000, 10000…## $ grade &lt;chr&gt; &quot;B&quot;, &quot;C&quot;, &quot;C&quot;, &quot;A&quot;, &quot;E&quot;, &quot;B&quot;, &quot;C&quot;, &quot;B&quot;, &quot;B&quot;, &quot;D&quot;, &quot;C&quot;, …## $ home_ownership &lt;chr&gt; &quot;RENT&quot;, &quot;RENT&quot;, &quot;RENT&quot;, &quot;RENT&quot;, &quot;RENT&quot;, &quot;OWN&quot;, &quot;RENT&quot;, …## $ annual_inc &lt;dbl&gt; 24000.00, 12252.00, 49200.00, 36000.00, 48000.00, 75000…## $ age &lt;int&gt; 33, 31, 24, 39, 24, 28, 22, 22, 28, 22, 23, 27, 30, 24,…## $ emp_cat &lt;chr&gt; &quot;0-15&quot;, &quot;15-30&quot;, &quot;0-15&quot;, &quot;0-15&quot;, &quot;0-15&quot;, &quot;0-15&quot;, &quot;0-15&quot;…## $ ir_cat &lt;chr&gt; &quot;8-11&quot;, &quot;Missing&quot;, &quot;11-13.5&quot;, &quot;Missing&quot;, &quot;Missing&quot;, &quot;11… 우선 타겟 데이터는 영어로 표현한다. 123loan_data2$loan_status &lt;- factor(loan_data2$loan_status, levels = c(0, 1), labels = c(&quot;non_default&quot;, &quot;default&quot;))loan_data2$grade &lt;- as.factor(loan_data2$grade)loan_data2$home_ownership &lt;- as.factor(loan_data2$home_ownership) 만약 한꺼번에 하고 싶다면 다음과 같이 할 수 있다. 12loan_data2 &lt;- loan_data2 %&gt;% mutate_if(is.character, as.factor) chr 데이터가 모두 factor로 바뀌었는지 확인한다. 1glimpse(loan_data2) 12345678910## Rows: 28,717## Columns: 8## $ loan_status &lt;fct&gt; non_default, non_default, non_default, non_default, non…## $ loan_amnt &lt;int&gt; 5000, 2400, 10000, 5000, 3000, 12000, 9000, 3000, 10000…## $ grade &lt;fct&gt; B, C, C, A, E, B, C, B, B, D, C, A, B, A, B, B, B, B, B…## $ home_ownership &lt;fct&gt; RENT, RENT, RENT, RENT, RENT, OWN, RENT, RENT, RENT, RE…## $ annual_inc &lt;dbl&gt; 24000.00, 12252.00, 49200.00, 36000.00, 48000.00, 75000…## $ age &lt;int&gt; 33, 31, 24, 39, 24, 28, 22, 22, 28, 22, 23, 27, 30, 24,…## $ emp_cat &lt;fct&gt; 0-15, 15-30, 0-15, 0-15, 0-15, 0-15, 0-15, 0-15, 0-15, …## $ ir_cat &lt;fct&gt; 8-11, Missing, 11-13.5, Missing, Missing, 11-13.5, 11-1… 4. 데이터 분리 훈련 데이터와 테스트 데이터로 분리한다. 1234set.seed(2021)inx &lt;- createDataPartition(loan_data2$loan_status, p = 0.6, list = F)train &lt;- loan_data2[ inx, ]test &lt;- loan_data2[-inx, ] 5. 모형 개발 준비 caret 패키지에서의 모형 개발 관련해서는 다음 웹사이트에서 확인 하기를 바란다. Ref. http://appliedpredictivemodeling.com/ (1) Controller trainControl 함수를 활용하여 기본 세팅을 진행한다. 123456control &lt;- trainControl( method = &quot;repeatedcv&quot;, number = 10, # 10겹 repeats = 3, # 3번 search = &quot;grid&quot;, classProbs = TRUE) (2) Feature Engineering 통계처리를 진행한다. 123456preProc &lt;- c(&quot;BoxCox&quot;, &quot;center&quot;, &quot;scale&quot;, &quot;spatialSign&quot;, &quot;corr&quot;, &quot;zv&quot;) (3) 독립 변수와 종속 변수의 정의 독립변수와 종속 변수를 정의한다. 1frml &lt;- loan_status ~ loan_amnt + grade + home_ownership + annual_inc + age + emp_cat + ir_cat 6. 모형개발 개발준비가 끝났다면, 다양한 모델을 개발하도록 한다. (1) 로지스틱회귀분석12345678910logis &lt;- train( frml, data = train, method = &quot;glm&quot;, metric = &quot;Accuracy&quot;, trControl = control, preProcess = preProc)logis 1234567891011121314## Generalized Linear Model ## ## 17231 samples## 7 predictor## 2 classes: 'non_default', 'default' ## ## Pre-processing: Box-Cox transformation (3), centered (20), scaled (20),## spatial sign transformation (20) ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 15509, 15508, 15508, 15507, 15508, 15507, ... ## Resampling results:## ## Accuracy Kappa ## 0.8878377 -0.0004200657 (2) 의사결정나무 의사결정 나무에서 하이퍼파라미터를 정의한다. 12rpartGrid &lt;- expand.grid(cp = c(0.001, 0.005, 0.01))modelLookup(&quot;rpart&quot;) 12## model parameter label forReg forClass probModel## 1 rpart cp Complexity Parameter TRUE TRUE TRUE 이제 모형을 개발한다. 1234567891011set.seed(2021)rpt &lt;- train( frml, data = train, method = &quot;rpart&quot;, metric = &quot;Accuracy&quot;, trControl = control, preProcess = preProc, tuneGrid = rpartGrid)rpt 12345678910111213141516171819## CART ## ## 17231 samples## 7 predictor## 2 classes: 'non_default', 'default' ## ## Pre-processing: Box-Cox transformation (3), centered (20), scaled (20),## spatial sign transformation (20) ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 15508, 15507, 15508, 15508, 15508, 15508, ... ## Resampling results across tuning parameters:## ## cp Accuracy Kappa ## 0.001 0.8872189 0.008546392## 0.005 0.8880506 0.000000000## 0.010 0.8880506 0.000000000## ## Accuracy was used to select the optimal model using the largest value.## The final value used for the model was cp = 0.01. 1ggplot(rpt) (3) 랜덤포레스트 이번에는 랜덤포레스트를 사용하기 위한 하이퍼파라미터를 정의한다. 12rfGrid &lt;- expand.grid(mtry = c(3, 4, 5))modelLookup(&quot;rf&quot;) 12## model parameter label forReg forClass probModel## 1 rf mtry #Randomly Selected Predictors TRUE TRUE TRUE 랜덤포레스트 모델을 개발한다. 1234567891011rf &lt;- train( frml, data = train, method = &quot;rf&quot;, metric = &quot;Accuracy&quot;, trControl = control, preProcess = preProc, tuneGrid = rfGrid)rf 12345678910111213141516171819## Random Forest ## ## 17231 samples## 7 predictor## 2 classes: 'non_default', 'default' ## ## Pre-processing: Box-Cox transformation (3), centered (20), scaled (20),## spatial sign transformation (20) ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 15508, 15508, 15507, 15508, 15509, 15508, ... ## Resampling results across tuning parameters:## ## mtry Accuracy Kappa ## 3 0.8768692 0.005206944## 4 0.8774109 0.005965678## 5 0.8777978 0.008642398## ## Accuracy was used to select the optimal model using the largest value.## The final value used for the model was mtry = 5. 1ggplot(rf) 7. 모형 Resampling 3개의 모형을 비교하도록 한다. 123456resamps &lt;- resamples( list(glm = logis, rpt = rpt, rf = rf))summary(resamps) 12345678910111213141516171819202122## ## Call:## summary.resamples(object = resamps)## ## Models: glm, rpt, rf ## Number of resamples: 30 ## ## Accuracy ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA's## glm 0.8861789 0.8879861 0.8879861 0.8878377 0.8879861 0.8885017 0## rpt 0.8879861 0.8879861 0.8879861 0.8880506 0.8879861 0.8885665 0## rf 0.8723157 0.8758701 0.8772134 0.8777978 0.8797156 0.8839234 0## ## Kappa ## Min. 1st Qu. Median Mean 3rd Qu. Max.## glm -0.004571755 0.000000000 0.0000000 -0.0004200657 0.00000000 0.00000000## rpt 0.000000000 0.000000000 0.0000000 0.0000000000 0.00000000 0.00000000## rf -0.024423256 -0.003628695 0.0129503 0.0086423981 0.01672265 0.03835002## NA's## glm 0## rpt 0## rf 0 1bwplot(resamps, layout = c(2, 1)) 8. 최종모형 선정 및 모형평가(1) Confusion Matrix1234pred_rpt &lt;- predict(rf, test, type = &quot;prob&quot;)pred_rpt$loan_status &lt;- ifelse(pred_rpt$non_default &gt; 0.85, 0, 1) # cut-off를 조정하며 맞춰보자pred_rpt$loan_status &lt;- factor(pred_rpt$loan_status, levels = c(0, 1), labels = c(&quot;non_default&quot;, &quot;default&quot;))confusionMatrix(pred_rpt$loan_status, test$loan_status, positive = &quot;non_default&quot;) 123456789101112131415161718192021222324252627## Confusion Matrix and Statistics## ## Reference## Prediction non_default default## non_default 7428 787## default 2772 499## ## Accuracy : 0.6901 ## 95% CI : (0.6816, 0.6986)## No Information Rate : 0.888 ## P-Value [Acc &gt; NIR] : 1 ## ## Kappa : 0.0694 ## ## Mcnemar's Test P-Value : &lt;2e-16 ## ## Sensitivity : 0.7282 ## Specificity : 0.3880 ## Pos Pred Value : 0.9042 ## Neg Pred Value : 0.1526 ## Prevalence : 0.8880 ## Detection Rate : 0.6467 ## Detection Prevalence : 0.7152 ## Balanced Accuracy : 0.5581 ## ## 'Positive' Class : non_default ## (2) ROC Curve &amp; AUC 이번에는 ROC Curve와 AUC를 계산하도록 한다. 12345library(ROCR)pr &lt;- prediction(as.numeric(pred_rpt$loan_status) - 1, as.numeric(test$loan_status) - 1)prf &lt;- performance(pr, measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;)plot(prf, col = &quot;blue&quot;)abline(a = 0, b = 1) 1234# AUC = Area Under Curve의 뜻으로auc &lt;- performance(pr, measure = &quot;auc&quot;)auc &lt;- auc@y.values[[1]]; auc 1## [1] 0.5581301","link":"/2021/11/29/R/loan_classification/"},{"title":"Text Mining in R(01)","text":"R을 이용한 TextMining NLP : 자연어 NLP slide 110p, python코드 책 3권 참조 딥 러닝을 이용한 자연어 처리 입문/Ko 토픽 모델링(Topic Modeling) 까지는 가능 다음 내용 :Text Mining in R (02) ** R Install에 관한 내용은여기 있다. 빅카인즈 (Korea) Text data 분석 하는 곳 bigkinds/ko 감정분석 댓글에서 부정/ 긍정 에 대해 확인 R 환경 설정12345678install.packaged(&quot;multilinguer&quot;)#위에 Install이 안되면, 아래 것으로 설치 install.packages(&quot;remotes&quot;)remotes::install_github(&quot;mrchypark/multilinguer&quot;)install_jdk()#자바 설치가 자동으로 path 설정 까지 될 수 있도록 해줌 package ‘rJava’ successfully unpacked and MD5 sums checked R-tool 설치 (path 설정) 이미 R-tool 이 설치가 되어있다면, Pass R-tool 설치 후 아래 코드를 실행 한 후 R Studio program 종료후 재시작 123write('PATH=&quot;${RTOOLS40_HOME}\\\\usr\\\\bin;${PATH}&quot;', file = &quot;~/.Renviron&quot;, append = TRUE)Sys.which(&quot;make&quot;) write(‘PATH=”${RTOOLS40_HOME}\\usr\\bin;${PATH}”‘, file = “~/.Renviron”, append = TRUE) Sys.which(“make”) ………………………………………make “C:\\rtools40\\usr\\bin\\make.exe” jsonlite install1install.packages(&quot;jsonlite&quot;, type = &quot;source&quot;) install.packages(“jsonlite”, type = “source”)‘C:/Users/brill/Documents/R/win-library/4.1’의 위치에 패키지(들)을 설치합니다.(왜냐하면 ‘lib’가 지정되지 않았기 때문입니다)trying URL ‘https://cran.rstudio.com/src/contrib/jsonlite_1.7.2.tar.gz'Content type ‘application/x-gzip’ length 421716 bytes (411 KB)downloaded 411 KB installing source package ‘jsonlite’ … package ‘jsonlite’ successfully unpacked and MD5 sums checked using staged installation libs“C:/rtools40/mingw64/bin/“gcc -I”C:/PROGRA1/R/R-411.2/include” -DNDEBUG -Iyajl/api -D__USE_MINGW_ANSI_STDIO -O2 -Wall -std=gnu99 -mfpmath=sse -msse2 -mstackrealign -c base64.c -o base64.o“C:/rtools40/mingw64/bin/“gcc -I”C:/PROGRA1/R/R-411.2/include” -DNDEBUG -Iyajl/api -D__USE_MINGW_ANSI_STDIO -O2 -Wall -std=gnu99 -mfpmath=sse -msse2 -mstackrealign -c collapse_array.c -o collapse_array.o“C:/rtools40/mingw64/bin/“gcc -I”C:/PROGRA1/R/R-411.2/include” -DNDEBUG -Iyajl/api -D__USE_MINGW_ANSI_STDIO -O2 -Wall -std=gnu99 -mfpmath=sse -msse2 -mstackrealign -c collapse_object.c -o collapse_object.o“C:/rtools40/mingw64/bin/“gcc -I”C:/PROGRA1/R/R-411.2/include” -DNDEBUG -Iyajl/api -D__USE_MINGW_ANSI_STDIO -O2 -Wall -std=gnu99 -mfpmath=sse -msse2 -mstackrealign -c collapse_pretty.c -o collapse_pretty.o“C:/rtools40/mingw64/bin/“gcc -I”C:/PROGRA1/R/R-411.2/include” -DNDEBUG -Iyajl/api -D__USE_MINGW_ANSI_STDIO -O2 -Wall -std=gnu99 -mfpmath=sse -msse2 -mstackrealign -c escape_chars.c -o escape_chars.o“C:/rtools40/mingw64/bin/“gcc -I”C:/PROGRA1/R/R-411.2/include” -DNDEBUG -Iyajl/api -D__USE_MINGW_ANSI_STDIO -O2 -Wall -std=gnu99 -mfpmath=sse -msse2 -mstackrealign -c integer64_to_na.c -o integer64_to_na.o“C:/rtools40/mingw64/bin/“gcc -I”C:/PROGRA1/R/R-411.2/include” -DNDEBUG -Iyajl/api -D__USE_MINGW_ANSI_STDIO -O2 -Wall -std=gnu99 -mfpmath=sse -msse2 -mstackrealign -c is_datelist.c -o is_datelist.o“C:/rtools40/mingw64/bin/“gcc -I”C:/PROGRA1/R/R-411.2/include” -DNDEBUG -Iyajl/api -D__USE_MINGW_ANSI_STDIO -O2 -Wall -std=gnu99 -mfpmath=sse -msse2 -mstackrealign -c is_recordlist.c -o is_recordlist.o“C:/rtools40/mingw64/bin/“gcc -I”C:/PROGRA1/R/R-411.2/include” -DNDEBUG -Iyajl/api -D__USE_MINGW_ANSI_STDIO -O2 -Wall -std=gnu99 -mfpmath=sse -msse2 -mstackrealign -c is_scalarlist.c -o is_scalarlist.o“C:/rtools40/mingw64/bin/“gcc -I”C:/PROGRA1/R/R-411.2/include” -DNDEBUG -Iyajl/api -D__USE_MINGW_ANSI_STDIO -O2 -Wall -std=gnu99 -mfpmath=sse -msse2 -mstackrealign -c modp_numtoa.c -o modp_numtoa.o“C:/rtools40/mingw64/bin/“gcc -I”C:/PROGRA1/R/R-411.2/include” -DNDEBUG -Iyajl/api -D__USE_MINGW_ANSI_STDIO -O2 -Wall -std=gnu99 -mfpmath=sse -msse2 -mstackrealign -c null_to_na.c -o null_to_na.o“C:/rtools40/mingw64/bin/“gcc -I”C:/PROGRA1/R/R-411.2/include” -DNDEBUG -Iyajl/api -D__USE_MINGW_ANSI_STDIO -O2 -Wall -std=gnu99 -mfpmath=sse -msse2 -mstackrealign -c num_to_char.c -o num_to_char.o“C:/rtools40/mingw64/bin/“gcc -I”C:/PROGRA1/R/R-411.2/include” -DNDEBUG -Iyajl/api -D__USE_MINGW_ANSI_STDIO -O2 -Wall -std=gnu99 -mfpmath=sse -msse2 -mstackrealign -c parse.c -o parse.o“C:/rtools40/mingw64/bin/“gcc -I”C:/PROGRA1/R/R-411.2/include” -DNDEBUG -Iyajl/api -D__USE_MINGW_ANSI_STDIO -O2 -Wall -std=gnu99 -mfpmath=sse -msse2 -mstackrealign -c prettify.c -o prettify.o“C:/rtools40/mingw64/bin/“gcc -I”C:/PROGRA1/R/R-411.2/include” -DNDEBUG -Iyajl/api -D__USE_MINGW_ANSI_STDIO -O2 -Wall -std=gnu99 -mfpmath=sse -msse2 -mstackrealign -c push_parser.c -o push_parser.o“C:/rtools40/mingw64/bin/“gcc -I”C:/PROGRA1/R/R-411.2/include” -DNDEBUG -Iyajl/api -D__USE_MINGW_ANSI_STDIO -O2 -Wall -std=gnu99 -mfpmath=sse -msse2 -mstackrealign -c r-base64.c -o r-base64.o“C:/rtools40/mingw64/bin/“gcc -I”C:/PROGRA1/R/R-411.2/include” -DNDEBUG -Iyajl/api -D__USE_MINGW_ANSI_STDIO -O2 -Wall -std=gnu99 -mfpmath=sse -msse2 -mstackrealign -c register.c -o register.o“C:/rtools40/mingw64/bin/“gcc -I”C:/PROGRA1/R/R-411.2/include” -DNDEBUG -Iyajl/api -D__USE_MINGW_ANSI_STDIO -O2 -Wall -std=gnu99 -mfpmath=sse -msse2 -mstackrealign -c row_collapse.c -o row_collapse.o“C:/rtools40/mingw64/bin/“gcc -I”C:/PROGRA1/R/R-411.2/include” -DNDEBUG -Iyajl/api -D__USE_MINGW_ANSI_STDIO -O2 -Wall -std=gnu99 -mfpmath=sse -msse2 -mstackrealign -c transpose_list.c -o transpose_list.o“C:/rtools40/mingw64/bin/“gcc -I”C:/PROGRA1/R/R-411.2/include” -DNDEBUG -Iyajl/api -D__USE_MINGW_ANSI_STDIO -O2 -Wall -std=gnu99 -mfpmath=sse -msse2 -mstackrealign -c validate.c -o validate.o“C:/rtools40/mingw64/bin/“gcc -I”C:/PROGRA1/R/R-411.2/include” -DNDEBUG -Iyajl/api -D__USE_MINGW_ANSI_STDIO -O2 -Wall -std=gnu99 -mfpmath=sse -msse2 -mstackrealign -c yajl/yajl.c -o yajl/yajl.o“C:/rtools40/mingw64/bin/“gcc -I”C:/PROGRA1/R/R-411.2/include” -DNDEBUG -Iyajl/api -D__USE_MINGW_ANSI_STDIO -O2 -Wall -std=gnu99 -mfpmath=sse -msse2 -mstackrealign -c yajl/yajl_alloc.c -o yajl/yajl_alloc.o“C:/rtools40/mingw64/bin/“gcc -I”C:/PROGRA1/R/R-411.2/include” -DNDEBUG -Iyajl/api -D__USE_MINGW_ANSI_STDIO -O2 -Wall -std=gnu99 -mfpmath=sse -msse2 -mstackrealign -c yajl/yajl_buf.c -o yajl/yajl_buf.o“C:/rtools40/mingw64/bin/“gcc -I”C:/PROGRA1/R/R-411.2/include” -DNDEBUG -Iyajl/api -D__USE_MINGW_ANSI_STDIO -O2 -Wall -std=gnu99 -mfpmath=sse -msse2 -mstackrealign -c yajl/yajl_encode.c -o yajl/yajl_encode.o“C:/rtools40/mingw64/bin/“gcc -I”C:/PROGRA1/R/R-411.2/include” -DNDEBUG -Iyajl/api -D__USE_MINGW_ANSI_STDIO -O2 -Wall -std=gnu99 -mfpmath=sse -msse2 -mstackrealign -c yajl/yajl_gen.c -o yajl/yajl_gen.o“C:/rtools40/mingw64/bin/“gcc -I”C:/PROGRA1/R/R-411.2/include” -DNDEBUG -Iyajl/api -D__USE_MINGW_ANSI_STDIO -O2 -Wall -std=gnu99 -mfpmath=sse -msse2 -mstackrealign -c yajl/yajl_lex.c -o yajl/yajl_lex.o“C:/rtools40/mingw64/bin/“gcc -I”C:/PROGRA1/R/R-411.2/include” -DNDEBUG -Iyajl/api -D__USE_MINGW_ANSI_STDIO -O2 -Wall -std=gnu99 -mfpmath=sse -msse2 -mstackrealign -c yajl/yajl_parser.c -o yajl/yajl_parser.o“C:/rtools40/mingw64/bin/“gcc -I”C:/PROGRA1/R/R-411.2/include” -DNDEBUG -Iyajl/api -D__USE_MINGW_ANSI_STDIO -O2 -Wall -std=gnu99 -mfpmath=sse -msse2 -mstackrealign -c yajl/yajl_tree.c -o yajl/yajl_tree.o“C:/rtools40/mingw64/bin/“ar rcs yajl/libstatyajl.a yajl/yajl.o yajl/yajl_alloc.o yajl/yajl_buf.o yajl/yajl_encode.o yajl/yajl_gen.o yajl/yajl_lex.o yajl/yajl_parser.o yajl/yajl_tree.oC:/rtools40/mingw64/bin/gcc -shared -s -static-libgcc -o jsonlite.dll tmp.def base64.o collapse_array.o collapse_object.o collapse_pretty.o escape_chars.o integer64_to_na.o is_datelist.o is_recordlist.o is_scalarlist.o modp_numtoa.o null_to_na.o num_to_char.o parse.o prettify.o push_parser.o r-base64.o register.o row_collapse.o transpose_list.o validate.o -Lyajl -lstatyajl -LC:/PROGRA1/R/R-411.2/bin/x64 -lRinstalling to C:/Users/brill/Documents/R/win-library/4.1/00LOCK-jsonlite/00new/jsonlite/libs/x64 R inst byte-compile and prepare package for lazy loadingin method for ‘asJSON’ with signature ‘“blob”‘: no definition for class “blob” help ** installing help indices converting help for package ‘jsonlite’ finding HTML links … done base64 html flatten html fromJSON html prettify html rbind_pages html read_json html serializeJSON html stream_in html unbox html validate html building package indices installing vignettes testing if installed package can be loaded from temporary location testing if installed package can be loaded from final location testing if installed package keeps a record of temporary installation path DONE (jsonlite) R packages 설치12install.packages(c(&quot;stringr&quot;, &quot;hash&quot;, &quot;tau&quot;, &quot;Sejong&quot;, &quot;RSQLite&quot;, &quot;devtools&quot;), type = &quot;binary&quot;) The downloaded source packages are in ‘C:\\Users\\brill\\AppData\\Local\\Temp\\RtmpmuDZXg\\downloaded_packages’install.packages(c(“stringr”, “hash”, “tau”, “Sejong”, “RSQLite”, “devtools”), type = &quot;binary&quot;) ‘C:/Users/brill/Documents/R/win-library/4.1’의 위치에 패키지(들)을 설치합니다.(왜냐하면 ‘lib’가 지정되지 않았기 때문입니다)‘fastmap’, ‘highr’, ‘xfun’, ‘diffobj’, ‘rematch2’, ‘bit’, ‘cachem’, ‘processx’, ‘prettyunits’, ‘digest’, ‘xopen’, ‘brew’, ‘commonmark’, ‘knitr’, ‘cpp11’, ‘brio’, ‘evaluate’, ‘praise’, ‘ps’, ‘waldo’, ‘bit64’, ‘blob’, ‘DBI’, ‘memoise’, ‘Rcpp’, ‘plogr’, ‘callr’, ‘pkgbuild’, ‘pkgload’, ‘rcmdcheck’, ‘roxygen2’, ‘rversions’, ‘sessioninfo’, ‘testthat’(들)을 또한 설치합니다. trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/fastmap_1.1.0.zip'Content type ‘application/zip’ length 215381 bytes (210 KB)downloaded 210 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/highr_0.9.zip'Content type ‘application/zip’ length 46725 bytes (45 KB)downloaded 45 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/xfun_0.28.zip'Content type ‘application/zip’ length 386111 bytes (377 KB)downloaded 377 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/diffobj_0.3.5.zip'Content type ‘application/zip’ length 999001 bytes (975 KB)downloaded 975 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/rematch2_2.1.2.zip'Content type ‘application/zip’ length 47584 bytes (46 KB)downloaded 46 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/bit_4.0.4.zip'Content type ‘application/zip’ length 635254 bytes (620 KB)downloaded 620 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/cachem_1.0.6.zip'Content type ‘application/zip’ length 79002 bytes (77 KB)downloaded 77 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/processx_3.5.2.zip'Content type ‘application/zip’ length 1246508 bytes (1.2 MB)downloaded 1.2 MB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/prettyunits_1.1.1.zip'Content type ‘application/zip’ length 37755 bytes (36 KB)downloaded 36 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/digest_0.6.29.zip'Content type ‘application/zip’ length 266591 bytes (260 KB)downloaded 260 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/xopen_1.0.0.zip'Content type ‘application/zip’ length 24785 bytes (24 KB)downloaded 24 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/brew_1.0-6.zip'Content type ‘application/zip’ length 113926 bytes (111 KB)downloaded 111 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/commonmark_1.7.zip'Content type ‘application/zip’ length 265490 bytes (259 KB)downloaded 259 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/knitr_1.36.zip'Content type ‘application/zip’ length 1469306 bytes (1.4 MB)downloaded 1.4 MB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/cpp11_0.4.2.zip'Content type ‘application/zip’ length 327396 bytes (319 KB)downloaded 319 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/brio_1.1.3.zip'Content type ‘application/zip’ length 48880 bytes (47 KB)downloaded 47 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/evaluate_0.14.zip'Content type ‘application/zip’ length 76790 bytes (74 KB)downloaded 74 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/praise_1.0.0.zip'Content type ‘application/zip’ length 19849 bytes (19 KB)downloaded 19 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/ps_1.6.0.zip'Content type ‘application/zip’ length 775912 bytes (757 KB)downloaded 757 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/waldo_0.3.1.zip'Content type ‘application/zip’ length 96434 bytes (94 KB)downloaded 94 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/bit64_4.0.5.zip'Content type ‘application/zip’ length 565517 bytes (552 KB)downloaded 552 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/blob_1.2.2.zip'Content type ‘application/zip’ length 48321 bytes (47 KB)downloaded 47 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/DBI_1.1.1.zip'Content type ‘application/zip’ length 686681 bytes (670 KB)downloaded 670 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/memoise_2.0.1.zip'Content type ‘application/zip’ length 50131 bytes (48 KB)downloaded 48 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/Rcpp_1.0.7.zip'Content type ‘application/zip’ length 3263462 bytes (3.1 MB)downloaded 3.1 MB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/plogr_0.2.0.zip'Content type ‘application/zip’ length 18943 bytes (18 KB)downloaded 18 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/callr_3.7.0.zip'Content type ‘application/zip’ length 437774 bytes (427 KB)downloaded 427 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/pkgbuild_1.3.0.zip'Content type ‘application/zip’ length 146266 bytes (142 KB)downloaded 142 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/pkgload_1.2.4.zip'Content type ‘application/zip’ length 156265 bytes (152 KB)downloaded 152 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/rcmdcheck_1.4.0.zip'Content type ‘application/zip’ length 170257 bytes (166 KB)downloaded 166 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/roxygen2_7.1.2.zip'Content type ‘application/zip’ length 1352846 bytes (1.3 MB)downloaded 1.3 MB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/rversions_2.1.1.zip'Content type ‘application/zip’ length 67399 bytes (65 KB)downloaded 65 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/sessioninfo_1.2.2.zip'Content type ‘application/zip’ length 186234 bytes (181 KB)downloaded 181 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/testthat_3.1.1.zip'Content type ‘application/zip’ length 2545637 bytes (2.4 MB)downloaded 2.4 MB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/stringr_1.4.0.zip'Content type ‘application/zip’ length 216715 bytes (211 KB)downloaded 211 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/hash_2.2.6.1.zip'Content type ‘application/zip’ length 178061 bytes (173 KB)downloaded 173 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/tau_0.0-24.zip'Content type ‘application/zip’ length 186662 bytes (182 KB)downloaded 182 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/Sejong_0.01.zip'Content type ‘application/zip’ length 1617954 bytes (1.5 MB)downloaded 1.5 MB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/RSQLite_2.2.9.zip'Content type ‘application/zip’ length 2511267 bytes (2.4 MB)downloaded 2.4 MB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/devtools_2.4.3.zip'Content type ‘application/zip’ length 423398 bytes (413 KB)downloaded 413 KB package ‘fastmap’ successfully unpacked and MD5 sums checkedpackage ‘highr’ successfully unpacked and MD5 sums checkedpackage ‘xfun’ successfully unpacked and MD5 sums checkedpackage ‘diffobj’ successfully unpacked and MD5 sums checkedpackage ‘rematch2’ successfully unpacked and MD5 sums checkedpackage ‘bit’ successfully unpacked and MD5 sums checkedpackage ‘cachem’ successfully unpacked and MD5 sums checkedpackage ‘processx’ successfully unpacked and MD5 sums checkedpackage ‘prettyunits’ successfully unpacked and MD5 sums checkedpackage ‘digest’ successfully unpacked and MD5 sums checkedpackage ‘xopen’ successfully unpacked and MD5 sums checkedpackage ‘brew’ successfully unpacked and MD5 sums checkedpackage ‘commonmark’ successfully unpacked and MD5 sums checkedpackage ‘knitr’ successfully unpacked and MD5 sums checkedpackage ‘cpp11’ successfully unpacked and MD5 sums checkedpackage ‘brio’ successfully unpacked and MD5 sums checkedpackage ‘evaluate’ successfully unpacked and MD5 sums checkedpackage ‘praise’ successfully unpacked and MD5 sums checkedpackage ‘ps’ successfully unpacked and MD5 sums checkedpackage ‘waldo’ successfully unpacked and MD5 sums checkedpackage ‘bit64’ successfully unpacked and MD5 sums checkedpackage ‘blob’ successfully unpacked and MD5 sums checkedpackage ‘DBI’ successfully unpacked and MD5 sums checkedpackage ‘memoise’ successfully unpacked and MD5 sums checkedpackage ‘Rcpp’ successfully unpacked and MD5 sums checkedpackage ‘plogr’ successfully unpacked and MD5 sums checkedpackage ‘callr’ successfully unpacked and MD5 sums checkedpackage ‘pkgbuild’ successfully unpacked and MD5 sums checkedpackage ‘pkgload’ successfully unpacked and MD5 sums checkedpackage ‘rcmdcheck’ successfully unpacked and MD5 sums checkedpackage ‘roxygen2’ successfully unpacked and MD5 sums checkedpackage ‘rversions’ successfully unpacked and MD5 sums checkedpackage ‘sessioninfo’ successfully unpacked and MD5 sums checkedpackage ‘testthat’ successfully unpacked and MD5 sums checkedpackage ‘stringr’ successfully unpacked and MD5 sums checkedpackage ‘hash’ successfully unpacked and MD5 sums checkedpackage ‘tau’ successfully unpacked and MD5 sums checkedpackage ‘Sejong’ successfully unpacked and MD5 sums checkedpackage ‘RSQLite’ successfully unpacked and MD5 sums checkedpackage ‘devtools’ successfully unpacked and MD5 sums checked 명사 분리기(KoNLP) 설치를 위한 remotes packages 설치 (in R)12345# install.packages(&quot;remotes&quot;)remotes::install_github(&quot;haven-jeon/KoNLP&quot;, upgrade = &quot;never&quot;, force = TRUE, INSTALL_opts = c(&quot;--no-multiarch&quot;)) # install.packages(“remotes”)remotes::install_github(“haven-jeon/KoNLP”, upgrade = &quot;never&quot;, force = TRUE, INSTALL_opts = c(&quot;--no-multiarch&quot;)) Downloading GitHub repo haven-jeon/KoNLP@HEAD√ checking for file ‘C:\\Users\\brill\\AppData\\Local\\Temp\\RtmpmuDZXg\\remotes2cd03d177e06\\haven-jeon-KoNLP-960fbbc/DESCRIPTION’ … preparing ‘KoNLP’: (722ms)√ checking DESCRIPTION meta-information … checking for LF line-endings in source and make files and shell scripts checking for empty or unneeded directories looking to see if a ‘data/datalist’ file should be added building ‘KoNLP_0.80.2.tar.gz’ ‘C:/Users/brill/Documents/R/win-library/4.1’의 위치에 패키지(들)을 설치합니다.(왜냐하면 ‘lib’가 지정되지 않았기 때문입니다) installing source package ‘KoNLP’ … using staged installation R data inst byte-compile and prepare package for lazy loading help ** installing help indices converting help for package ‘KoNLP’ finding HTML links … done HangulAutomata html KtoS html MorphAnalyzer html SimplePos09 html SimplePos22 html StoK html backupUsrDic html buildDictionary html concordance_file html concordance_str html convertHangulStringToJamos html convertHangulStringToKeyStrokes html convertTag html editweights html extractNoun html get_dictionary html is.ascii html is.hangul html is.jaeum html is.jamo html is.moeum html mergeUserDic html mutualinformation html reloadAllDic html reloadUserDic html restoreUsrDic html scala_library_install html statDic html tags html useNIADic html useSejongDic html useSystemDic html building package indices installing vignettes testing if installed package can be loaded from temporary location[1] “DEBUG start”[1] “C:/Users/brill/Documents/R/win-library/4.1/00LOCK-KoNLP/00new/KoNLP/java/scala-library-2.11.8.jar”[1] “My R is over 3.2.0”[1] “scala-library target url: https://repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.8/scala-library-2.11.8.jar&quot;[1] “‘method’ parameter for download.file() function in your R: wininet”URL ‘https://repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.8/scala-library-2.11.8.jar'을 시도합니다Content type ‘application/java-archive’ length 5744974 bytes (5.5 MB)downloaded 5.5 MB [1] TRUE[1] 5744974Successfully installed Scala runtime library in C:/Users/brill/Documents/R/win-library/4.1/00LOCK-KoNLP/00new/KoNLP/java/scala-library-2.11.8.jar** testing if installed package can be loaded from final location** testing if installed package keeps a record of temporary installation path DONE (KoNLP) 명사 분리기 KoNLP 설치12library(KoNLP)useNIADic() library(KoNLP)useNIADic()Backup was just finished!Downloading package from url: https://github.com/haven-jeon/NIADic/releases/download/0.0.1/NIADic_0.0.1.tar.gzInstalling 16 packages: colorspace, viridisLite, RColorBrewer, munsell, labeling, farver, base64enc, htmltools, scales, isoband, gtable, jquerylib, tinytex, ggplot2, data.table, rmarkdown‘C:/Users/brill/Documents/R/win-library/4.1’의 위치에 패키지(들)을 설치합니다.(왜냐하면 ‘lib’가 지정되지 않았기 때문입니다)trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/colorspace_2.0-2.zip'Content type ‘application/zip’ length 2645307 bytes (2.5 MB)downloaded 2.5 MB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/viridisLite_0.4.0.zip'Content type ‘application/zip’ length 1299504 bytes (1.2 MB)downloaded 1.2 MB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/RColorBrewer_1.1-2.zip'Content type ‘application/zip’ length 55707 bytes (54 KB)downloaded 54 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/munsell_0.5.0.zip'Content type ‘application/zip’ length 245486 bytes (239 KB)downloaded 239 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/labeling_0.4.2.zip'Content type ‘application/zip’ length 62679 bytes (61 KB)downloaded 61 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/farver_2.1.0.zip'Content type ‘application/zip’ length 1752621 bytes (1.7 MB)downloaded 1.7 MB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/base64enc_0.1-3.zip'Content type ‘application/zip’ length 43156 bytes (42 KB)downloaded 42 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/htmltools_0.5.2.zip'Content type ‘application/zip’ length 347310 bytes (339 KB)downloaded 339 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/scales_1.1.1.zip'Content type ‘application/zip’ length 558895 bytes (545 KB)downloaded 545 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/isoband_0.2.5.zip'Content type ‘application/zip’ length 2726764 bytes (2.6 MB)downloaded 2.6 MB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/gtable_0.3.0.zip'Content type ‘application/zip’ length 434327 bytes (424 KB)downloaded 424 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/jquerylib_0.1.4.zip'Content type ‘application/zip’ length 525848 bytes (513 KB)downloaded 513 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/tinytex_0.35.zip'Content type ‘application/zip’ length 126495 bytes (123 KB)downloaded 123 KB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/ggplot2_3.3.5.zip'Content type ‘application/zip’ length 4130301 bytes (3.9 MB)downloaded 3.9 MB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/data.table_1.14.2.zip'Content type ‘application/zip’ length 2600846 bytes (2.5 MB)downloaded 2.5 MB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/rmarkdown_2.11.zip'Content type ‘application/zip’ length 3660449 bytes (3.5 MB)downloaded 3.5 MB package ‘colorspace’ successfully unpacked and MD5 sums checkedpackage ‘viridisLite’ successfully unpacked and MD5 sums checkedpackage ‘RColorBrewer’ successfully unpacked and MD5 sums checkedpackage ‘munsell’ successfully unpacked and MD5 sums checkedpackage ‘labeling’ successfully unpacked and MD5 sums checkedpackage ‘farver’ successfully unpacked and MD5 sums checkedpackage ‘base64enc’ successfully unpacked and MD5 sums checkedpackage ‘htmltools’ successfully unpacked and MD5 sums checkedpackage ‘scales’ successfully unpacked and MD5 sums checkedpackage ‘isoband’ successfully unpacked and MD5 sums checkedpackage ‘gtable’ successfully unpacked and MD5 sums checkedpackage ‘jquerylib’ successfully unpacked and MD5 sums checkedpackage ‘tinytex’ successfully unpacked and MD5 sums checkedpackage ‘ggplot2’ successfully unpacked and MD5 sums checkedpackage ‘data.table’ successfully unpacked and MD5 sums checkedpackage ‘rmarkdown’ successfully unpacked and MD5 sums checked The downloaded binary packages are in C:\\Users\\brill\\AppData\\Local\\Temp\\RtmpmuDZXg\\downloaded_packages√ checking for file ‘C:\\Users\\brill\\AppData\\Local\\Temp\\RtmpmuDZXg\\remotes2cd0437ea43\\NIADic/DESCRIPTION’ … preparing ‘NIADic’:√ checking DESCRIPTION meta-information …√ checking vignette meta-information … checking for LF line-endings in source and make files and shell scripts checking for empty or unneeded directories building ‘NIADic_0.0.1.tar.gz’ ‘C:/Users/brill/Documents/R/win-library/4.1’의 위치에 패키지(들)을 설치합니다.(왜냐하면 ‘lib’가 지정되지 않았기 때문입니다) installing source package ‘NIADic’ … using staged installation R inst byte-compile and prepare package for lazy loading help ** installing help indices converting help for package ‘NIADic’ finding HTML links … done get_dic html building package indices installing vignettes testing if installed package can be loaded from temporary location testing if installed package can be loaded from final location testing if installed package keeps a record of temporary installation path DONE (NIADic)1213109 words dictionary was built. 명사 분리기 설치 후 확인New 12345678910111213141516text = &quot;뿌리산업’의 기반이 되는 공정기술의 범위가 관련법 제정 10년 만에 확대 개편된다. 뿌리기업 우대 지원과 청년층 등 신규인력 유입 지원을 강화하기 위한 법적 토대도 마련된다.산업통상자원부는 이 같은 내용을 담은 ‘뿌리산업 진흥과 첨단화에 관한 법률(뿌리산업법) 시행령’ 개정안이 14일 국무회의에서 의결돼 오는 16일부터 시행된다고 밝혔다.먼저 뿌리산업법 기반 공정기술(뿌리기술)의 범위가 기존 6개(주조, 금형, 소성가공, 용접, 표면처리, 열처리)에서 14개로 늘어난다.구체적으로 소재 다원화 공정기술에 사출·프레스, 정밀가공, 적층제조, 산업용 필름 및 지류 등 4개 기술이 포함된다. 산업부는 이를 통해 세라믹, 플라스틱, 탄성소재, 탄소, 펄프 등 다양한 소재 기반 제조 공정을 확산할 계획이다. 또 지능화 공정기술로 로봇, 센서, 산업 지능형 소프트웨어, 엔지니어링 설계 등 4개 기술이 추가된다.뿌리기술 범위가 확대되면서 뿌리산업의 범위도 기존 6대 산업, 76개 업종에서 14대 산업, 111개 업종으로 늘어난다.이번 개정을 통해 뿌리기업 확인 절차, 확인서 유효기간(3년), 사후관리 등에 관한 규정도 신설됐다. 뿌리기업은 뿌리기술을 활용해 사업을 영위하는 업종 또는 뿌리기술에 활용되는 장비 제조 분야를 말한다.뿌리기업 확인 제도는 외국인 근로자 고용 우대 혜택 등이 주어지는 뿌리산업 관련 우대 지원 대상을 명확히 정하기 위한 것으로 국가뿌리산업진흥센터에서 확인서를 발급해오고 있다. 2012년부터 1만1766건이 발급됐으며 현재 5843건이 유효한 것으로 집계됐다.‘일하기 좋은 뿌리기업’ 선정을 위한 기준과 절차, 지원 내용 등에 관한 규정도 새로 만들어졌다. ‘일하기 좋은 뿌리기업’은 뿌리산업에 청년층 등 신규 인력 유입을 촉진하기 위해 근로·복지 환경, 성장 역량 등이 우수한 기업을 산업부가 선정해 홍보 등을 지원하는 제도다.산업부는 이번 개정 사항이 원활히 시행될 수 있도록 업종별 협·단체, 뿌리기업, 지자체 등을 대상으로 적극 홍보할 방침이다. 아울러 매년 발간하는 뿌리산업 백서를 통해 새롭게 추가되는 8대 차세대 공정기술에 대한 내용, 기술 동향 등을 상세하게 제공하기로 했다．산업부 관계자는 “이번 개정은 2011년 뿌리산업법 제정 후 10년 만에 뿌리기술을 소재다원화와 지능화 중심으로 확장한 것으로, 뿌리산업의 기술 융복합화와 첨단화를 촉진하고 신규 인력 유입 지원을 강화하기 위한 법적 토대를 마련하였다는 데에 의의가 있다”고 말했다.&quot;extractNoun(text) extractNoun(text) [1] “뿌리산업’” “기반” “공정기술” [4] “범위” “관련” “법” [7] “제정” “10” “년” [10] “만” “확대” “개편” [13] “뿌리” “기업” “우대” [16] “지원” “청년층” “등” [19] “신규인력” “유입” “지원” [22] “강화” “하기” “토대” [25] “마련” “산업” “통상” [28] “자원” “부” “내용” [31] “담” “‘뿌리산업” “진흥” [34] “첨단화” “법률(뿌리산업법)” “시행령’” [37] “개정안” “14” “국무회의” [40] “의결” “16” “일” [43] “시행” “뿌리” “산업” [46] “법” “기반” “공정기술(뿌리기술)” [49] “범위” “기존” “6개(주조” [52] “금형” “소성” “가공” [55] “용접” “표면처리” “열처리” [58] “14” “개” “구체” [61] “적” “소재” “다원화” [64] “공정기술” “사출·프레스” “정밀가공” [67] “적층” “제조” “산업용” [70] “필름” “지류” “등” [73] “4” “개” “기술” [76] “포함” “산업” “부” [79] “이” “세라믹” “플라스틱” [82] “탄성소” “재” “탄소” [85] “펄프” “등” “다양” [88] “한” “소재” “기반” [91] “제조” “공정” “확산” [94] “할” “계획” “지능화” [97] “공정기술” “로봇” “센서”[100] “산업” “지능형” “소프트웨어”[103] “엔지니어링” “설계” “등”[106] “4” “개” “기술”[109] “추가” “뿌리” “기술”[112] “범위” “확대” “되”[115] “뿌리” “산업” “범위”[118] “기존” “6” “대”[121] “산업” “76” “개”[124] “업종” “14” “대”[127] “산업” “111” “개”[130] “업종” “이번” “개정”[133] “뿌리” “기업” “확인”[136] “절차” “확인” “유효”[139] “기” “3” “년”[142] “사후관리” “등” “규정도”[145] “신설” “뿌리” “기업”[148] “뿌리” “기술” “활용”[151] “해” “사업” “영위”[154] “하” “업종” “뿌리”[157] “기술” “활용” “되”[160] “장비” “제조” “분야”[163] “말” “뿌리” “기업”[166] “확인” “제” “외국”[169] “근로자” “고용” “우대”[172] “혜택” “등” “뿌리”[175] “산업” “관련” “우대”[178] “지원” “대상” “것”[181] “국가” “뿌리” “산업진흥”[184] “센터” “확인서” “발급”[187] “해오” “2012” “년”[190] “1” “만” “1766”[193] “건” “발급” “5843”[196] “건” “유효” “한”[199] “것” “집계” “‘일하기”[202] “뿌리기업’” “선정” “기준”[205] “절차” “지원” “내용”[208] “등” “규정도” “‘일하기”[211] “뿌리기업’은” “뿌리” “산업”[214] “청년층” “등” “신규”[217] “인력” “유입” “촉진”[220] “하기” “근로·복지” “환경”[223] “성장” “역량” “등”[226] “우수” “한” “기업”[229] “산업” “부” “선정”[232] “해” “홍보” “등”[235] “지원” “하” “제도”[238] “산업” “부” “이번”[241] “개정” “사항” “시행”[244] “수” “업종” “별”[247] “협·단체” “뿌리” “기업”[250] “지자체” “등” “대상”[253] “적극” “홍보” “할”[256] “방침” “발간” “하”[259] “뿌리” “산업” “백서”[262] “추가” “되” “8”[265] “대” “차세대” “공정기술”[268] “내용” “기술” “동향”[271] “등” “상세” “하게”[274] “제공” “하기” “산업”[277] “부” “관계자” ““이번”[280] “개정” “2011” “년”[283] “뿌리” “산업” “법”[286] “제정” “후” “10”[289] “년” “만” “뿌리”[292] “기술” “소재” “다원화”[295] “지능화” “중심” “확장”[298] “한” “것” “뿌리”[301] “산업” “기술” “융복합”[304] “화” “첨단화” “촉진”[307] “신규” “인력” “유입”[310] “지원” “강화” “하기”[313] “토대” “마련” “데”[316] “의의” “있다”고” “말” - 명사 분리기 설치 끝","link":"/2021/12/14/R/textmining(01)/"},{"title":"Text Mining in R(02)","text":"Text Mining in R (02)앞선 내용 :Text Mining in R (01)library(KoNLP), useNIADic() 사용/설치 다음 내용 :Text Mining in R (03) § MeCab 설치Mecab-ko 형태소 분석기 사용 위해서는 Rcppmecab 패키지가 있어야함. RcppMeCab install file URL: 해당 깃허브에서 설치해야 할 파일을 다운로드 받은 후, 압축 해제 시에 C drive 에서 mecab folder 생성 오른쪽 버튼 클릭 후 여기에압출풀기를 선택하면 쉽다. 이 과정에서 위의 file내의 폴더 형태와, file 명, 경로 가 같지 않으면 다음과 같은 에러가 난다. Exception:list() 경로, file명 등을 확인 하기 바란다. 오류 해결 참조 § R 에서 설치1234# library(remotes)remotes::install_github(&quot;junhewk/RcppMeCab&quot;, force = TRUE)library(RcppMeCab) # library(remotes)remotes::install_github(“junhewk/RcppMeCab”, force = TRUE)Downloading GitHub repo junhewk/RcppMeCab@HEADInstalling 2 packages: BH, RcppParallel‘C:/Users/brill/Documents/R/win-library/4.1’의 위치에 패키지(들)을 설치합니다.(왜냐하면 ‘lib’가 지정되지 않았기 때문입니다)trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/BH_1.75.0-0.zip'Content type ‘application/zip’ length 19675040 bytes (18.8 MB)downloaded 18.8 MB trying URL ‘https://cran.rstudio.com/bin/windows/contrib/4.1/RcppParallel_5.1.4.zip'Content type ‘application/zip’ length 2140731 bytes (2.0 MB)downloaded 2.0 MB package ‘BH’ successfully unpacked and MD5 sums checkedpackage ‘RcppParallel’ successfully unpacked and MD5 sums checked The downloaded binary packages are in C:\\Users\\brill\\AppData\\Local\\Temp\\RtmpmuDZXg\\downloaded_packages√ checking for file ‘C:\\Users\\brill\\AppData\\Local\\Temp\\RtmpmuDZXg\\remotes2cd0f4c5d4d\\junhewk-RcppMeCab-e1800aa/DESCRIPTION’ (414ms) preparing ‘RcppMeCab’: (373ms)√ checking DESCRIPTION meta-information … cleaning src checking for LF line-endings in source and make files and shell scripts checking for empty or unneeded directories Omitted ‘LazyData’ from DESCRIPTION building ‘RcppMeCab_0.0.1.3-2.tar.gz’ ‘C:/Users/brill/Documents/R/win-library/4.1’의 위치에 패키지(들)을 설치합니다.(왜냐하면 ‘lib’가 지정되지 않았기 때문입니다) installing source package ‘RcppMeCab’ … using staged installation libs“C:/rtools40/mingw64/bin/“g++ -std=gnu++11 -I”C:/PROGRA1/R/R-411.2/include” -DNDEBUG -I../inst/include -DBOOST_NO_AUTO_PTR -I’C:/Users/brill/Documents/R/win-library/4.1/Rcpp/include’ -I’C:/Users/brill/Documents/R/win-library/4.1/RcppParallel/include’ -I’C:/Users/brill/Documents/R/win-library/4.1/BH/include’ -DRCPP_PARALLEL_USE_TBB=1 -DDLL_IMPORT -DSTRICT_R_HEADERS -Wno-parentheses -O2 -Wall -mfpmath=sse -msse2 -mstackrealign -c RcppExports.cpp -o RcppExports.o“C:/rtools40/mingw64/bin/“g++ -std=gnu++11 -I”C:/PROGRA1/R/R-411.2/include” -DNDEBUG -I../inst/include -DBOOST_NO_AUTO_PTR -I’C:/Users/brill/Documents/R/win-library/4.1/Rcpp/include’ -I’C:/Users/brill/Documents/R/win-library/4.1/RcppParallel/include’ -I’C:/Users/brill/Documents/R/win-library/4.1/BH/include’ -DRCPP_PARALLEL_USE_TBB=1 -DDLL_IMPORT -DSTRICT_R_HEADERS -Wno-parentheses -O2 -Wall -mfpmath=sse -msse2 -mstackrealign -c posParallelRcpp.cpp -o posParallelRcpp.o“C:/rtools40/mingw64/bin/“g++ -std=gnu++11 -I”C:/PROGRA1/R/R-411.2/include” -DNDEBUG -I../inst/include -DBOOST_NO_AUTO_PTR -I’C:/Users/brill/Documents/R/win-library/4.1/Rcpp/include’ -I’C:/Users/brill/Documents/R/win-library/4.1/RcppParallel/include’ -I’C:/Users/brill/Documents/R/win-library/4.1/BH/include’ -DRCPP_PARALLEL_USE_TBB=1 -DDLL_IMPORT -DSTRICT_R_HEADERS -Wno-parentheses -O2 -Wall -mfpmath=sse -msse2 -mstackrealign -c posRcpp.cpp -o posRcpp.o“C:/rtools40/mingw64/bin/“g++ -std=gnu++11 -I”C:/PROGRA1/R/R-411.2/include” -DNDEBUG -I../inst/include -DBOOST_NO_AUTO_PTR -I’C:/Users/brill/Documents/R/win-library/4.1/Rcpp/include’ -I’C:/Users/brill/Documents/R/win-library/4.1/RcppParallel/include’ -I’C:/Users/brill/Documents/R/win-library/4.1/BH/include’ -DRCPP_PARALLEL_USE_TBB=1 -DDLL_IMPORT -DSTRICT_R_HEADERS -Wno-parentheses -O2 -Wall -mfpmath=sse -msse2 -mstackrealign -c posloopRcpp.cpp -o posloopRcpp.oC:/rtools40/mingw64/bin/g++ -shared -s -static-libgcc -o RcppMeCab.dll tmp.def RcppExports.o posParallelRcpp.o posRcpp.o posloopRcpp.o -L../inst/libs/x64 -LC:/Users/brill/Documents/R/win-library/4.1/RcppParallel/lib/x64 -ltbb -ltbbmalloc -lm -llibmecab -LC:/PROGRA1/R/R-411.2/bin/x64 -lRinstalling to C:/Users/brill/Documents/R/win-library/4.1/00LOCK-RcppMeCab/00new/RcppMeCab/libs/x64 R inst byte-compile and prepare package for lazy loading help ** installing help indices converting help for package ‘RcppMeCab’ finding HTML links … done RcppMeCab html pos html posParallel html building package indices testing if installed package can be loaded from temporary location testing if installed package can be loaded from final location testing if installed package keeps a record of temporary installation path DONE (RcppMeCab) RcppMeCab 설치 확인 (형태소 분리기)text 1에 한글을 써 본다. 12text1 = &quot;안녕하세요?!&quot;pos(sentence = text1) text1 = “안녕하세요?!”pos(sentence = text1)$�ȳ\\xe7\\xc7ϼ��\\xe4?![1] “�/SY” “ȳ/SL” “\\xe7\\xc7\\xcf/SH”[4] “���/SY” “\\xe4?!/SH” - 인코딩이 UTF-8로 되어 있지 안아서 생기는 문제이다. 12text2 = enc2utf8(text1)pos(sentence = text2) text2 = enc2utf8(text1) pos(sentence = text2) $안녕하세요?! [1] “안녕/NNG” “하/XSV” “세요/EP+EF” “?/SF” “!/SF” 강사님 도움 받기강사님 강의 듣기 페이가 안맞아서 그런가 우리 수업에서는 이렇게 안해준다. 못가르치는 것이 아니라 안가르치는 것이어서 화가 나지만, 각자의 사정이 있는것이겠지. 나도 국비 과정 들으면서 너무 많은 것을 바란건 아닌지 생각 해 본다. 설치/ 확인 끝","link":"/2021/12/14/R/textmining(02)/"},{"title":"Text Mining in R","text":"Text Mining in R (03)앞선 내용 :Text Mining in R (01): library(KoNLP), useNIADic() 사용/설치 확인Text Mining in R (02)Rcppmecab 설치, 확인다음 내용 :Lecture 앞서서 설치한 files 바탕으로 TextMining을 해 보자.data 수집 data 전처리 cheatsheets Tokenizetidytextmining 1install.packages(&quot;tidytext&quot;)","link":"/2021/12/15/R/textmining(03)/"},{"title":"MarkDown Table, hr, &#96;code&#96;","text":"MarkDown Table마크다운 문법에서 테이블 그리기 pipe로 table을 쉽게 생성 할 수 있다. 아래 표는table ref.에서 가져온 것. 123456| 종류 | 기호 | ex | exp ||:-----------:|:-------------:|:-------------:|:------:|| assignment | = | a = 10 | 10에 a 를 바인딩 || assignmented assignment | **=, +=, -=, *=, //=, %=. &lt;&lt;=, &gt;&gt;=, &amp;=, &amp;#124;=, ^=, @= | a+= 10 |a에 10을 더한 결과 객체에 a를 바인딩 | 대충 칸에 맞게 순서만 맞으면아래와 같이 예쁜 표를 만들 수 있다. 종류 기호 ex exp assignment = a = 10 10에 a 를 바인딩 assignmented assignment **=, +=, -=, *=, //=, %=. &lt;&lt;=, &gt;&gt;=, &amp;=, |=, ^=, @= a+= 10 a에 10을 더한 결과 객체에 a를 바인딩 table 안에 정렬1234&lt;!-- 하이픈 갯수로 크기 조절 --&gt;|왼쪽정렬 | 중간정렬|오른쪽정렬||:---------|:---------------:|---------:| 왼쪽정렬 중간정렬 오른쪽정렬 왼쪽정렬 중간정렬 오른쪽정렬 pipe 보이기pipe는 Enrer Key 위에 있다. 1&amp;#124 하지만, Markdown형식에서 보이지 않으닌까 \\ | 만약 사용 하고 싶으면, &amp;#124 를 사용 하도록 한다. ^0^ table만드는 것이 귀찮다면 아래 링크를 타고 가보쟈 table 쉽게 만드는 곳 code 강조하기inline code 강조하는 방법 1234&lt;!--Tab key 위쪽, 1 왼쪽에 위치한 Grave 키 이용--&gt;`code` code 이것이 가능하다니, 이제야 알았다. !!! 수평선12345678---(hyphen X 3)***(asterisk X 3)___(Underscore X 3) (hyphen X 3) (asterisk X 3) (Underscore X 3) Ref.","link":"/2021/12/06/Markdown/CreateTable/"},{"title":"Test page","text":"Hello world https://velog.io/@kwonhl0211/Hello-Kaggle-%EC%BA%90%EA%B8%80%EC%9D%B4-%EC%B2%98%EC%9D%8C%EC%9D%B8-%EB%B6%84%EB%93%A4%EC%9D%84-%EC%9C%84%ED%95%9C-%EC%BA%90%EA%B8%80-%EA%B0%80%EC%9D%B4%EB%93%9C kaggle guide ko!! https://medium.com/@kaggleteam/how-to-get-started-with-data-science-in-containers-6ed48cb08266 Kaggle note랑 연동 하는 방법이 나와 있는듯나중에 Posting 해 봐야지 ^^ https://www.chartjs.org/docs/latest/samples/bar/horizontal.html chart에대해 많은 List가 있는데 완전 유용할듯 neo4j 를 이용하여 graph 만들수 있다.neo4j git blog: Hexo로 multi, push1234567git config --global user.email &quot;ssiasoda@gmil.com&quot;git config --global user.name &quot;YoonHwa-P&quot;git push origin HEAD:main 내가 push 할때 쓰려고 저장 Kaggle/competitionskaggle competition에 참가 할 수 있다. 1!pip install kaggle Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10) 12345678910from google.colab import filesuploaded = files.upload()for fn in uploaded.keys(): print('User uploaded file &quot;{name}&quot; with length {length} bytes'.format( name=fn, length=len(uploaded[fn]))) # Then move kaggle.json into the folder where the API expects to find it.!mkdir -p ~/.kaggle/ &amp;&amp; mv kaggle.json ~/.kaggle/ &amp;&amp; chmod 600 ~/.kaggle/kaggle.json Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) &lt;ipython-input-57-5e2b5f92ba05&gt; in &lt;module&gt;() 1 from google.colab import files 2 ----&gt; 3 uploaded = files.upload() 4 5 for fn in uploaded.keys(): /usr/local/lib/python3.7/dist-packages/google/colab/files.py in upload() 62 result = _output.eval_js( 63 'google.colab._files._uploadFiles(&quot;{input_id}&quot;, &quot;{output_id}&quot;)'.format( ---&gt; 64 input_id=input_id, output_id=output_id)) 65 files = _collections.defaultdict(_six.binary_type) 66 # Mapping from original filename to filename as saved locally. /usr/local/lib/python3.7/dist-packages/google/colab/output/_js.py in eval_js(script, ignore_result, timeout_sec) 38 if ignore_result: 39 return ---&gt; 40 return _message.read_reply_from_input(request_id, timeout_sec) 41 42 /usr/local/lib/python3.7/dist-packages/google/colab/_message.py in read_reply_from_input(message_id, timeout_sec) 99 reply = _read_next_input_message() 100 if reply == _NOT_READY or not isinstance(reply, dict): --&gt; 101 time.sleep(0.025) 102 continue 103 if (reply.get('type') == 'colab_reply' and KeyboardInterrupt: 데이터 다운로드 및 불러오기1!kaggle competitions list Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4) ref deadline category reward teamCount userHasEntered --------------------------------------------- ------------------- --------------- --------- --------- -------------- contradictory-my-dear-watson 2030-07-01 23:59:00 Getting Started Prizes 63 False gan-getting-started 2030-07-01 23:59:00 Getting Started Prizes 81 False store-sales-time-series-forecasting 2030-06-30 23:59:00 Getting Started Knowledge 487 False tpu-getting-started 2030-06-03 23:59:00 Getting Started Knowledge 157 False digit-recognizer 2030-01-01 00:00:00 Getting Started Knowledge 1459 False titanic 2030-01-01 00:00:00 Getting Started Knowledge 14879 False house-prices-advanced-regression-techniques 2030-01-01 00:00:00 Getting Started Knowledge 4418 True connectx 2030-01-01 00:00:00 Getting Started Knowledge 263 False nlp-getting-started 2030-01-01 00:00:00 Getting Started Knowledge 1321 False competitive-data-science-predict-future-sales 2022-12-31 23:59:00 Playground Kudos 12891 False g-research-crypto-forecasting 2022-02-01 23:59:00 Featured $125,000 148 False petfinder-pawpularity-score 2022-01-13 23:59:00 Research $25,000 1631 False optiver-realized-volatility-prediction 2022-01-10 23:59:00 Featured $100,000 3852 False nfl-big-data-bowl-2022 2022-01-06 23:59:00 Analytics $100,000 0 False sartorius-cell-instance-segmentation 2021-12-30 23:59:00 Featured $75,000 495 False wikipedia-image-caption 2021-12-09 11:59:00 Playground Swag 71 False lux-ai-2021 2021-12-06 23:59:00 Featured $10,000 928 False tabular-playground-series-nov-2021 2021-11-30 23:59:00 Playground Swag 352 False kaggle-survey-2021 2021-11-28 23:59:00 Analytics $30,000 0 False chaii-hindi-and-tamil-question-answering 2021-11-15 23:59:00 Research $10,000 807 False 1!kaggle competitions download -c house-prices-advanced-regression-techniques User cancelled operation 1234import pandas as pd train = pd.read_csv('train.csv')test = pd.read_csv('test.csv')print('Data Loading is done!') Data Loading is done! 데이터 둘러보기12print(&quot;The shape of Train Data is:&quot;, train.shape)print(&quot;The shape of Test Data is:&quot;, test.shape) The shape of Train Data is: (1460, 81) The shape of Test Data is: (1459, 80) 1print(train.info()) &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 1460 entries, 0 to 1459 Data columns (total 81 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Id 1460 non-null int64 1 MSSubClass 1460 non-null int64 2 MSZoning 1460 non-null object 3 LotFrontage 1201 non-null float64 4 LotArea 1460 non-null int64 5 Street 1460 non-null object 6 Alley 91 non-null object 7 LotShape 1460 non-null object 8 LandContour 1460 non-null object 9 Utilities 1460 non-null object 10 LotConfig 1460 non-null object 11 LandSlope 1460 non-null object 12 Neighborhood 1460 non-null object 13 Condition1 1460 non-null object 14 Condition2 1460 non-null object 15 BldgType 1460 non-null object 16 HouseStyle 1460 non-null object 17 OverallQual 1460 non-null int64 18 OverallCond 1460 non-null int64 19 YearBuilt 1460 non-null int64 20 YearRemodAdd 1460 non-null int64 21 RoofStyle 1460 non-null object 22 RoofMatl 1460 non-null object 23 Exterior1st 1460 non-null object 24 Exterior2nd 1460 non-null object 25 MasVnrType 1452 non-null object 26 MasVnrArea 1452 non-null float64 27 ExterQual 1460 non-null object 28 ExterCond 1460 non-null object 29 Foundation 1460 non-null object 30 BsmtQual 1423 non-null object 31 BsmtCond 1423 non-null object 32 BsmtExposure 1422 non-null object 33 BsmtFinType1 1423 non-null object 34 BsmtFinSF1 1460 non-null int64 35 BsmtFinType2 1422 non-null object 36 BsmtFinSF2 1460 non-null int64 37 BsmtUnfSF 1460 non-null int64 38 TotalBsmtSF 1460 non-null int64 39 Heating 1460 non-null object 40 HeatingQC 1460 non-null object 41 CentralAir 1460 non-null object 42 Electrical 1459 non-null object 43 1stFlrSF 1460 non-null int64 44 2ndFlrSF 1460 non-null int64 45 LowQualFinSF 1460 non-null int64 46 GrLivArea 1460 non-null int64 47 BsmtFullBath 1460 non-null int64 48 BsmtHalfBath 1460 non-null int64 49 FullBath 1460 non-null int64 50 HalfBath 1460 non-null int64 51 BedroomAbvGr 1460 non-null int64 52 KitchenAbvGr 1460 non-null int64 53 KitchenQual 1460 non-null object 54 TotRmsAbvGrd 1460 non-null int64 55 Functional 1460 non-null object 56 Fireplaces 1460 non-null int64 57 FireplaceQu 770 non-null object 58 GarageType 1379 non-null object 59 GarageYrBlt 1379 non-null float64 60 GarageFinish 1379 non-null object 61 GarageCars 1460 non-null int64 62 GarageArea 1460 non-null int64 63 GarageQual 1379 non-null object 64 GarageCond 1379 non-null object 65 PavedDrive 1460 non-null object 66 WoodDeckSF 1460 non-null int64 67 OpenPorchSF 1460 non-null int64 68 EnclosedPorch 1460 non-null int64 69 3SsnPorch 1460 non-null int64 70 ScreenPorch 1460 non-null int64 71 PoolArea 1460 non-null int64 72 PoolQC 7 non-null object 73 Fence 281 non-null object 74 MiscFeature 54 non-null object 75 MiscVal 1460 non-null int64 76 MoSold 1460 non-null int64 77 YrSold 1460 non-null int64 78 SaleType 1460 non-null object 79 SaleCondition 1460 non-null object 80 SalePrice 1460 non-null int64 dtypes: float64(3), int64(35), object(43) memory usage: 924.0+ KB None 1print(test.info()) &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 1459 entries, 0 to 1458 Data columns (total 80 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Id 1459 non-null int64 1 MSSubClass 1459 non-null int64 2 MSZoning 1455 non-null object 3 LotFrontage 1232 non-null float64 4 LotArea 1459 non-null int64 5 Street 1459 non-null object 6 Alley 107 non-null object 7 LotShape 1459 non-null object 8 LandContour 1459 non-null object 9 Utilities 1457 non-null object 10 LotConfig 1459 non-null object 11 LandSlope 1459 non-null object 12 Neighborhood 1459 non-null object 13 Condition1 1459 non-null object 14 Condition2 1459 non-null object 15 BldgType 1459 non-null object 16 HouseStyle 1459 non-null object 17 OverallQual 1459 non-null int64 18 OverallCond 1459 non-null int64 19 YearBuilt 1459 non-null int64 20 YearRemodAdd 1459 non-null int64 21 RoofStyle 1459 non-null object 22 RoofMatl 1459 non-null object 23 Exterior1st 1458 non-null object 24 Exterior2nd 1458 non-null object 25 MasVnrType 1443 non-null object 26 MasVnrArea 1444 non-null float64 27 ExterQual 1459 non-null object 28 ExterCond 1459 non-null object 29 Foundation 1459 non-null object 30 BsmtQual 1415 non-null object 31 BsmtCond 1414 non-null object 32 BsmtExposure 1415 non-null object 33 BsmtFinType1 1417 non-null object 34 BsmtFinSF1 1458 non-null float64 35 BsmtFinType2 1417 non-null object 36 BsmtFinSF2 1458 non-null float64 37 BsmtUnfSF 1458 non-null float64 38 TotalBsmtSF 1458 non-null float64 39 Heating 1459 non-null object 40 HeatingQC 1459 non-null object 41 CentralAir 1459 non-null object 42 Electrical 1459 non-null object 43 1stFlrSF 1459 non-null int64 44 2ndFlrSF 1459 non-null int64 45 LowQualFinSF 1459 non-null int64 46 GrLivArea 1459 non-null int64 47 BsmtFullBath 1457 non-null float64 48 BsmtHalfBath 1457 non-null float64 49 FullBath 1459 non-null int64 50 HalfBath 1459 non-null int64 51 BedroomAbvGr 1459 non-null int64 52 KitchenAbvGr 1459 non-null int64 53 KitchenQual 1458 non-null object 54 TotRmsAbvGrd 1459 non-null int64 55 Functional 1457 non-null object 56 Fireplaces 1459 non-null int64 57 FireplaceQu 729 non-null object 58 GarageType 1383 non-null object 59 GarageYrBlt 1381 non-null float64 60 GarageFinish 1381 non-null object 61 GarageCars 1458 non-null float64 62 GarageArea 1458 non-null float64 63 GarageQual 1381 non-null object 64 GarageCond 1381 non-null object 65 PavedDrive 1459 non-null object 66 WoodDeckSF 1459 non-null int64 67 OpenPorchSF 1459 non-null int64 68 EnclosedPorch 1459 non-null int64 69 3SsnPorch 1459 non-null int64 70 ScreenPorch 1459 non-null int64 71 PoolArea 1459 non-null int64 72 PoolQC 3 non-null object 73 Fence 290 non-null object 74 MiscFeature 51 non-null object 75 MiscVal 1459 non-null int64 76 MoSold 1459 non-null int64 77 YrSold 1459 non-null int64 78 SaleType 1458 non-null object 79 SaleCondition 1459 non-null object dtypes: float64(11), int64(26), object(43) memory usage: 912.0+ KB None","link":"/2021/10/28/blog/1028/"},{"title":"become a BDS","text":"##Big data 를 이용 할 수 있는 BioData Scientist가 되고 싶다. 생물학은 점점 디지털화되고 있으며 이제 양적인 과학 분야의 빛을 발하고 있다. 핵심 추진 요인은 생물학적 연구에서 처리량이 높은 기술 플랫폼의 확산이 증가하는 것으로,체계적인 연구를 위해 수천 개의 조직과 유기체에 걸친유전자, 단백질 및 기타 생물학적 부분에 대한 수백만 개의 데이터 포인트가수집, 세척, 저장 및 통합될 수 있도록 한다. 이처럼 데이터가 풍부한 환경에서 생물학적(그리고 임상 샘플에 배치된 경우 생물의학)연구의 미래는 데이터의 전략적 극대화 에 있다고 해도 과언이 아니다. 오늘날의 기술 환경에서 데이터 과학 및 인공지능(AI)은이미 비즈니스 및 금융과 같은 영역에서 혁신 동력으로 작용하고 있다.여기서 데이터 과학자는 막후에서 작업하는 대신 데이터를 실질적인통찰력으로 변환하는 역할을 담당하고 있다. 예를 들어, 인공지능 기반 알고리즘 거래와 금융 기술(FinTech)의 주식 추천 시스템,엔지니어링의 자동화 엔진 설계, 시스템 유지보수 및 로봇공학 등이 있다.최근의 데이터 폭발과 이에 따른 비즈니스, 금융 및 컴퓨팅과 같은다른 분야의 데이터 과학의 발전을 감안할 때, 우리는 특히 생물학과 관련된 영역별문제를 다루는 새로운 변형인 빠르고 방대한 양의 데이터 생성과 함께 데이터 과학이등장할 것으로 예상한다. 이를 “바이오 데이터 과학” 이라고 한다. ###BDS(BioData Science)에는 세 가지 핵심 분야가 있다. 생물학 영역: Biology 수학 및 통계 : mathematics (statistics) 컴퓨터 공학 : computer science 생물학 영역은 질병의 원인이나 유추된 바이오 마커의 진단 효용 이해와 같은 생물학적 기원에 대한 질문과 관련이 있다. 컴퓨터 과학 코어는 특히 분석할 데이터가 큰 경우 문제 해결을 위한 적절한알고리즘 고안, 반복 처리 (예: 데이터의 큰 부분 집합에서동일한 알고리즘 여러 번 실행) 및 데이터 저장 문제 해결과 관련이 있다. 수학 및 통계 핵심 영역은 데이터 요약, 정규화 및 모델링을 포함한 문제와 관련이 있다.기술 및 탐색적 통계 데이터 분석이 BDS에만 국한된 것은 아니지만(생물 통계의 필수 구성 요소이기도 하며, 더 낮은 정도로 생물 정보학이기도 함),BDS는 빅데이터에 AI/ML을 적용하는 것에 기초한 신흥 기술을 이용한 예측에 초점을 맞추고 있다. ###바이오 데이터 과학은 다른 과학 분야와 다르지 않은 탐구 과학이다.BDS는 단순히 기술(technology), 기계 학습(machine learning) 인공지능(AI)이 아니다. 인공지능이 여러분을 위해 그렇게 해주기를 바라는 대신, 사람의 강한 논리적 사고에 기초해야 한다.BDS가 궁극적으로 연구의 과학이라는 이런 점에서, 전형적인 과학적 조사와 다르지 않다. 우리는 유전자 발현 변화가 정신 상태와 의미있게 상관되는지에 대한 질문에 답하도록 돕기 위해다음의 7가지 단계를 사용할 수 있다. 가장 큰 차이점은 BDS는 낮은 처리량 또는 저전력 물리적 실험에 대한 강조를 줄이면서의미 있는 데이터 조작 및 분석에 강력한 능력을 필요로 한다는 것이다. 데이터 과학은 다른 과학적 추구와 같은 과정으로 진행 된다. - 조사할 질문을 먼저 선택 - 테스트 가능한 관련 가설을 확인함으로써 이 질문의 범위를 넓힘 - 가설에 답하기 위한 데이터를 얻기 위한 적절한 실험을 설계하고 현장 적용할 - 결과를 결정하고 그 타당성, 즉 데이터가 연구 질문에 답하는 데 적합한지 여부를 평가 - 마지막으로, 모델을 배치하고 연구 결과가 반복 가능한지 확인 ###데이터 분석은 복잡한 다단계 프로세스이다.**BDS(BioData Science)**는 도전적인 분야이지만, 생물정보학이나 전산생물학과 비슷하게 어렵다. 생물학적 시스템을 측정하기 위한 기술적 플랫폼은 매우 정교하지만, 생물학적 시스템은 매우 복잡하다.게다가 생물학적 실체를 측정하기 위해 개발된 기술적 도구는 생물학적 시스템의구성요소가 변화하고 시간이 지남에 따라 자연스럽게 변화하는 동안 기술적 불확실성에 영향을 받는다.바이오 빅 데이터는 이러한 문제에 대한 자연스러운 솔루션이 아니며 새로운 문제를 야기한다. BDS 데이터는 매우 많은 수의 관측에서 보존된 패턴을 식별하는 과정과 같은 데이터 과학노력을 촉진할 수 있지만, 적절한 분석 파이프라인이 개발될 경우에만 그렇게 할 수 있다.이 작업은 하찮지 않다. 이러한 분석 파이프라인은 데이터 수집에서 시작하여더 높은 수준의 생물학적 해석과 통찰력을 향한 계산 및 통계 평가를 통해 계속 이어지는다양한 접근 방식의 엔드 투 엔드 통합으로 상상할 수 있다. omics 데이터의 바이오 마커 분석을 위한 단순화된 파이프라인과 관련 주요 고려 사항은 다음과 같을 수 있다. 분석 파이프라인은 매우 유연해야 하며 연구 질문의 필요에 따라 변화해야 한다.완벽한 지식이 부족하기 때문에 최적화와 재현성을 어느 정도 달성하기 위해 몇 단계를 왔다 갔다반복하고 다듬는 것도 일반적이다. 예를 들어, 정규화 단계에서 두 개의 서로 다른 정규화 절차를사용하여 매우 다르고 겹치지 않는 차등 유전자 세트를 발견했다고 가정 해 보자.정규화 절차는 데이터에 대해 잘못된 가정을 하거나 잘못 구현되었을 수 있다.표시된 주요 고려사항은 엄청나게 많다. 고려사항의 예와 함께 단계를 보여주는 목적은각 단계마다 완벽한 시스템이나 파이프라인이 없지만, 각 의사결정 지점마다 이후 단계에 대한결과를 갖는 많은 고려사항이 있음을 입증하는 것이다. 우리는 또한 특정 정규화 접근법이 다운스트림 통계 절차와 잘 작동하는지와 같은호환성 문제나 특정 절차가 배치 효과 보정 알고리즘 및 일부 다중 시험 보정 방법과 관련된과다 탈락 및 과다 첨가로 이어질 수 있는지 여부와 같은 문제를 걱정해야 한다. 일반적으로 좋은 결과를 보장하는 노선도나 표준 운영 절차는 없다는 점에서BDS는 예술에 가까운 과학인 것이다. Ref.https://gohwils.github.io/biodatascience/biodatascience.html","link":"/2021/11/04/blog/BDS/"},{"title":"decisionTree Classifier()","text":"DecisionTreeClassifierClassifier functionDecision Tree Classifier는 데이터 집합에서 다중 클래스 분류를 수행할 수 있는 클래스이다. 다른 분류자와 마찬가지로 Decision Tre Classifier는 훈련 샘플을 고정하는 배열 X(n_샘플, n_특징)와 훈련 샘플에 대한 클래스 레이블을 고정하는 정수 값, 형상(n_샘플, n_특징)의 배열 Y의 두 배열을 입력으로 사용합니다. 12345from sklearn import treeX = [[0, 0], [1, 1]]Y = [0, 1]clf = tree.DecisionTreeClassifier()clf = clf.fit(X, Y) 이제 예측 모델을 만들어 봅시다. 12clf.predict([[2., 2.]])clf.predict_proba([[2., 2.]]) array([[0., 1.]]) 의사결정트리의 분류는 classification과 multiclass 양쪽으로 모두 분류 할 수 있다. iris dataset을 하용하면, 우리는 아래와 같은 plot_Tree를 만들 수 잇다. 1234567from sklearn.datasets import load_irisfrom sklearn import treeiris = load_iris()x, y = iris.data, iris.targetclf = tree.DecisionTreeClassifier()clf = clf.fit(x, y)tree.plot_tree(clf) [Text(167.4, 199.32, 'X[2] &lt;= 2.45\\ngini = 0.667\\nsamples = 150\\nvalue = [50, 50, 50]'), Text(141.64615384615385, 163.07999999999998, 'gini = 0.0\\nsamples = 50\\nvalue = [50, 0, 0]'), Text(193.15384615384616, 163.07999999999998, 'X[3] &lt;= 1.75\\ngini = 0.5\\nsamples = 100\\nvalue = [0, 50, 50]'), Text(103.01538461538462, 126.83999999999999, 'X[2] &lt;= 4.95\\ngini = 0.168\\nsamples = 54\\nvalue = [0, 49, 5]'), Text(51.50769230769231, 90.6, 'X[3] &lt;= 1.65\\ngini = 0.041\\nsamples = 48\\nvalue = [0, 47, 1]'), Text(25.753846153846155, 54.359999999999985, 'gini = 0.0\\nsamples = 47\\nvalue = [0, 47, 0]'), Text(77.26153846153846, 54.359999999999985, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 1]'), Text(154.52307692307693, 90.6, 'X[3] &lt;= 1.55\\ngini = 0.444\\nsamples = 6\\nvalue = [0, 2, 4]'), Text(128.76923076923077, 54.359999999999985, 'gini = 0.0\\nsamples = 3\\nvalue = [0, 0, 3]'), Text(180.27692307692308, 54.359999999999985, 'X[2] &lt;= 5.45\\ngini = 0.444\\nsamples = 3\\nvalue = [0, 2, 1]'), Text(154.52307692307693, 18.119999999999976, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 2, 0]'), Text(206.03076923076924, 18.119999999999976, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 1]'), Text(283.2923076923077, 126.83999999999999, 'X[2] &lt;= 4.85\\ngini = 0.043\\nsamples = 46\\nvalue = [0, 1, 45]'), Text(257.53846153846155, 90.6, 'X[1] &lt;= 3.1\\ngini = 0.444\\nsamples = 3\\nvalue = [0, 1, 2]'), Text(231.7846153846154, 54.359999999999985, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 0, 2]'), Text(283.2923076923077, 54.359999999999985, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1, 0]'), Text(309.04615384615386, 90.6, 'gini = 0.0\\nsamples = 43\\nvalue = [0, 0, 43]')] decision tree는 약 20여종의 parameter가 있다. Parameter criterion : 분할 품질을 측정하는 기능 (default : gini) splitter : 각 노드에서 분할을 선택하는 데 사용되는 전략 (default : best) max_depth : 트리의 최대 깊이 (값이 클수록 모델의 복잡도가 올라간다.) min_samples_split : 자식 노드를 분할하는데 필요한 최소 샘플 수 (default : 2) min_samples_leaf : 리프 노드에 있어야 할 최소 샘플 수 (default : 1) min_weight_fraction_leaf : min_sample_leaf와 같지만 가중치가 부여된 샘플 수에서의 비율 max_features : 각 노드에서 분할에 사용할 특징의 최대 수 random_state : 난수 seed 설정 max_leaf_nodes : 리프 노드의 최대수 min_impurity_decrease : 최소 불순도 min_impurity_split : 나무 성장을 멈추기 위한 임계치 class_weight : 클래스 가중치 presort : 데이터 정렬 필요 여부 12sklearn.tree.plot_tree(decision_tree, *, max_depth=None, feature_names=None, class_names=None, label='all', filled=False, impurity=True, node_ids=False, proportion=False, rounded=False, precision=3, ax=None, fontsize=None)tree.plot_tree() 가장 원시적이면서 기본적인 parameter가 있는 decision tree의 parameter를 외우기 보다 지금 어떤 형태로 코드가 들어 가는지에 집중하자. 왜냐하면, 최신의 버전은 decision tree가 아니기 때문. 123456from sklearn.datasets import load_irisfrom sklearn import treeclf = tree.DecisionTreeClassifier(random_state=0)iris = load_iris()","link":"/2021/11/04/blog/DecisionTreeClassifier/"},{"title":"Google Colaboratory file을 GitHube에 Upload하기","text":"Google Colaboratory file을 GitHub에 바로 올리기 google Colaboratory란? google Colaboratory (G-CLB, 구글 코랩)로 작업 했다면 이 file을 바로 gitHub blog에 올리고 싶을 것이다.우리는 다음과 같은 과정을 통해 G-CLB file을 다른 변환 과정 없이 file 째로 올릴 수 있다. 우리에게 필요한 app은 ‘아나콘다’ 이다. Anaconda의 JupyterLab을 이용하여 변환 해 봅시다. 아나콘다의 주피터랩&gt; file Tab에서 Export Note로 간다 MarkDown file으로 다운 받아서 _post 경로에 넣어주면 완성 !!! 다른 방법도 있다. !!","link":"/2021/11/01/blog/GC_Upload_GH/"},{"title":"Index","text":"GitHub Index made By @YoonHwa-P Python python, Tuple * python, List python, Matplotlib python, Pandas pandas, series pandas, dataframe * python, Numpy python, basic 머신러닝 (의사결정트리) 의사결정트리, Classification 의사결정트리, 이론 python, plotly Kaggle_typical Kaggle_Africa DBS DBS, 이론 논문리뷰, *github girhub, nodjs github, 카테고리 github, Multi jupyterExport(.MD만들기) github, blog 만들기 github, Repository 만들기 올려야 하는 Posting List 의사결정트리, Regression seaborn matplotlib 수정 해야 할것도 넘넘 많다.일단 이번 주 주말에는 Kaggle 먼저 (Team project 닌까.) * : 수정중 123&lt;hr style=&quot;dashed 10px pink;&quot;&gt;&lt;hr align=&quot;center&quot; style=&quot;border: solid 10px #304A84; width: 50%;&quot;&gt;&lt;span style=&quot;font-size:150%; color: Red;&quot;&gt; * &lt;/span&gt; ※ 참고 할 Github ㅁ https://github.com/dschloe/R_edu ㅁ https://github.com/JunghoGIT ㅁ https://github.com/kimgoden/JAVA01 ㅁ https://github.com/WDWDWWDff/Red ㅁ https://github.com/OliverKang ㅁ https://github.com/hanbeen1992 ㅁ https://github.com/kjw1390","link":"/2021/11/05/blog/Index/"},{"title":"Make a gitHub","text":"학원에서 배운 코드들을 올려봅시다. 깃허브 디자인 더 하고 싶은데 안되넹 흐흐 오늘 한 내용 정리 + slack join (green_702) + anaconda install + system Path (환경변수 설정_자동) + pycham install + gitHub blog 만들기 (회원가입 & repository 생성) + gitHub에 repository 만들고, file UPloading 하기_01 1. gitHub에서 repository 만들기 클릭 2. 이름 red로 설정후 하단에 add a README.file 체크 3. 바탕화면에서 우측클릭후 git bash here 선택 4. 링크로 연결 : git clone https://github.com/각자계정/red.git + gitHub에 repository 만들고, file UPloading 하기_02 1. file 만들기(Blue) 2. GitHub에 repository 만들기(Blue); 이름이 같지 않으면 안됨 3. 원하는 file 넣기 (folder 가능) + git 명령어 + terminal Tap에 Git Bash를 활성화하여 명령어 입력 $ git init >$ git init_ git을 initiation 해 준다. (초기화) ![img.png](..\\imeges\\Make_gitHub\\img.png) 맨 처음에만 해 주면 된다. $ git add . > $ git add : 지금 Update한 data를 서버에 올려준다. ![img_1.png](..\\imeges\\Make_gitHub\\img_1.png) $ git add . : 경로에 있는 모든 file Upload $ git add [filename.Ex] : [file 이름과 확장자] 특정 file 을 저장 $ git commit - m \"Comment(History log)\" > $ git commit - m \"Comment(History log)\" ![img_2.png](..\\imeges\\Make_gitHub\\img_2.png) commit하여 확정 해 준다. 5개의 sql files가 올라간 것을 볼 수 있다. ![img_3.png](..\\imeges\\Make_gitHub\\img_3.png) 확정 해 주고 마지막으로 push 해 주면 다음과같은 History Log를 github main에서 볼 수 있다. $ git status $git status 깃허브 commit 하기 전에 올릴 파일이 있는지 등의 상태를 알아 볼 수 있다. commit 이후에는 사라지나부다 사진이 없다. 올리기 전에 급하게 한컷 찍어 보앗쥐 커밋해야하는 상태를 보여준다. $ git push > $ git push 최종 브라우저에 저장. ![img_4.png](..\\imeges\\Make_gitHub\\img_4.png) $ git push 후 main -> main 이 나오면 성공 ! > Ref. *파이참, 아나콘다 DownLoad ㅁ https://www.anaconda.com/products/individual#Downloads ㅁ https://www.jetbrains.com/pycharm/download/ ㅁ https://git-scm.com/ Ref..https://80000coding.oopy.io/865f4b2a-5198-49e8-a173-0f893a4fed45 ㄴ&gt; 깃허브 꾸미기 ++앞으로 참고 하고 싶은 github++ㅁ https://github.com/dschloe/R_edu ㅁ https://github.com/JunghoGIT ㅁ https://github.com/kimgoden/JAVA01 ㅁ https://github.com/WDWDWWDff/Red ㅁ https://github.com/OliverKang ㅁ https://github.com/hanbeen1992 ㅁ https://github.com/kjw1390","link":"/2021/10/29/blog/Make_gitHub/"},{"title":"Make a gitHub Draft","text":"Github blog에 초안(draft) 작성하기 ref.","link":"/2021/11/12/blog/Make_gitHub_draft/"},{"title":"Use Notion","text":"Notion이 그렇게 편하다며?Notion 을 이용하여 github blog 를 편하게 쓸수 있다고 한다.어쩐지 Naver Blog 는 쓰기 넘넘 쉬운데 Github blog 넘 쓰기 힘들었다. 특히 사진을 넣는 부분 ㅠㅠ진짜 넘 힘들었는데 이제는 자신있다 ! Notion을 사용 해 보자아마도 이 blog posting이 pycham을 이용하는 마지막 posting이 되지 않을까 싶다. 마지막 posting을 화려하게 마무리 해 보자. Notion에 들어가서 google account or 본인이 원하는 email을 사용하여 회원가입을 한다. 나의 경우에는 gmail(google 계정) 이 있었기 때문에 클릭 2번으로 회원가입 완료 시작 하기에 대부분의 내용이 나와 있기 때문에 이 부분을 보면 된다. 페이지 추가하기를 누르면 페이지가 추가 되고, 페이지의 하위페이지도 추가 할 수 있다. 커버 사진도 손쉽게 변경 할 수 있고, 명령어 사용을 위해 /을 입력하면 원하는 명령어가 리스트로 나온다. 사진의 경우 네이버 처럼 Ctrl + C, Ctrl + V, 즉 복붙을 통해 편리하게 넣을 수 있다. 나의 경우 Image를 Ctrl + C, Ctrl + V,를 통해 가져와서 pycham 에서 directory를 image file 로 옮기면서 이름을 변경하는 형식으로 진행했다. 사실 포스팅 하나마다 포스팅 directory를 이용하면, pycham으로 작성하는 것이 더 쉬울 수도 … Mark Down 문서를 작성하기 가장 쉬운 곳은 visual studio code 라고 한다. Notion! 결론은 ?directory를 source안에 많이 만드는 것이 훨씬 쉽게 작성 하는 방법 인것 같다. 예쁘게 만들고 싶다면 처음부터 Notion을 사용 하는 것이 좋을 듯 ! 내가 정리 한 글이 마음에 들지 않는다면, 노션 가이드북을 참고 하는 방법도 있다.","link":"/2022/01/20/blog/Notion(01)/"},{"title":"Study Numpy","text":"Numpy Numpy 정의NumPy는 행렬이나 일반적으로 대규모 다차원 배열을 쉽게 처리 할 수 있도록 지원하는 파이썬의 라이브러리Numpy는 데이터 구조 외에도 수치 계산을 위해 효율적으로 구현된 기능을 제시 한다. Ref. Wiki 라이브러리에서 numpy 불러오기우리는 numpy를 import 하여 numpy에 내장되어 있는 함수를 가져와 쓸 수 있다. 일반적으로 np에 저장하여 많이 사용 하는 듯 하다. 123456789import numpy as npprint(np.__version__)print (&quot;Numpy의 version을 확인 해 볼 수 있다. &quot;)temp = np.array([1, 2, 3])print(type(temp))print (&quot;Numpy의 type은 nd array인 것을 볼 수 있다. &quot;) 1.19.5 Numpy의 version을 확인 해 볼 수 있다. &lt;class 'numpy.ndarray'&gt; Numpy의 Type을 보면 nd array 인 것을 볼 수 있는데 ND : N dimension 을 의미한다.즉 한국어로 번역 해 보면 N차 행렬 정도로 볼 수 있다. Numpy 에서 배열을 생성 해 보자.1차원 배열 생성1차원의 배열을 생성해서 array와 List의 다른 점을 알아보자. 차이점은 shpae를 찍어 보면 알 수 있다.내장 함수 : (fuction or method or attribute) 123456789101112131415161718data1 = [0, 1, 2, 3, 4]data2 = [10, 9, 8, 7, 6]My_array = np.array(data1)print(&quot;data1은 List이다. &quot;)print(data1)print(type(data1))#print(data1.shape) #List의 경우 shape 함수가 내장 되어 있지 않아 shape를 알 수 없다. print(&quot;My_array은 numpy형식의 tuple인 것을 알 수 있다. &quot;)print(My_array)print(My_array.shape)print(type(My_array))print(&quot;.dtype() 는 data의 type을 확인 할 수 있는 function 이다.&quot;)print(My_array.dtype) data1은 List이다. [0, 1, 2, 3, 4] &lt;class 'list'&gt; My_array은 numpy형식의 tuple인 것을 알 수 있다. [0 1 2 3 4] (5,) &lt;class 'numpy.ndarray'&gt; .dtype() 는 data의 type을 확인 할 수 있는 function 이다. int64 List 형식의 경우 shape 함수가 내장 되어 있지 않은 반면,numpy 형식의 np.array 의 경우 tuple shape 함수가 내장 되어 에러가 나지 않과 (5, )의 형태로 result가 나오는 것 을 볼 수 있다. .dtype() 는 data의 type을 확인 할 수 있는 function 이다.이때 나타나는 int 64는 64byte의 타입 이라는 것을 알려 준다. https://rfriend.tistory.com/285 2차원 배열 생성4 X 3 배열을 만들어 보자. 1234my_array4 = np.array([[2,4,6],[8,10,12],[14,16,18],[20,22,24]])print(my_array4)my_array4.shape [[ 2 4 6] [ 8 10 12] [14 16 18] [20 22 24]] (4, 3) 3차원 배열 생성123my_array5 = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])print(my_array5)my_array5.shape [[[1 2] [3 4]] [[5 6] [7 8]]] (2, 2, 2) Numpy 기본 함수들 arange zeroes, ones reshape sort argsort arangenp.arange(5)를 넣으면 array안에 5개의 숫자가 순서대로 나오는 배열이 자동으로 만들어진다. 12Array = np.arange(5)print(Array) [0 1 2 3 4] np.arange(a, b, c) : a의 숫자부터 b 숫자까지 C씩 띄워서 배열생성 12aArray = np.arange(1, 9, 2)print(aArray) [1 3 5 7] zeroes, oneszeroes와 ones 함수를 살펴 보자각 함수들은 0과 1을 채워 넣어 배열을 생성하는 함수 들 이다. 1234567891011print(&quot;Zeros_Array&quot;)zeros_array = np.zeros((2,3))print(zeros_array)print(&quot;Data Type is:&quot;, zeros_array.dtype)print(&quot;Data Shape is:&quot;, zeros_array.shape)print(&quot;Ones_Array&quot;)ones_array = np.ones((3,4), dtype='int32')print(ones_array)print(&quot;Data Type is:&quot;, ones_array.dtype)print(&quot;Data Shape is:&quot;, ones_array.shape) Zeros_Array [[0. 0. 0.] [0. 0. 0.]] Data Type is: float64 Data Shape is: (2, 3) Ones_Array [[1 1 1 1] [1 1 1 1] [1 1 1 1]] Data Type is: int32 Data Shape is: (3, 4) 8행에 보면 Array를 행성 하면서 dtype을 int32로 지정 해 준 것을 볼 수 있다. Zeros_Array의 경우 채워진 0들이 모두 float type의 실수 이기 때문에 0. 이라고 나타는 것을 볼 수 있지만, Ones_Array의 경우 1 만 나타난 Int 형태의 type인 것을 볼 수 있다. reshapereshape는 행렬의 모양을 바꿔주는 함수이다.행렬의 모양을 바꿀 때에는 약간의 제약이 있는데예를 들어 설명 해 보자면, 3X4 = 12, 6X2 =12로 형태 변환을 할 수 있지만,3X5 = 15이기 때문에 변환이 불가능 하다.이것이 이해가 가지 않는다면 중학교로 돌아 가야 할 지도 모른다. 123456print(ones_array)print(&quot;Data Shape is:&quot;, ones_array.shape)print(&quot;Ones_array의 형태를 reshpe로 바꿔보자 \\n&quot;)after_reshape = ones_array.reshape(6,2)print(after_reshape)print(&quot;Data Shape is:&quot;, after_reshape.shape) [[1 1 1 1] [1 1 1 1] [1 1 1 1]] Data Shape is: (3, 4) Ones_array의 형태를 reshpe로 바꿔보자 [[1 1] [1 1] [1 1] [1 1] [1 1] [1 1]] Data Shape is: (6, 2) 2차원 배열은 3차원으로도 reshape 할 수 있다. 제약 조건은 3 X 4 = 12 였기 때문에 2 X 3 X 2 = 12가 되기 때문에 reshape 가 가능 하다. 123456789101112after_reshape = ones_array.reshape(2,3,2)print(after_reshape)print(&quot;Data Shape is:&quot;, after_reshape.shape, &quot;\\n&quot;)after_reshape2= ones_array.reshape(-1,6)print(&quot;reshape(-1,6)? \\n&quot;)print(after_reshape2)after_reshape3= ones_array.reshape(3,-1)print(&quot;reshape(3, -1)? \\n&quot;)print(after_reshape3)print(&quot;Data Shape is:&quot;, after_reshape3.shape) [[[1 1] [1 1] [1 1]] [[1 1] [1 1] [1 1]]] Data Shape is: (2, 3, 2) reshape(-1,6)? [[1 1 1 1 1 1] [1 1 1 1 1 1]] reshape(3, -1)? [[1 1 1 1] [1 1 1 1] [1 1 1 1]] Data Shape is: (3, 4) 만일 2차 행렬 reshape에서 한개의 변수만 정해 졌다면, 나머지 는 -1을 써주면 자동으로 알맞은 변수를 정해 줍니다. 3차 행렬 역시 남은 1개만 -1을 써서 reshape 함수로 행렬을 변환 할 수 있습니다. 21.11.02(이 아래 부분은 다음에 UPdate하기로 한다. ) short1 argsortNumpy 인덱싱과 슬라이딩Numpy 정렬","link":"/2021/11/01/blog/Numpy/"},{"title":"Study Pandas","text":"Pandas 판다스란pandas는 데이터를 시각화 하기 좋은 python base의 한 라이브러리이다. python을 이용한 data분석과 같은 작업에서 필수적으로 쓰이고 있다. 아나콘다와 같은 IDE를 이용하여 작업 할 수도 있지만, 기본적으로 Import하여 편하게 사용 할 수 있다. 판다스의 이름은 계량 경제학에서 사용되는 용어인 **’PANel DAta’**의 앞 글자를 따서 지어졌다.판다스는 R에서 사용되던 data.frame 구조를 본뜬 DataFrame이라는 구조를 사용하기 때문에, R의 data.frame에서 사용하던 기능 상당수를 무리없이 사용할 수 있도록 만들었다.더욱이 파이썬이라는 접근성이 좋은 언어 기반으로 동작하기 때문에 데이터 분석을 파이썬으로 입문하는사람들이 필수적으로 사용하는 라이브러리가 되었다.’ 판다스 Import 하기쥬피터 노트로 설치가 가능 하다고 하지만, 구글코랩이나 케글 노트에서는 Import하여 쉽게 사용한다. 12import pandas as pdpd.__version__ pandas는 오픈소스로 누구나 무료로 이용 할 수 있고,Numpy, matplotlib등 다른 라이브러리들과 함께 쓰인다. 일반적으로 pandas는 pd로 import되기 때문에 pd.[function] 으로 써있으면pandas 라이브러리를 이용한다고 생각 하면 된다. Livraly의 경우 version 오류가 많이 있으므로,Import 해 주는 라이브러리는 version을 꼭 확인하여 오류에 대비 하도록 한다. Pandas에는 3가지 자료 구조가 있는데series 와dataFrame,panel 이 그것이다. pandas에대해 더 알아보고 싶다면, 링크를 타고 가보자. ref. 판다스/위키 판다스를 이용에는 유용한 Tutorial들이 있다.아래 있으니 시간이 날 때마다 보면서 Update하자. 영문판이 있지만 지금은 일단 한글 번역한것을 보자화이팅 ! 판다스/doc10 minutes to Pandas/한글","link":"/2021/11/02/blog/Pandas/"},{"title":"python_List","text":"google Colaboratory 를 이용한 실습 Intro_ print와 주석처리 print Out 1print(&quot;Hello, World !!&quot;) Hello, World !! 이제 우리는 모든 것을 나타 낼 수 있습니다. 주석처리 1234567# 한 줄로 주석 처리 하는 방법 &quot;&quot;&quot;여러줄은 이렇게 print(&quot;Hello, World??&quot;)&quot;&quot;&quot;print(&quot;Hello, World!!&quot;) #주석 처리한 부분은 안나오지 Hello, World!! 변수의 종류 int : 정수 float : 실수 bool : 참/거짓 none : null String : 문자 이전에 배웠던 JAVA와는 다르게 변수형을 지정 해 주지 않아도 된다 Type 함수는 그 변수의 type이 어떤 것인지 알려준다. 123456789101112N_int = 1print (type(N_int))#Type 함수 : type 을 print 해 준다 N1_float = 2.0print (type(N1_float))bool_N = Trueprint (type(bool_N))X= Noneprint (type(X)) &lt;class 'int'&gt; &lt;class 'float'&gt; &lt;class 'bool'&gt; &lt;class 'NoneType'&gt; 연산 사칙연산 Number type 123456789101112131415161718192021a = 1b = 2c = 3.14d = 1.414print (&quot;정수_int_의 사칙연산 &quot;)print ('a + b = ' , a+b)print('a - b = ', a-b)print('a * b = ', a*b)print('a / b = ', a/b)print('a // b = ', a//b)print('a % b = ', a%b)print('a ** b = ', a**b)print (&quot;실수_float_의 사칙연산 &quot;)print('c + d =', c+d)print('c - d =', c-d)print('c * d =', c*d)print('c / d =', c/d)print('c // d =', c//d)print('c % d =', c%d)print('c ** d =', c**d) 정수_int_의 사칙연산 a + b = 3 a - b = -1 a * b = 2 a / b = 0.5 a // b = 0 a % b = 1 a ** b = 1 실수_float_의 사칙연산 c + d = 4.554 c - d = 1.7260000000000002 c * d = 4.43996 c / d = 2.2206506364922207 c // d = 2.0 c % d = 0.3120000000000003 c ** d = 5.042646477882085 String 연산 1234str1 = &quot;hi&quot;str2 = &quot;bye&quot;print('str1 + str2 = ', str1 + str2 )print ('str 1 * 3 + str * 2 = ', str1 * 3 + str2 * 2 ) str1 + str2 = hibye str 1 * 3 + str * 2 = hihihibyebye indexing1234567891011121314151617181920greeting = &quot;Hi!Hellow? AnNyung!&quot;print(greeting)#greeting, 이하니처럼 인사를 해 봅시다. print(greeting[6:])print(greeting [:6])G = &quot;12345 6789@&quot;print (G[2:])print (G[:3])print (G[1:9])print (G[0:7:3])&quot;&quot;&quot; [a : b : c] a : 시작 index b : 끝 Index c : 띄워서&quot;&quot;&quot; Hi!Hellow? AnNyung! low? AnNyung! Hi!Hel 345 6789@ 123 2345 678 146 ' \\n[a : b : c]\\n a : 시작 index\\n b : 끝 Index\\n c : 띄워서\\n' List List 함수 종류 .append() : 추가 .extend() : 연장 .remove() : 제거 .del() : 제거 .pop() :Index설정 .clear() : 전체 삭제 Internet에 더 많은 함수를 찾을 수 있는 documents 가 있을 것이다.^^ List Function List란, java로 치면 배열 List create 부터 123456789101112131415161718192021222324252627282930313233343536373839404142print(&quot;List 를 생성 해 봅시다.&quot;)A = [] #변수선언으로 빈 리스트 만들기a= list() #List 함수로 만들기 a = ['I', 'Have', ['a', 'DreAm']]print (&quot;a함수에 직접적 Index로 넣어주기&quot;)print (a)print (&quot;a[0]= 'YOU'의 결과 &quot;)a[0]= 'YOU'print (a)#List에 값 추가 해 주기print(A)A.append('U')A.append('song')A.append(['to', 'sing'])print(&quot;A[0:] : A의 모든 원소 추출&quot;)print(A[0:])print(&quot; &quot;)#Extend print(&quot;a : a에 Extend를 이용하여 원소 추가하기&quot;)print(a)a.extend('U')print(a)a.extend(['song', 'to', 'sing'])print(a)#Extend 와 같은 느낌으로 &quot;+=&quot; 연산자를 써 줄수 있다. print(&quot;a : a에 연산자를 이용하여 원소 추가하기&quot;) a += ['Like U']print(a)#Insert는 중간에 인덱스 값을 넣어 주어서 List에 값을 추가 해 줄 수 있다. print(a)print(&quot;a : a에 insert를 이용하여 원하는 Index번호에 원소 추가하기&quot;)a.insert(1,'do not')print(a)del a[1:1]#다음 코드를 위해 a의 Index 1에 넣어준 원소를 제거 했음 List 를 생성 해 봅시다. a함수에 직접적 Index로 넣어주기 ['I', 'Have', ['a', 'DreAm']] a[0]= 'YOU'의 결과 ['YOU', 'Have', ['a', 'DreAm']] [] A[0:] : A의 모든 원소 추출 ['U', 'song', ['to', 'sing']] a : a에 Extend를 이용하여 원소 추가하기 ['YOU', 'Have', ['a', 'DreAm']] ['YOU', 'Have', ['a', 'DreAm'], 'U'] ['YOU', 'Have', ['a', 'DreAm'], 'U', 'song', 'to', 'sing'] a : a에 연산자를 이용하여 원소 추가하기 ['YOU', 'Have', ['a', 'DreAm'], 'U', 'song', 'to', 'sing', 'Like U'] ['YOU', 'Have', ['a', 'DreAm'], 'U', 'song', 'to', 'sing', 'Like U'] a : a에 insert를 이용하여 원하는 Index번호에 원소 추가하기 ['YOU', 'do not', 'Have', ['a', 'DreAm'], 'U', 'song', 'to', 'sing', 'Like U'] 리스트 연산 12C = a+Aprint(&quot;a+A의 연산결과 : &quot;, C) ['U', 'do not', 'song', 'to', 'sing', 'U', 'song', ['to', 'sing']] 리스트에 값 삭제 해 주기 12345678910111213141516171819202122232425262728293031323334353637383940414243444546b=[1, 2, 3, 4, 5, 6 ]B=[7, 8, 9, 10, 11, 12, 13, 1, 4]print(&quot;리스트 값 삭제하기 &quot;)print(b)print(B)print(&quot;b의 1번째 삭제&quot;)#1개씩 삭제 해 주기 del b[1]print(b)print(&quot;B의 1~2 번째 삭제 : [1:2]&quot;)#범위로 삭제 해 주기 del B[1:2]print(B)#중복 값을 삭제 해 주는 functionK=[0, 1, 2, 3, 4, 5, 8, 6, 7, 9, 10, 0, 12]print(&quot;K : &quot;, K)K.remove(0)print(&quot;K.remove(0) : &quot;, K)K.insert(0, 1)print(&quot;K.insert(0,1) : &quot;, K)#pop은 Index를 정해서 특정 문자에 정해 줄 수 있다. S = b.pop()SS= B.pop(1)print(&quot;b.pop() : &quot;)print (S, &quot; : ()안에 아무것도 쓰지 않으면 맨 마지막 &quot;)print(&quot;B.pop(1) : &quot;)print(SS, &quot; : ()안에 숫자를 쓰면 (1) 1번Index 뽑아내기&quot;)#pop과 같은 느낌으로 특정 문자를 뽑아 내기 print(&quot;Index 숫자로 pop과 같은 기능을 실행 할 수 있다. &quot;)print (a[0:])print (a[2][1])print (a[2][1][3])#List 전체 삭제 리스트 값 삭제하기 [1, 2, 3, 4, 5, 6] [7, 8, 9, 10, 11, 12, 13, 1, 4] b의 1번째 삭제 [1, 3, 4, 5, 6] B의 1~2 번째 삭제 : [1:2] [7, 9, 10, 11, 12, 13, 1, 4] K : [0, 1, 2, 3, 4, 5, 8, 6, 7, 9, 10, 0, 12] K.remove(0) : [1, 2, 3, 4, 5, 8, 6, 7, 9, 10, 0, 12] K.insert(0,1) : [1, 1, 2, 3, 4, 5, 8, 6, 7, 9, 10, 0, 12] b.pop() : 6 : ()안에 아무것도 쓰지 않으면 맨 마지막 B.pop(1) : 9 : ()안에 숫자를 쓰면 (1) 1번Index 뽑아내기 Index 숫자로 pop과 같은 기능을 실행 할 수 있다. ['hi', 'do not', 'Have', ['a', 'DreAm'], 'U', 'song', 'to', 'sing', 'd', 'i', 'd', ' ', 'n', 'o', 't'] a --------------------------------------------------------------------------- IndexError Traceback (most recent call last) &lt;ipython-input-112-d37801788a81&gt; in &lt;module&gt;() 40 print (a[0:]) 41 print (a[2][1]) ---&gt; 42 print (a[2][1][3]) 43 44 #List 전체 삭제 IndexError: string index out of range List 값을 덮어 쓰는 기능 12345678print (&quot;A[] = &quot;, A) print (&quot;A[1:2]= ['i', 'do', 'NOT', 'know']의 결과 &quot;)A[1:2]= ['i', 'do', 'NOT', 'know']print (A) # A[]에 1번 Index에 범위보다 큰 수가 엎어 씌어 진 것을 볼 수 있다. del A [1:4]print(&quot;del A [1:4] : &quot;, A) A[] = ['U', 'i', 'do', 'NOT', 'know', 'do', 'NOT', 'know', 'do', 'NOT', 'know', ['to', 'sing']] A[1:2]= ['i', 'do', 'NOT', 'know']의 결과 ['U', 'i', 'do', 'NOT', 'know', 'do', 'NOT', 'know', 'do', 'NOT', 'know', 'do', 'NOT', 'know', ['to', 'sing']] del A [1:4] : ['U', 'know', 'do', 'NOT', 'know', 'do', 'NOT', 'know', 'do', 'NOT', 'know', ['to', 'sing']] Ref.https://dojang.io/course/view.php?id=7 보고 공부 할 수 있어요","link":"/2021/11/05/blog/Python_List/"},{"title":"python_Tuple","text":"TupleTuple은 List에비해 메모리 사용 효율이 좋지만, 정보 수정이 어렵다. splicing 기능, +, * 의 연산은 가능하다. 1 Dictionary1 If 조건문1 for 반복문1","link":"/2021/11/05/blog/Python_tuple/"},{"title":"Kaggle Note 사용하기","text":"google colab에서 실행되지 않는google colab에서 version 오류로 실행 되지 않는 것들이 있다고 한다.따라서 이때 Kaggle data를 이용 할 예정이면 Kaggle Note를 사용해 보는 것은 어떨까? ‘+ Create’ 표시를 누른다. - New NoteBook - New dataSet 탭이 나온다. New NoteBook을 선택하여 이용해 보자. 아래와 같은 코드가 나오게 된다. 123456789101112131415161718192021# This Python 3 environment comes with many helpful analytics libraries installed# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python# For example, here's several helpful packages to loadimport numpy as np # linear algebraimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)# Input data files are available in the read-only &quot;../input/&quot; directory# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directoryimport osfor dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename))# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using &quot;Save &amp; Run All&quot; # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session 이것을 유지 한채 아래에 code 연습을 하면 되는 것 같다. 21.11.05_","link":"/2021/11/05/blog/Using_Kaggle_Note/"},{"title":"visualization by python","text":"Visualization _python으로[1103 학습]시각화 연습 및 작성 코드 깃헙 블로그 및 깃헙에 올리기, 개인별로 공유 시각화 올릴 때, 전체 코드 올리지 마시고,나눠서 올리세요. 설명 글 추가하시고여~ (예시 하단) 산점도: 산점도란 무엇인가? 언제 쓰는가? 코드 작성 및 간단 설명박스플롯: 박스플롯이란 무엇인가? 언제 쓰는가? 코드 작성 및 간단 설명 오후 1시까지 1차로 한번 올려서 개인별로 공유해주세요. 홧팅요 Intropython에서 visualiztion 하기 위해서는 많은 방법이 있다. data Analist들은 시각화를 위해 많은 tool을 사용 하는데 우리는 Seaborn과 Matplotlib을 이용하여 시각화를 할 예정이다. 코드 기반(python)의 data visualiztion의 장점 여러 그래프 동시 작성 가능 기존의 코드 재활용성 데이터 그기의 제한이 없음 (RAM등의 제약조건 없을때) Matplotlib 는 이미지 데이터와 정형 데이터(정적 그래프)를 시각화 할 수 있는데 나와 같은 비전공자들에게 시각화 문법이 조금 어렵다고 한다. 하지만 pandas data frame에서 쉽게 시각화 구현 하며, 통계(회귀선) 그래프 등을 쉽게 구현 할 수 있기 때문에 이를 배워야 한다. seaborn의 경우 그래프가 예쁘게 나오지 않지만 비교적 간단한 코드로 시각화를 할 수 있다. 하지만, 세부 옵션을 수정 하고 싶다면 Matplotlib를 알아야 한다. 이는 내부 원리를 파악 할 수 없기 때문에 내가 감당하기 힘들다. 때문에 지금 단계에서는 Matplotlib과 seaborn의 장점을 적절하게 섞어서 시각화를 진행 해 보자. 아래와 같은 Tutorial을 진행 하면 세부 옵션을 조정 하기 쉽다고 한다.https://matplotlib.org/stable/tutorials/index.html matplotlibmatplotlib를 이용하여 visualiztion을 해 보자. 제일 먼저 해 주어야 할 일은 Import하여 matplotlib를 불러오고 이를 plt등의 객체에 저장 해 주는 것이다. 123456789import matplotlib.pyplot as pltdates = [ '2021-01-01', '2021-01-02', '2021-01-03', '2021-01-04', '2021-01-05', '2021-01-06', '2021-01-07', '2021-01-08', '2021-01-09', '2021-01-10']min_temperature = [20.7, 17.9, 18.8, 14.6, 15.8, 15.8, 15.8, 17.4, 21.8, 20.0]max_temperature = [34.7, 28.9, 31.8, 25.6, 28.8, 21.8, 22.8, 28.4, 30.8, 32.0] 위의 data는 강사님의 data를 가져 온 것이다. data를 시각화 자료로 만들어 보자, 12345fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10,6))axes.plot(dates, min_temperature, label = 'Min Temperature')axes.plot(dates, max_temperature, label = 'Max Temperature')axes.legend()plt.show() 위의 표는 legend가 있고, 날짜별로 Min Temp와 Max Temp가 있는 그래프 이다. 먼저 fig, ax = plt.subplots()를 이용하여 표를 생성 한다. 이때 figure에대한 정보를 함께 생성 할 수 있는데, (nrows=1, ncols=1, figsize=(10,6)) 가 의미하는 것은 (행의 갯수 =1, 열의 갯수=1, fig size는 10X6)이다. ax를 통하여 plot 형태의 그래프를 그리는데, (x축, y축 , label = “[name]”)의 형태로 plot을 추가 해 준다.이때 Legend 를 생성 하고 싶다면, ax.legend()함수로 추가 해 줄 수 있다. 마지막으로 plt.show() 를 사용하여 마무리 해 준다. 물론 마지막 코드를 넣지 않아도 보여 주지만, 넣어주는 것이 인지상정이라고 한다. 123print(fig)print(axes) Figure(720x432) AxesSubplot(0.125,0.125;0.775x0.755) 위의 표인 fig의 정보를 알 수 있게 print 함수로 뽑아 봤는데 이건 무슨 말 인지 잘 모르겠다. matplotlib로 선 그래프 그리기아직 data를 어디에서 받을 수 있는지 잘 모르기 때문에 강사님이 다운받은 표를 그대로 가져와 보자 참조: https://pypi.org/project/fix-yahoo-finance/ 1!pip install yfinance --upgrade --no-cache-dir Collecting yfinance Downloading yfinance-0.1.64.tar.gz (26 kB) Requirement already satisfied: pandas&gt;=0.24 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.1.5) Requirement already satisfied: numpy&gt;=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.19.5) Requirement already satisfied: requests&gt;=2.20 in /usr/local/lib/python3.7/dist-packages (from yfinance) (2.23.0) Requirement already satisfied: multitasking&gt;=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.9) Collecting lxml&gt;=4.5.1 Downloading lxml-4.6.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.3 MB) \u001b[K |████████████████████████████████| 6.3 MB 7.7 MB/s \u001b[?25hRequirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24-&gt;yfinance) (2.8.2) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24-&gt;yfinance) (2018.9) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas&gt;=0.24-&gt;yfinance) (1.15.0) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.20-&gt;yfinance) (1.24.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.20-&gt;yfinance) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.20-&gt;yfinance) (2.10) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.20-&gt;yfinance) (2021.5.30) Building wheels for collected packages: yfinance Building wheel for yfinance (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for yfinance: filename=yfinance-0.1.64-py2.py3-none-any.whl size=24109 sha256=25a6e16cba240e66cb4999d0947a231b790b2b96766767434407e09149ec9302 Stored in directory: /tmp/pip-ephem-wheel-cache-a8wml444/wheels/86/fe/9b/a4d3d78796b699e37065e5b6c27b75cff448ddb8b24943c288 Successfully built yfinance Installing collected packages: lxml, yfinance Attempting uninstall: lxml Found existing installation: lxml 4.2.6 Uninstalling lxml-4.2.6: Successfully uninstalled lxml-4.2.6 Successfully installed lxml-4.6.4 yfinance-0.1.64 123456import yfinance as yfdata= yf.download('AAPL', '2019-08-01', '2021-08-01')data.info()print(yf) [*********************100%***********************] 1 of 1 completed &lt;class 'pandas.core.frame.DataFrame'&gt; DatetimeIndex: 504 entries, 2019-08-01 to 2021-07-30 Data columns (total 6 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Open 504 non-null float64 1 High 504 non-null float64 2 Low 504 non-null float64 3 Close 504 non-null float64 4 Adj Close 504 non-null float64 5 Volume 504 non-null int64 dtypes: float64(5), int64(1) memory usage: 27.6 KB &lt;module 'yfinance' from '/usr/local/lib/python3.7/dist-packages/yfinance/__init__.py'&gt; info() 함수를 통해 column정보와 data의 갯수, data의 type까지 알 수 있다.date time index에 구간이 내가 정한데로 나와 있는 것도 볼 수 있다. 12ts = data['Open']print(ts.head()) Date 2019-08-01 53.474998 2019-08-02 51.382500 2019-08-05 49.497501 2019-08-06 49.077499 2019-08-07 48.852501 Name: Open, dtype: float64 ts 객체에 Open data를 담고 .head() 함수로 상위 5개의 항목을 가져와 본다. 혹시,Pandas 에대하여 더 정리된 것을 알고 싶다면, 링크를 통해 확인 할 수 있다. Pyplot API아래의 예는 Pyplot API 방법을 이용하여, 한개씩 data를 넣어 준 형태 이다. 이는 객체지향으로 만들었다고 하기 어렵지만, 가능은 하다. 12345678910111213# import fix_yahoo_finance as yfimport yfinance as yfimport matplotlib.pyplot as pltdata = yf.download('AAPL', '2019-08-01', '2020-08-01')ts = data['Open']plt.figure(figsize=(10,6))plt.plot(ts)plt.legend(labels=['Price'], loc='best')plt.title('Stock Market fluctuation of AAPL') plt.xlabel('Date') plt.ylabel('Stock Market Open Price') plt.show() [*********************100%***********************] 1 of 1 completed Pyplot API 객체지향12345from matplotlib.backends.backend_aggimport FigureCanvasAgg as FigureCanvasfrom matplotlib.figure import Figureimport matplotlib.pyplot as plt 객체지향을 위해 import를 해 준다. 1234567891011fig = Figure()import numpy as npnp.random.seed(6)x = np.random.randn(20000)ax = fig.add_subplot(111)ax.hist(x, 100)ax.set_title('Artist Layer Histogram')# fig.savefig('Matplotlib_histogram.png')plt.show()","link":"/2021/11/03/blog/Visualiztion__python/"},{"title":"BearSoup_Review","text":"&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD 논문 리뷰======= c5589a672e850f5edca309f81ca50915cbf17ada","link":"/2021/11/03/blog/bearSoup_Review/"},{"title":"Making Category","text":"--- title: &quot;Making Category&quot; excerpt :&quot;Credit card&quot; classes: wide categories: -init tags: -python -title -coding last_modified_at: 2021-11-03 --- color 바꾸고 싶은에 안되네 https://www.color-hex.com/color/f4dcdc","link":"/2021/11/04/blog/categories/"},{"title":"Column 뽑아오기","text":"-python: for문으로 뽑아오기 12345for column in df17_Ea:... print(column)df.coulumns#이건 column 이름 나옴 경력은 Tenure 인거 같다. 1df18.loc[:0] Question을 다 삭제 해 버렸는데 질문 정히 해 놓은 것이 사라지면 어쩔 수 없이 다시 불러와야지 판다스 옵션 pandas 옵션을 조정하여 생략없이 긴 data를 볼 수 있다. 1pd.describe_option() 1pd.set_option('display.max_seq_items', None) 1234# row 생략 없이 출력pd.set_option('display.max_rows', None)# col 생략 없이 출력pd.set_option('display.max_columns', None) 경력 : [Tenure] in df17, [Q8] in df18, [Q15] in 19, [Q6] in 20, [Q6]in 21연봉 : [Q9] in df18, [Q10] in df19, [Q24] in 20, [Q25]in 21*2017연봉의 경우 ‘CompensationAmount’ 컬럼에 있지만, 통화가 다르므로 하지 말자.","link":"/2021/11/19/blog/columnPOP/"},{"title":"Pandas_DataFrame","text":"#pandas에서 dataFrame 자료구조. dataFrame은 표와 같은 스프레드 시트 형식의 자료 구조이다. 2차원 배열 또는 리스트, data table 전체를 포함하는 object라고 볼수 있음. 여러개의 column이 있고, 각 컬럼은 숫자, 문자열, boolean type을 담을 수 있다. dataFrame은 Rew, column에대한 Index 이렇게 2가지 변수를 담고 있는데 matrix라고 할 수 있다. pd.dataframe()1234567import pandas as pdvalues = [['rose', 'tulip', 'Liry'], [4, 5, 6], ['red', 'blue', 'green']]index = ['flower', 'Number', 'color']columns = [1, 2, 3]df = pd.DataFrame(values, index=index, columns=columns)print(df) df 는 data frame의 준말. 1 2 3 flower rose tulip Liry Number 4 5 6 color red blue green Index와 column 의 dtype은 object이다. 데이터프레임은 리스트(List), 시리즈(Series), 딕셔너리(dict), Numpy의 ndarrays,또 다른 데이터프레임으로 생성할 수 있습니다. Ref.DataFrame","link":"/2021/11/06/blog/dataFrame(pandas)/"},{"title":"Think About My Mow!!","text":"21.12.06 Mow The grass of the GitHub is related to Commit. My the most green mow day is 2021/11/09 that day i created 120commits because the github was created that day. I haven’t been able to do GitHub often these days. This was because I focused all my energyon the Kaggle competition as a team project. In addition, due to the nature of the competition,there was not much to commit. My lawn is as green as Naver. But the color of the grassI want comes out when I commit more than 30. When can I get out of this commit hell? Please, I want to grow my lawn in a non-artificial way.","link":"/2021/12/05/blog/gitHubMaw/"},{"title":"to use gitHub in Multi place","text":"깃허브 노트북과 데스크탑 두군데서깃허브를 동시에 사용하고 싶은 분들은아래 코드 확인해서 해보세요. 이 때, 깃헙 blog 저장소 삭제할 필요가 없어요~ 123456789101112$ hexo init myblog # 여기는 각자 소스 레포 확인$ cd myblog$ git init $ git remote add origin https://github.com/rain0430/myblog.git # 각자 소스 레포 주소$ git pull --set-upstream origin main # 에러 발생$ git clean -d -f$ git pull --set-upstream origin main # 에러 발생 안함 / 소스 확인$ npm install $ hexo clean$ hexo generate$ hexo server 저장 해 놓고 나중에 해 봐야지 .","link":"/2021/11/02/blog/gitHub_multi/"},{"title":"Hello World","text":"Welcome to Hexo!This is your very first post. Check documentationfor more info. If you get any problems when using Hexo,you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2021/10/01/blog/hello-world/"},{"title":"machine Learning_basic","text":"machine learningDecision tree learing Decision tree 란 관측값과 목표값을 연결 시켜주는 예측 모델 통계학과 데이터 마이닝, 기계학습에서 예측모델링으로 사용하는 방법 데이터 기능에서 유추된 결정규칙을 학습하여 대상 변수값을 예측 하는 모델을 만들기 위해 사용 종류 분류 트리 변수가 유한한 수의 값을 가지는 것, 클래스 출력 Leaf node는 클래스 라벨을 나타내고 가지는 클래스 라벨과 관련있는 특징들의 논리 곱을 나타낸다. 회귀트리 목표변수가 연속하는 값(일반적으로 실수 )을 가지는 트리 특정 의미를 가지는 실수값을 출력 의사결정 분석에서 결정트리는 시각적으로 명시적인 방법으로 과정을 보여준다. 결정트리의 학습 결정 트리의 학습 : 자료 집합을 적절한 분할기준 또는 분할 테스트에 따라 부분집합들로 나누는 과정 하향식 결정 트리 귀납법 (TDIDT_top-down induction of decision trees) 순환분할 방식으로 나눠진 자료의 부분집합에 재귀적으로 반복됨. 분할로 인해 더이상 새로운 예측값이 추가 되지 않거나 부분집합의 노드가 목표변수와 같은 값을 지닐대 까지 계속됨. 데이터 마이닝에서 결정트리 수학 적으로 표현됨 예를 들어 아래와 같은 데이터 마이닝에서의 결정 트리가 있다고 가정 해 보자. ( f{x},Y ) = (x_1, x_2, x_3, ..., x_k, Y) 종속 변수 Y는 분류를 통해 학습하고자 하는 목표 변수이며, 벡터 x는 { x_{1}, x_{2}, x_{3} } x_1, x_2, x_3 등의 입력 변수로 구성된다. 장점1. 이해하기 쉬우며 시각화 가능 2. data 준비가 거의 필요하지 않음 3. 수치형, 범주형 data를 모두 처리 가능 4. multi-output problems를 다룰 수 있다. 5. a white box model을 사용 할 수 있다. (bool 가능) 6. 통계 검정을 사용하여 모형을 검증하기 때문에 모델의 신뢰성을 설명할 수 있다. 7. 생성된 데이터가 실제 모형에 의해 가정이 다소 위반되더라도 잘 수행된다. 단점 data 일반화가 잘 되지 못하면 복잡한 트리가 만들어짐 (과적합) - 가지치기, 리프노드에 필요한 샘플 최소화, 트리 최대깊이 설정 으로 해결 가능 variation이 작은 경우 의사결정트리가 불안정 할 수 있다. 앙상블(ensemble) 내에서 의사결정 트리 사용으로 해결 가능. 실용적인 의사 결정 트리 학습 알고리즘은 각 노드에서 국소적으로 최적의 의사결정이 이루어지는 그리디 알고리즘과 같은 경험적 알고리즘을 기반으로 한다. 이러한 알고리즘은 전역 최적 의사 결정 트리를 반환한다고 보장할 수 없다. 이는 특징과 샘플이 교체와 함께 무작위로 샘플링되는 앙상블 학습기에서 여러 트리를 훈련시킴으로써 완화될 수 있다. 의사 결정 트리의 예측은 근사치이기 때문에 좋은 추정은 아닐 수 있다. 의사결정 트리의 최적화의 문제는 NP-complete로 잘 알려진 문제이다.잘 모르겠다. NP-complete XOR, parity or multiplexer problems와 같이 배우기 쉽지 않은 컨셉들 때문에 표현하기 쉽지않다. 학습자가 편향된 트리르르 만들 수 있으므로, 데이터 세트의 균형을 맞춰줘야 한다. 이어지는 posting classification Regression Ref. 결정트리학습법/wiki 결정트리 국문 Decusuib Trees 1.10.1 1.10.2","link":"/2021/11/04/blog/machineLearning_basic/"},{"title":"Method Making New Repository","text":"Hello World I’m in the Mars Github에서###새로운 저장소 만들기 git hub 로그인 후 새로운 저장소를 만들어 보도록 합시다. new 를 누르면 새로운 repository를 생성하기 위한 정보를 입렬 할 수있다. Repository name 입력 후 Public(전체 공개)으로 할 것인지, Private(비공개)로 할 것인지 선택한다. 마지막으로 create repository를 누르면 생성됨. 생성된 repository에 README.md file을 생성해 봅시다. git bash를 열어봅시다. (원하는 경로에서) git bash 에서 github에서 만든 file을 내려받아 봅니다.1$ git clone + 나의 경로 혹시 나의 경로를 어디서 찾는지 잘 모르는 나를 위해 남긴다. git bash네… bush인줄알았는데… 원하는 경로에 file이 생긴 것을 볼 수 있다. 이제 README.md file 을 만들어 보자. 위의 My path image를 보면 아랫쪽에 코드가 나와있는 것을 볼 수 있다. New file을 파이참으로 열어준 후 아래 코드를 한줄씩 입력 하면 된다. (위 사진에 나와 있는 코드!! )1234567echo &quot;# New&quot; &gt;&gt; README.mdgit initgit add README.mdgit commit -m &quot;first commit&quot;git branch -M maingit remote add origin https://github.com/ 이 부분이 사람마다 다르니 웹페이지 보고 하기!git push -u origin main 이제 저장소와 Desktop file이 페어링 되었다. 앞으로는 file Update후 git 명령어로 올리면 된다. 예를 들자면, folder에 file을 넣고, pycham으로 foler를 열어서 다음과 같은 명령어를 넣으면 된다. 123git add . -- 모든 file을 업로드 하기 위해 저장git commit -m &quot;history Log로 확인 할수 있는 message&quot; -- 확인git push -- 최종적으로 file을 git hub에 올림 이제 전세계 어디에 있던 대용량 파일 저장소를 직접 손에 들고 다닐 일이 없어진거다. (물론 인터넷이 잘 된다는 가정 하에서….)하지만, 나는 오늘도 노파심에 외장하드를 들고 나왔다 하하 앞으로 github에 다 정리해서 넣고 외장하드 안가지고 다녀야지 ^^ 화이팅 !!","link":"/2021/10/28/blog/make_NewRepository/"},{"title":"Making Blog method","text":"Hello World I’m in the Mars Hi my name is YoonHwa Park ! just call me Yoon ^^. Nice to meet you.Today’s Topic is Making Blog !!! it is not easy to make. (ㅜ_ㅜ) but We Can DO IT ^0^!! If you try to do….(…may be…)Let’s Start!! 깃허브 블로그 만드는 방법들어가기 전에!Install Applications gitHub : https://github.com/ pycham : https://www.jetbrains.com/pycharm/ nodejs : https://nodejs.org/en/ ※ 주의 : 설치 중에 SYSTEM PATH 관련 텝이 나오면 Check 하고 넘어가기회원가입 github : mail 로 회원가입 하기. 나머지 회원가입 권유는 무시해도됨. (하고 싶으면 하시오.) 1$ hexo new &quot;My New Post&quot; Ref.this file is written by MARKDOWN마크다운 기초 확인 하고 작성https://gist.github.com/ihoneymon/652be052a0727ad59601","link":"/2021/10/28/blog/make_blog/"},{"title":"Deploy gitHub blog by hexo","text":"##github blog를 Hexo를 이용하여 deploy 해 보자. Ref. https://dschloe.github.io/settings/hexo_blog/ ###Hexo를 이용하여 블로그를 만들어 보자 Node js를 download 해 준다. Node js 설치 GitBash 에서 node의 version을 확인 1$ node -v Hexo를 설치 해 준다. Hexo의 경우 npm을 이용하여 설치 한다. 1$ npm install -g hexo-cli git bash를 적당한 경로에서 들어간다.makeBlog folder를 만들어준다. 12345$ mkdir makeBlog$ cd makeBlog$ hexo init myblog hexo init 을 myblog에서 해 준다.hexo server와 deployer를 설치 해 준다. 이를 설치 하지 않으면 에러가 날 수 있다. 1234$ cd myblog$ npm install$ npm install hexo-server --save$ npm install hexo-deployer-git --save","link":"/2021/11/04/blog/nodjs/"},{"title":"Lecture_python_basic","text":"Hello World12print(&quot;Hello, world!&quot;)print(&quot;hi ^0^&quot;) Hello, world! 주석처리12345# 한 줄 주석 처리&quot;&quot;&quot;여러 줄 주석 예제 동일한 따옴표(큰따옴표 혹은 작은 따옴표) 세 개와 세 개 사이에는어떠한 내용, 몇 줄이 들어가더라도 모두 주석으로 처리된다.&quot;&quot;&quot;print(&quot;Hello, world!&quot;) Hello, world! 변수의 종류12num_int = 1print(type(num_int)) &lt;class 'int'&gt; 12num_float = 0.2print(type(num_float)) &lt;class 'float'&gt; 12bool_true = Trueprint(type(bool_true)) &lt;class 'bool'&gt; 12none_x = Noneprint(type(none_x)) &lt;class 'NoneType'&gt; 사칙 연산123456789a = 3b = 2print('a + b = ', a+b)print('a - b = ', a-b)print('a * b = ', a*b)print('a / b = ', a/b)print('a // b = ', a//b)print('a % b = ', a%b)print('a ** b = ', a**b) a + b = 5 a - b = 1 a * b = 6 a / b = 1.5 a // b = 1 a % b = 1 a ** b = 9 123456789c = 3.0d = 2.0print('c + d =', c+d)print('c - d =', c-d)print('c * d =', c*d)print('c / d =', c/d)print('c // d =', c//d)print('c % d =', c%d)print('c ** d =', c**d) c + d = 5.0 c - d = 1.0 c * d = 6.0 c / d = 1.5 c // d = 1.0 c % d = 1.0 c ** d = 9.0 논리형 연산자1234print(True and True)print(True and False)print(False and True)print(False and False) True False False False 1234print(True or True)print(True or False)print(False or True)print(False or False) True True True False 비교 연산자123456print(4 &gt; 3)print(4 &lt; 3)print(4 &gt;= 3)print(4 &lt;= 3)print(4 &gt; 4)print(4 &gt;= 4) True False True False False True 논리형 &amp; 비교 연산자 응용123456#input(&quot;숫자를 입력하세요&quot;)_연습하기data = input (&quot;숫자를 입력하세요.&quot;)data2 =int(data)print(type(data2))# class 가 달라진다. data = string , data 2 = int -&gt; 형변환 12345678num1 = int(input(&quot;첫번째 숫자를 입력하세요: &quot;))num2 = int(input(&quot;두번째 숫자를 입력하세요: &quot;))num3 = int(input(&quot;세번째 숫자를 입력하세요: &quot;))num4 = int(input(&quot;네번째 숫자를 입력하세요: &quot;))var1 = num1 &gt;= num2var2 = num3 &lt; num4print(var1 and var2) 첫번째 숫자를 입력하세요: 10 두번째 숫자를 입력하세요: 20 세번째 숫자를 입력하세요: 30 네번째 숫자를 입력하세요: 11 False String12print(&quot;'Hello, world!'&quot;)print('&quot;Hello, world!&quot;') 'Hello, world!' &quot;Hello, world!&quot; String Operators123456str1 = &quot;Hello &quot;str2 = &quot;World &quot;print('str1 + str2 = ', str1 + str2)greet = str1 + str2print('greet * 3 = ', greet * 3) str1 + str2 = Hello World greet * 3 = Hello World Hello World Hello World Indexing12greeting = &quot;Hello Kaggle&quot;print(greeting[6]) K Slicing123456greeting = &quot;Hello Kaggle&quot;print(greeting[:])print(greeting[6:])print(greeting[:6])print(greeting[3:8])print(greeting[0:9:2]) Hello Kaggle Kaggle Hello lo Ka HloKg 1greeting[13] 리스트1234567891011a = [] # 빈 리스트a_func = list() #list()함수로도 빈 리스트를 만들 수 있다.b = [1] # 숫자도 요소가 될 수 있다.c = ['apple'] # 문자열도 요소가 될 수 있다d = [1, 2, ['apple']] # 리스트 안에 리스트를 요소로 넣을 수 있다.print(a)print(a_func)print(b)print(c)print(d) [] [] [1] ['apple'] [1, 2, ['apple']] 123456a = [1, 2, 3]# index [[0], [1], [2]]print(a[0]) # 첫번째 요소print(a[1]) # 두번째 요소print(a[2]) # 세번째 요소print(a[-1]) 1 2 3 3 1234567a = [['apple','banana','cherry'], 1]print(a[0]) # 리스트 내의 리스트print(a[0][0]) # 리스트 내의 리스트의 첫번째 문자열print(a[0][0][3]) # 리스트 내의 리스트의 첫번째 문자열 'apple' 중 첫번째 인덱스print(a[0][1]) # 리스트 내의 리스트의 두번째 문자열print (a[0][2]) ['apple', 'banana', 'cherry'] apple l banana cherry 12345678910111213a = [1,2,3,4,5,6,7,8,9,10]b = a[:4] # 인덱스 0부터 3까지c = a[1:4] # 인덱스 1부터 3까지d = a[0:7:2] # 인덱스 0부터 6까지 인덱스 2씩 건너 띄우기e = a[::-1] # 리스트 a의 역순f = a[::2] # 리스트 전체구간에서 인덱스 2씩 건너띄우기print(&quot;a[:4]&quot;, b)print(&quot;a[1:4]&quot;, c)print(&quot;a[0:7:2]&quot;, d)print(&quot;a[::-1]&quot;, e)print(&quot;a[::2]&quot;, f) a[:4] [1, 2, 3, 4] a[1:4] [2, 3, 4] a[0:7:2] [1, 3, 5, 7] a[::-1] [10, 9, 8, 7, 6, 5, 4, 3, 2, 1] a[::2] [1, 3, 5, 7, 9] 12345a = ['alice', 'bob', 'cat']b = ['apple', 'banana', 'cherry']c = a+bprint(c) ['alice', 'bob', 'cat', 'apple', 'banana', 'cherry'] 12345a = ['a','b','c']b = a*3c = a*0print(&quot;a * 3:&quot;, b)print(&quot;a * 0:&quot;, c) a * 3: ['a', 'b', 'c', 'a', 'b', 'c', 'a', 'b', 'c'] a * 0: [] 리스트 값 수정하기1234a = [0,1,2]a[1] = &quot;b&quot;print(a) [0, 'b', 2] 리스트 값 추가하기123456a = [100, 200, 300]a.append(400)print(a)a.append([500,600])print(a) [100, 200, 300, 400] [100, 200, 300, 400, [500, 600]] 1234a = [1,2,3]a.extend([40,500])print('a.extend([40,500]) result')print(a) a.extend([40,500]) result [1, 2, 3, 40, 500] 1234a = [0,1,2]a.insert(1,100)print(a) [0, 100, 1, 2] 12345678910111213a = [0,1,2,3]a[2:2] = [100,200]print(a)# 시작과 끝의 범위보다 큰 수를 덮어쓰는 예시b = [0,1,2,3]b[1:2] = [100,200,300,400] print(b)# 시작과 끝의 범위가 작을때의 예시c = [0,1,2,3]c[1:3] = [100]print(c) [0, 1, 100, 200, 2, 3] [0, 100, 200, 300, 400, 2, 3] [0, 100, 3] 리스트 값 삭제하기123456789a =[1,2,1,2]#리스트의 첫번째 1이 삭제a.remove(1)print(a)#리스트의 두번째 1이 삭제a.remove(1)print(a) [2, 1, 2] [2, 2] 12345678910a = [0,1,2,3,4,5,6,7,8,9]# 1 삭제del a[1]print(a)b = [0,1,2,3,4,5,6,7,8,9]# 범위로 삭제del b[1:3] #list는 항상 시작하는 index부터, 종료하는 n의 n-1까지의 범위를 잡아줍니다.print(b) [0, 2, 3, 4, 5, 6, 7, 8, 9] [0, 3, 4, 5, 6, 7, 8, 9] 123456#인덱스를 지정한 pop()a = [0,1,2,3,4]r = a.pop(1)print(a)print(r) [0, 2, 3, 4] 1 123456#인덱스를 지정하지 않은 pop()b = ['a','b','c','d']x = b.pop()print(b)print(x) ['a', 'b', 'c'] d 그 외 유용한 메서드12345a = [0,1,2,3]print(a)a.clear()print(a) [0, 1, 2, 3] [] 12a = [&quot;Gold&quot;, &quot;Gold&quot;, &quot;Silver&quot;, &quot;Silver&quot;]print(&quot;Silver가 처음 등장하는 인덱스 번호&quot;, a.index(&quot;Silver&quot;)) Silver가 처음 등장하는 인덱스 번호 2 12345678a = [1, 4, 5, 2, 3]b = [1, 4, 5, 2, 3]a.sort()print(&quot;sort():&quot;,a)b.sort(reverse=True)print(&quot;sort(reverse=True):&quot;, b) sort(): [1, 2, 3, 4, 5] sort(reverse=True): [5, 4, 3, 2, 1] 1234b = [4,3,2,'a']b.sort()print(b) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-38-1624da3f09a9&gt; in &lt;module&gt;() 1 b = [4,3,2,'a'] 2 ----&gt; 3 b.sort() 4 print(b) TypeError: '&lt;' not supported between instances of 'str' and 'int' 튜플1234567891011tuple1 = (0) # 끝에 콤마(,)를 붙이지 않았을 때tuple2 = (0,) # 끝에 콤마(,)를 붙여줬을 때tuple3 = 0,1,2print(tuple1)print(tuple2)print(tuple3)print(type(tuple1)) # 콤마(,)를 붙여주지 않으면 튜플이 아닙니다.print(type(tuple2)) # 콤마(,)를 붙여주어야 튜플 자료형 입니다.print(type(tuple3)) # 여러개의 값 일경우 괄호를 없애주어도 튜플 자료형 입니다. 0 (0,) (0, 1, 2) &lt;class 'int'&gt; &lt;class 'tuple'&gt; &lt;class 'tuple'&gt; 12a = (0,1,2,3,'a')del a['a'] --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-41-c41b8ecfc68f&gt; in &lt;module&gt;() 1 a = (0,1,2,3,'a') ----&gt; 2 del a['a'] TypeError: 'tuple' object does not support item deletion 12a = (0,1,2,3,'a')a[1]='t' --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-42-04fb068f82e0&gt; in &lt;module&gt;() 1 a = (0,1,2,3,'a') ----&gt; 2 a[1]='t' TypeError: 'tuple' object does not support item assignment 튜플 인덱싱 및 슬라이싱 하기1234t = (0,1,2,'b',4)print(t[1])print(t[3]) 123t = (0,1,2,3,4)print(t[2:])print(t[0:2]) (2, 3, 4) (0, 1) 더하기 및 곱셈 연산자 사용12345t1 = (0,1,2,3,4)t2 = ('a','b','c')t3 = t1+t2print(t1+t2)print(t3) 123t1 = ('a','b')print(t1*0)print(t1*3) 딕셔너리12345dic = {'teacher':'alice', 'class': 5, 'studentid': '15', 'list':[1,2,3]}print(dic['teacher'])print(dic['class'])print(dic['list']) alice 5 [1, 2, 3] 12dic = {'teacher':'alice', 'class': 5, 'studentid': '15', 'list':[1,2,3]}print(dic['real']) --------------------------------------------------------------------------- KeyError Traceback (most recent call last) &lt;ipython-input-44-fd82dcc94904&gt; in &lt;module&gt;() 1 dic = {'teacher':'alice', 'class': 5, 'studentid': '15', 'list':[1,2,3]} ----&gt; 2 print(dic['real']) KeyError: 'real' 12a = {'name': 'bob', 'job': 'farmer', 'age': 35}a.keys() dict_keys(['name', 'job', 'age']) 12a = {'name': 'bob', 'job': 'farmer', 'age': 35}a.values() dict_values(['bob', 'farmer', 35]) 1234a = {'name': 'chris', 'job': 'painter', 'age': 30}print(a.get('name'))print(a.get('dinner'))print(a.get('dinner', 'empty')) chris None empty 집합 연산자12345678s = {}print(type(s))s = set()print(type(s))s = {1,2,3}print(type(s)) &lt;class 'dict'&gt; &lt;class 'set'&gt; &lt;class 'set'&gt; 1234567a = {1,3,5}b = {2,4,6}c = a|bd = a.union(b)print(&quot;a|b:&quot;, c)print(&quot;a.union(b)&quot;, d) a|b: {1, 2, 3, 4, 5, 6} a.union(b) {1, 2, 3, 4, 5, 6} 1234567891011a = {1,3,5}b = {2,4,6}c = a&amp;bprint(c)e = {1,2,5}f = {2,3,5}g1 = e&amp;fg2 = e.intersection(f)print(&quot;e&amp;f:&quot;, g1)print(&quot;e.intersection(f):&quot;, g2) set() e&amp;f: {2, 5} e.intersection(f): {2, 5} 1234567a = {1,3,5}b = {2,4,5}c1 = a-bc2 = a.difference(b)print(&quot;a-b:&quot;, c1)print(&quot;a.difference(b)&quot;, c2) a-b: {1, 3} a.difference(b) {1, 3} 1234567a = {1,2,3,4,5}b = {3,4,5,6,7}c1 = a^bc2 = a.symmetric_difference(b)print(&quot;a^b&quot;, c1)print(&quot;a.symmetric_difference(b)&quot;, c2) a^b {1, 2, 6, 7} a.symmetric_difference(b) {1, 2, 6, 7} if 조건문12345678910a = -5if a&gt;5: print('a is bigger than 5')elif a &gt; 0: print(&quot;a is bigger than 0 but a is smaller than 5 &quot;)else: print(&quot;a is negative&quot;) a is negative 반복문1print(&quot;Hello World&quot;) Hello World 123print(&quot;Hello World&quot;)print(&quot;Hello World&quot;)print(&quot;Hello World&quot;) Hello World Hello World Hello World 12for i in range(10000): print(&quot;Hello World&quot;) \u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m Hello World Hello World Hello World Hello World (손으로 자름.) Hello World Hello World 1234567a = &quot;Kaggle&quot;for x in a: print(x) if x == 'g': break K a g 123alphabets = ['A', 'B', 'C']for index, value in enumerate(alphabets): print(index, value) 0 A 1 B 2 C","link":"/2021/11/02/blog/python_basic/"},{"title":"Pandas_panel","text":"#pandas에서 panel 자료구조.","link":"/2021/11/06/blog/panel(pandas)/"},{"title":"Pandas_Series","text":"#Pandas에서 series 자료구조 series는 1차원 배열같은 자료 구조를 말한다. 아래 code는 python pandas의 parameter 값이다. 123def __init__(data=None, index=None, dtype=None, name=None, copy=False, fastpath=False) series의 parameter는 data, index, dtype, name, copy, fastpath로 나뉘어져 있는데name의 경우는 이름 인 것 같고 기본적으로 Index와 value라는 parameter를 많이 이용 하는 듯 하다. Index : 배열의 이름 value : 값 python의 dictionalry와 거의 유사 한 것 같다. (다음에 찾아보자 오늘은 벅참.) series의 dtype에는 str, numpy.dtype, or ExtensionDtype, optional Data type 을담을 수 있는데 이는 자동으로 값이 입력 되는 것같다. series 객체를 생성 할 때 value와 Index를 직접 지정 해 줄 수 있다. 1234import pandas as pdsr = pd.Series([24000, 20000, 1000, 5000], index=[&quot;피자&quot;, &quot;치킨&quot;, &quot;콜라&quot;, &quot;생맥&quot;])print(sr) 구글 코랩에서 작업 하고 있는데,아래에 보면 series 객체의 parameter에대한 팝업이 나와 공부 하기 참 편하게 해 준다. def init(data=None, index=None, dtype=None, name=None, copy=False, fastpath=False)One-dimensional ndarray with axis labels (including time series). Labels need not be unique but must be a hashable type. The object supports both integer- and label-based indexing and provides a host of methods for performing operations involving the index. Statistical methods from ndarray have been overridden to automatically exclude missing data(currently represented as NaN).Operations between Series (+, -, /, *, **) align values based on their associated index values– they need not be the same length. The result index will be the sorted union of the two indexes. Parameters data : array-like, Iterable, dict, or scalar value Contains data stored in Series. index : array-like or Index (1d) Values must be hashable and have the same length as data. Non-unique index values are allowed. Will default to RangeIndex (0, 1, 2, …, n)if not provided. If both a dict and index sequence are used, the index willoverride the keys found in the dict. dtype : str, numpy.dtype, or ExtensionDtype, optional Data type for the output Series. If not specified, this will be inferred from data. See the user guide &lt;basics.dtypes&gt; for more usages. name : str, optional The name to give to the Series. copy : bool, default False Copy input data. type list [] tuple () set {} dict {Key:value} series pandas","link":"/2021/11/06/blog/series(pandas)/"},{"title":"Xenomix","text":"제노믹스 Ref","link":"/2021/12/11/job/XenoMix/"},{"title":"to prepare kaggle Competition","text":"#Kaggle Competition 준비하기 Kaggle Note 에서 작성됨. files and Library import 1234567891011121314151617181920212223import numpy as npimport pandas as pdimport seaborn as snsimport matplotlib.pylab as pltimport plotly.io as pioimport plotly.express as pximport plotly.graph_objects as goimport plotly.figure_factory as fffrom plotly.subplots import make_subplotsfrom plotly.offline import init_notebook_mode, iplotinit_notebook_mode(connected=True)pio.templates.default = &quot;none&quot;# import plotly.offline as py# py.offline.init_notebook_mode()import osfor dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename))import warningswarnings.filterwarnings(&quot;ignore&quot;) result /kaggle/input/kaggle-survey-2018/SurveySchema.csv /kaggle/input/kaggle-survey-2018/freeFormResponses.csv /kaggle/input/kaggle-survey-2018/multipleChoiceResponses.csv /kaggle/input/kaggle-survey-2017/freeformResponses.csv /kaggle/input/kaggle-survey-2017/schema.csv /kaggle/input/kaggle-survey-2017/RespondentTypeREADME.txt /kaggle/input/kaggle-survey-2017/multipleChoiceResponses.csv /kaggle/input/kaggle-survey-2017/conversionRates.csv /kaggle/input/kaggle-survey-2020/kaggle_survey_2020_responses.csv /kaggle/input/kaggle-survey-2020/supplementary_data/kaggle_survey_2020_methodology.pdf /kaggle/input/kaggle-survey-2020/supplementary_data/kaggle_survey_2020_answer_choices.pdf /kaggle/input/kaggle-survey-2021/kaggle_survey_2021_responses.csv /kaggle/input/kaggle-survey-2021/supplementary_data/kaggle_survey_2021_methodology.pdf /kaggle/input/kaggle-survey-2021/supplementary_data/kaggle_survey_2021_answer_choices.pdf /kaggle/input/kaggle-survey-2019/survey_schema.csv /kaggle/input/kaggle-survey-2019/multiple_choice_responses.csv /kaggle/input/kaggle-survey-2019/other_text_responses.csv /kaggle/input/kaggle-survey-2019/questions_only.csv dataframe create 12345df17= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2017/multipleChoiceResponses.csv&quot;, encoding=&quot;ISO-8859-1&quot;)df18= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2018/multipleChoiceResponses.csv&quot;, )df19= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2019/multiple_choice_responses.csv&quot;, )df20= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2020/kaggle_survey_2020_responses.csv&quot;, )df21= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2021/kaggle_survey_2021_responses.csv&quot;, ) data 확인하기 12345df21 = df21.iloc[1:, :]#df21.value_counts()#df21.countdf21.head() data 1개씩 표로 만들어서 불러오기 12345678910111213141516171819country = ( df21['Q3'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'country', 'Q3':'Count'}) .sort_values(by=['country'], ascending=False) ) Wage = ( df21['Q25'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'Wage', 'Q25':'Count'}) .sort_values(by=['Wage'], ascending=True) ) Wage Problem나라별 임금을 확인 하기 위해temp에 나라이름과 임금에 대해 넣었다.C_ls : country list라는 List를 만들어 나라 별로 임금을 뽑을 수 있었다. 123456# India, Russia, Chinatemp = df21[['Q3','Q25']]print(temp['Q3'].unique())C_Ls = temp['Q3'].unique() #나라이름 뽑아주기 12345tempR = temp[temp['Q3'] == str(C_Ls[0])].value_counts()# 이 경우에는 temp가 가공 할 수 없는 templet이기 때문에 plot을 그릴 수가 없다. trouble shooting12345678910111213#선생님temp= df21[['Q3','Q25']]temp = temp.groupby(['Q3','Q25']) .size() .reset_index() .rename(columns = {0:&quot;Count&quot;})C_Ls = temp['Q3'].unique()for i in range(1,len(df21['Q3'])): temp2 = temp[temp['Q3'] == str(C_Ls[i])] fig = px.bar(temp2, x='Q25', y='Count',title=C_Ls[i]) fig.show() 선생님의 도움을 받아 for문 안에 나라 이름을 넣어서 모든 나라의 임금을 확인 할 수 있었다. 모든 나라의 임금을 확인 했는데, 차별이 심한 나라를 찾기 힘들었다. 뭐때문에 이 graph를 그리려고 했는지 잃어버렸다. 한국에서 돈 잘 벌려면 어떤 직업, 어떤 … 을 해야 하는가 한국보다 평균임금이 더 많은 나라는? 한국 보다 평균임금이 더 많은 나라는 뭐가 다를까? 임금에 대한 것을 버려야 할까… 이런 재미있는 Issue에 대해 Note를 만들어 보려고 했는데 ㅎㅎ 코딩 실력이 안된당 흐흐 대회에서 잘 하려는 마음보다는 코드를 더 잘 읽고, 쓰는데 집중하자. ㅠㅠ D-17 (대회 종료까지) 대회 종료 final version","link":"/2021/11/11/kgg/KggComp_/"},{"title":"Newbie as a data scientist in East Asia! (kaggle Competition)","text":"Newbie as a data scientist in East Asia!notebook Hello, Kaggers! Nice to meet you! We are a team in East Asia that wants to be data scientists As newbies, we want to know what and/or how Kaggler is! so, let’s have a time to learn about Kaggle as a senior with us from now. If you want to support us (or feel qute) , I ask for a comment! (PLZ) ^0^ And !! Since we are not native English speakers, please ask questions if there is a context that you don’t understand because it’s not smooth. I’ll do my best to answer. 1 Introduction what is the Kagglea subsidiary of Google LLC, is an online community of data scientists and machine learning practitioners. If we use kaggle, we can take the following advantages. 1) to find and publish data sets 2) to explore and build models in a web-based data-science environment 3) to work with other data scientists and machine learning engineers 4) to enter competitions to solve data science challenges so, As data scientist beginners, we try to participate in the Kaggle competition. 21 Kaggle Machine Learning and Data Science Survey The most comprehensive dataset available for ML and data science status This is the theme of the competition we will participate in this time. To become a data scientist, we compared what kind of job Kagglers has, how much experience he has, and how much money he earns by dividing into the world and East Asia. In addition, there are detailed comparisons in East Asia, and ultimately, we will to find out what data the Kaggle competition data shows. The 2021 survey, like 2017, 2018, 2019, and 2020, launched an industry-wide survey that comprehensively presents the current status of data science and machine learning. The survey was conducted from 09/01/2021 to 10/04/2021, and after cleaning the data, Kaggle received 25,973 responses! This year, Kaggle will award $30,000 in prize money to winner in this competition. we want to receive $30,000 for winning the competition, but we just hope it will help us become a data scientist because it is difficult for a rookie. Ref. [1] Kgg_competitions [2] Kgg_definition [3] kaggle-survey-2021 1.2 Contents Introduction Contents Summary Data Import and Preprocessing Plots and Description Kaggle's transformation. (World/East_Asia) 1 user transformation 2 Gender transformation 3 Job transformation 4 Age transformation 5 Degree transformation 6 Experience transformation 7 Salary transformation 8 Language transformation Position of Data Scientist in East Asia 1 Salary 2 Salary-Experience 3 Degree 4 Salary-Degree 5 Language Discussion Close 1.3 Summary used data We used all the data for five years. (2017~2021) used Language and Library Numpy Metplotlib seaborn Plotly plotly.express : An interface where you can draw a graph easily and quickly. plotly.graph_objects : You can customize it in the way you want because you can do more detailed work than express. plotly.figure_factory : Used before express existed and remains in the module for compatibility with previous versions plotly.subplots : A module that displays multiple graphs in one figure. plotly.offline : Save locally and create HTML that opens in a web browser and make it standalone Grouping data sections East Asia and World East Asia : [‘China’,’Taiwan’, ‘South Korea’, ‘Japan’] World : all data Gender [Male, Female, Others] Job Data_Analyst =[‘Data Analyst’,’Data Miner,Information technology’,’Data Miner’, 'Predictive Modeler','Information technology, networking, or system administration', 'A business discipline (accounting, economics, finance, etc.)', 'Business Analyst', Humanities', 'Statistician', 'Mathematics or statistics', 'Medical or life sciences (biology, chemistry, medicine, etc.)', Physics or astronomy', 'Social sciences (anthropology, psychology, sociology, etc.)', 'Environmental science or geology', 'Humanities (history, literature, philosophy, etc.)'] Data_Scientist =[‘Data Scientist’, ‘Research Scientist’, ‘Researcher’,’Machine Learning Engineer’, ‘Scientist/Researcher’] Developer=[‘Developer Relations/Advocacy’,’Data Engineer’,’Engineer’,’Engineering (non-computer focused)’, ‘Programmer’,’Software Engineer’, ‘Computer Scientist’,’Computer science (software engineering, etc.)’, ‘Fine arts or performing arts’,’Product Manager’, ‘Software Developer/Software Engineer’, ‘Product/Project Manager’,’Program/Project Manager’,’DBA/Database Engineer’] Not_Employed =[‘Currently not employed’, ‘Not employed’, ‘Student’] Others = [‘I never declared a major’, ‘Other’] Age [18-21, 20s, 30s, 40s, 50s, 60s&lt;] Degree [‘college’, ‘Bachelor’s degree’,’Master’s degree’, ‘Doctoral degree‘, ‘etc’] Experience [&lt;1, 1-3, 3-5, 5-10, 10+] Salary [&lt;999, 1,000-20,000, 20,000-59,999, 60,000-99,999, 100,000-199,999, 200,000~] 2. data Import and pre-treatments 1234567891011121314151617181920import numpy as npimport pandas as pdimport seaborn as snsimport matplotlib.pylab as pltimport plotly.io as pioimport plotly.express as pximport plotly.graph_objects as goimport plotly.figure_factory as fffrom plotly.subplots import make_subplotsfrom plotly.offline import init_notebook_mode, iplotinit_notebook_mode(connected=True)pio.templates.default = &quot;none&quot;import osfor dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename))import warningswarnings.filterwarnings(&quot;ignore&quot;) 12345df17= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2017/multipleChoiceResponses.csv&quot;, encoding=&quot;ISO-8859-1&quot;)df18= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2018/multipleChoiceResponses.csv&quot;, )df19= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2019/multiple_choice_responses.csv&quot;, )df20= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2020/kaggle_survey_2020_responses.csv&quot;, )df21= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2021/kaggle_survey_2021_responses.csv&quot;, ) 3. plots and description 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109#질문 제거하기, replacedf17= df17.iloc[1:, :].replace(&quot;People 's Republic of China&quot;,'China')df18= df18.iloc[1:, :].replace('Republic of Korea','South Korea')df19= df19.iloc[1:, :].replace('Republic of Korea','South Korea')df20= df20.iloc[1:, :].replace('Republic of Korea','South Korea')df21= df21.iloc[1:, :]## East Asia에는 대한민국, 일본, 중국, 타이완, 몽골, 북조선 총 6개의 국가가 속해 있다. ## 이유는 알 수 없지만, 18년도엔 타이완이 없다. EastAsia17 = ['China',&quot;People 's Republic of China&quot;, 'Taiwan', 'South Korea', 'Japan']EastAsia18= ['China', 'South Korea', 'Japan', 'Republic of Korea'] EastAsia19 = ['China','Taiwan', 'South Korea', 'Japan', 'Republic of Korea']EastAsia20 = ['China','Taiwan', 'South Korea','Republic of Korea', 'Japan']EastAsia21 = ['China','Taiwan', 'South Korea', 'Japan']EastAsia = ['Republic of Korea','China','Taiwan', 'South Korea', 'Japan', &quot;People 's Republic of China&quot; ]df21_Ea = df21[df21['Q3'].isin(EastAsia)]df21_Wo = df21[~df21['Q3'].isin(EastAsia)]df21['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df21['Q3']]df20_Ea = df20[df20['Q3'].isin(EastAsia)]df20_Wo = df20[~df20['Q3'].isin(EastAsia)]df20['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df20['Q3']]df19_Ea = df19[df19['Q3'].isin(EastAsia)]df19_Wo = df19[~df19['Q3'].isin(EastAsia)]df19['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df19['Q3']]df18_Ea = df18[df18['Q3'].isin(EastAsia)]df18_Wo = df18[~df18['Q3'].isin(EastAsia)]df18['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df18['Q3']]df17_Ea = df17[df17['Country'].isin(EastAsia)]df17_Wo = df17[~df17['Country'].isin(EastAsia)]df17['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df17['Country']]df21['year'] = '2021'df20['year'] = '2020'df19['year'] = '2019'df18['year'] = '2018'df17['year'] = '2017'years = ['2017', '2018', '2019', '2020', '2021']df21_Ea = df21[df21['Q3'].isin(EastAsia21)]Ea21= ( df21_Ea['Q3'].value_counts().to_frame() .reset_index().rename(columns={'index':'Country', 'Q3':'21'}))df20_Ea=df20[df20['Q3'].isin(EastAsia)]Ea20= ( df20_Ea['Q3'].replace('Republic of Korea','South Korea') .value_counts().to_frame().reset_index() .rename(columns={'index':'Country', 'Q3':'20'}))df19_Ea=df19[df19['Q3'].isin(EastAsia)]Ea19= (df19_Ea['Q3'].replace('Republic of Korea','South Korea') .value_counts().to_frame().reset_index() .rename(columns={'index':'Country', 'Q3':'19'}))df18_Ea=df18[df18['Q3'].isin(EastAsia)]Ea18= (df18_Ea['Q3'].replace('Republic of Korea','South Korea') .value_counts().to_frame().reset_index() .rename(columns={'index':'Country', 'Q3':'18'}))Ea18.value_counts()#df18 열에 taiwan = 0을 추가 해야 합니다. df17_Ea = df17[df17['Country'].isin(EastAsia)]Ea17= (df17_Ea['Country'].replace(&quot;People 's Republic of China&quot;,'China') .value_counts().to_frame().reset_index() .rename(columns={'index':'Country', 'Country':'17'}))#data를 합쳐서 하나의 dataframe으로 만들어 줌.df5years = pd.merge(Ea17, Ea18, on='Country', how='outer')df5year =pd.merge(Ea19,Ea20, on='Country', how='outer')df5year=pd.merge(df5year, Ea21, on='Country', how='outer')df5years = pd.merge(df5years, df5year, on='Country', how='outer')Ea21 = len(df21_Ea)Wo21 = len(df21) - len(df21_Ea)Ea20 = len(df20_Ea)Wo20 = len(df20) - len(df20_Ea)Ea19 = len(df19_Ea)Wo19 = len(df19) - len(df19_Ea)Ea18 = len(df18_Ea)Wo18 = len(df18) - len(df18_Ea)Ea17 = len(df17_Ea)Wo17 = len(df17) - len(df17_Ea)years = ['2017','2018','2019','2020', '2021']def percent (a, b): result =a/(a+b)*100 result = np.round(result, 2) return resultdef percentR (b, a): result =a/(a+b)*100 result = np.round(result, 2) return resultpercent = [percent(Ea17, Wo17), percent(Ea18, Wo18), percent(Ea19, Wo19), percent(Ea20, Wo20), percent(Ea21, Wo21)] 3.1 Kaggle’s transformation (World/East Asia) 3.1.1 user transformation Number of respondents (bar, scatter plot : number of respondents to World and East Asia,Map plot : number of respondents to East Asia) World and East Asia: The same trend. East Asia: 15% of the total continent and 20.3% of the population (16/78.7: Ea/Wo) 2018 Issue: Significant increase in respondents-&gt;Hypothesis: Due to the rapid increase in China. 2018 Outliers Considering: 2022 Kaggle survey Respondents: Increased in both World and East Asia I wish our team the honor of becoming a respondent to the Kaggle survey in 2022…. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980fig = go.Figure()y=[len(df17_Ea),len(df18_Ea), len(df19_Ea),len(df20_Ea),len(df21_Ea)]fig.add_trace(go.Bar(x=years, y=y, base=0, marker_color='#F2D64B', yaxis = &quot;y1&quot;, name='East Asia', text= percent, texttemplate='%{text} %', textposition='outside', hovertemplate='&lt;b&gt;KaggleUser&lt;/b&gt;: %{x}&lt;br&gt;'+ '&lt;b&gt;Count&lt;/b&gt;: %{y}'))fig.add_trace(go.Scatter(name = &quot;World&quot;, x=years, y=[len(df17), len(df18), len(df19), len(df20), len(df21)], marker_color='#979DA6', mode = 'lines+markers', # please check option here yaxis = &quot;y2&quot;))fig.update_traces(hovertemplate='&lt;b&gt;Count&lt;/b&gt;: %{y}&lt;br&gt;&lt;extra&gt;&lt;/extra&gt;'+ '&lt;b&gt;Year&lt;/b&gt;: %{x}&lt;br&gt;')fig.update_layout(yaxis = dict(title = &quot;Kaggle User in East Asia&quot;,showgrid = False, range=[0, len(df21_Ea)*1.2]), yaxis2 = dict(title = &quot;Kaggle User in World&quot;, overlaying = &quot;y1&quot;, side = &quot;right&quot;, showgrid = False, zeroline = False, range=[0, len(df21)*1.2]))fig.update_layout(title='&lt;b&gt;Kaggle Users&lt;/b&gt;',title_font_size=20, margin = dict(t=200, l=100, r=50, b=200), height=700, width=700)fig.update_layout(legend=dict( orientation=&quot;h&quot;, yanchor=&quot;bottom&quot;, y=1.1, xanchor=&quot;right&quot;, x=1))fig.add_annotation(dict(font=dict(size=14), x=0.9, y=-0.25, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show()def world_map(locations,counts,title): data = [ dict( type = 'choropleth', locations = locations, z = counts, colorscale = 'Reds', locationmode = 'country names', autocolorscale = False, reversescale = False, marker = dict( line = dict(color = '#F7F7F7', width = 1.5)), colorbar = dict(autotick = True, legth = 3, len=0.75, title = 'respodents', max = 1000, min = 0))] layout = dict( title=title, titlefont={'size': 28}, width=700, height=600, paper_bgcolor='#FFFFFF', margin=dict(l=50, r=50, t=100, b=100), geo = dict( showframe = True, showcoastlines = True, fitbounds=&quot;locations&quot;)) fig = dict(data=data, layout=layout) iplot(fig, validate=False, filename='world-map')z = df21_Ea['Q3'].value_counts() world_map(locations=z.index, counts=z.values, title= '&lt;b&gt;EastAsia Countries&lt;b&gt;') 18’ : User change between United States and India. China’s markedly increase in 2018 There is no Taiwan, but only China has increased. : East Asian political situation Issue can be suspected. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384A18 = ( df18['Q3'] .replace({'Republic of Korea':'South Korea', 'I do not wish to disclose my location' : 'Other'}) .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'Q3':'2018'}) .groupby('type') .sum() .reset_index())A19 = ( df19['Q3'] .replace('Republic of Korea','South Korea') .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'Q3':'2019'}) .groupby('type') .sum() .reset_index())A17 = ( df17['Country'] .replace({'United States': 'United States of America', 'Hong Kong': 'Hong Kong (S.A.R.)', 'United Kingdom':'United Kingdom of Great Britain and Northern Ireland', }) .replace(&quot;People 's Republic of China&quot;,'China') .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'Country':'2017'}) .groupby('type') .sum() .reset_index())A18A19=pd.merge(A18,A19, how='outer',on='type').fillna(0)A18A17=pd.merge(A18,A17, how='outer',on='type').fillna(0)A18A19['minus']= A18A19['2018']-A18A19['2019']A18A17['minus']= A18A17['2018']-A18A17['2017']A18A17=A18A17.sort_values(by=&quot;minus&quot;, ascending=False)A18A19=A18A19.sort_values(by=&quot;minus&quot;, ascending=False)fig = go.Figure(data=[ go.Bar(x =A18A19['type'], y = A18A19['minus'], marker_color='#979DA6', name = '2018-2019', base=0), go.Bar(x =A18A17['type'], y = A18A17['minus'], marker_color='#F2D64B', name = '2018-2017', base=0) ])fig.update_layout(title='&lt;b&gt; Predicting outliers (2018)&lt;/b&gt;',title_font_size=20, margin = dict(t=200, l=100, r=10, b=200), height=700, width=700, xaxis_title=None, yaxis_title=None)fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.update_layout(legend=dict( orientation=&quot;h&quot;, yanchor=&quot;bottom&quot;, y=1.1, xanchor=&quot;right&quot;, x=1))fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.5, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() Total population: 1.4 billion (85%) in China, 130 million in Japan, 0.5 billion in Korea, and 0.2 billion in Taiwan. China: The number of respondents is smaller than the population. Japan: Starting in 2019, overtaking China Taiwan : 2018 data 0 =? Diplomatic issues? The growth trend is weak. Korea : Respondents at a similar level to Japan’s population. East Asia: The number of respondents will increase further. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485#data preprocessingtotal17 = ( df17['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index())total18 = ( df18['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index())total19 = ( df19['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index())total20 = ( df20['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index())total21 = ( df21['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index())#graphcolors = ['#F2D64B','#979DA6']fig = make_subplots(rows=1, cols=5, specs=[[{'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}]], subplot_titles=(&quot;2017&quot;, &quot;2018&quot;, &quot;2019&quot;, &quot;2020&quot;, &quot;2021&quot;))fig.add_trace(go.Pie(marker=dict(colors=colors),labels=total21['type'], values=total21['respodents'], name=&quot;2021&quot;, scalegroup='one'), 1, 5)fig.add_trace(go.Pie(marker=dict(colors=colors),labels=total20['type'], values=total20['respodents'], name=&quot;2020&quot;, scalegroup='one'), 1, 4)fig.add_trace(go.Pie(marker=dict(colors=colors),labels=total19['type'], values=total19['respodents'], name=&quot;2019&quot;, scalegroup='one'), 1, 3)fig.add_trace(go.Pie(marker=dict(colors=colors),labels=total18['type'], values=total18['respodents'], name=&quot;2018&quot;, scalegroup='one'), 1, 2)fig.add_trace(go.Pie(marker=dict(colors=colors),labels=total17['type'], values=total17['respodents'], name=&quot;2017&quot;, scalegroup='one'), 1, 1)fig.update_traces(hole=.0, hoverinfo=&quot;label+percent+name&quot;, textposition='inside', textinfo='percent+label', textfont_size=12)fig.update_layout(title='&lt;b&gt;World vs EastAsia&lt;/b&gt;',title_font_size=23, margin = dict(t=300, l=0, r=0, b=200), height=700, width=700)fig.update_layout(legend=dict( orientation=&quot;h&quot;, yanchor=&quot;bottom&quot;, y=1.3, xanchor=&quot;right&quot;, x=1))fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.25, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() 123456789101112131415161718192021222324252627282930fig = go.Figure(data=[ go.Bar(name='2017', x=df5years['Country'], y=df5years['17'], marker_color='#F2798F',text=df5years['17'].tolist(), textposition='outside'), go.Bar(name='2018', x=df5years['Country'], y=df5years['18'], marker_color='#88BFBA',text=df5years['18'].fillna(0).astype(int).tolist(), textposition='outside',), go.Bar(name='2019', x=df5years['Country'], y=df5years['19'], marker_color='#CDD9A3',text=df5years['19'].tolist(), textposition='outside'), go.Bar(name='2020', x=df5years['Country'], y=df5years['20'], marker_color='#F28705',text=df5years['20'].tolist(), textposition='outside',), go.Bar(name='2021', x=df5years['Country'], y=df5years['21'], marker_color='#D9946C',text=df5years['21'].tolist(), textposition='outside')])fig.update_layout(barmode='group')fig.update_layout(title='&lt;b&gt;Kaggle User in East Asia&lt;/b&gt;',title_font_size=23, margin = dict(t=200, l=100, r=10, b=200), height=600, width=700)fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.update_traces(hovertemplate='&lt;b&gt;Count&lt;/b&gt;: %{y}')fig.update_layout(legend=dict( orientation=&quot;v&quot;, yanchor=&quot;bottom&quot;, y=1.15, xanchor=&quot;right&quot;, x=1))fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.5, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() 3.1.2 Gender transformation World: The proportion of female respondents increases (still below 20%) The number of respondents is increasing in all genders. Our team is also a team with high female members and wants to contribute as a respondent in 2022. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495#data preprocessingGender_17 = ( df17['GenderSelect'] .replace(['A different identity', 'Prefer to self-describe', 'Non-binary, genderqueer, or gender non-conforming'], 'Others') .fillna('Others') .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'GenderSelect':'Gender'}) .groupby('type') .sum() .reset_index())Gender_18 = ( df18['Q1'] .replace(['Prefer not to say', 'Prefer to self-describe'], 'Others') .fillna('Others') .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'Q1':'Gender'}) .groupby('type') .sum() .reset_index())Gender_19 = ( df19['Q2'] .replace(['Prefer not to say','Prefer to self-describe'],'Others') .fillna('Others') .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'Q2':'Gender'}) .groupby('type') .sum() .reset_index())Gender_20 = ( df20['Q2'] .replace(['Prefer not to say', 'Prefer to self-describe', 'Nonbinary'], 'Others') .replace(['Man', 'Woman'], ['Male', 'Female']) .fillna('Others') .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'Q2':'Gender'}) .groupby('type') .sum() .reset_index())Gender_21 = ( df21['Q2'] .replace(['Prefer not to say', 'Prefer to self-describe', 'Nonbinary'], 'Others') .replace(['Man', 'Woman'], ['Male', 'Female']) .fillna('Others') .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'Q2':'Gender'}) .groupby('type') .sum() .reset_index())colors = ['#D9946C','#88BFBA', '#CDD9A3']fig = make_subplots(rows=1, cols=5, specs=[[{'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}]],)fig.add_trace(go.Pie(marker=dict(colors=colors), labels=Gender_21['type'], values=Gender_21['Gender'], name=&quot;2021&quot;, scalegroup='one', text=np.array(Gender_21['Gender'].sum()), title=&quot;2021&quot;, titleposition='bottom center'), 1, 5)fig.add_trace(go.Pie(marker=dict(colors=colors), labels=Gender_20['type'], values=Gender_20['Gender'], name=&quot;2020&quot;, scalegroup='one', text=np.array(Gender_20['Gender'].sum()), title=&quot;2020&quot;, titleposition='bottom center'), 1, 4)fig.add_trace(go.Pie(marker=dict(colors=colors), labels=Gender_19['type'], values=Gender_19['Gender'], name=&quot;2019&quot;, scalegroup='one', text=np.array(Gender_19['Gender'].sum()), title=&quot;2019&quot;, titleposition='bottom center'), 1, 3)fig.add_trace(go.Pie(marker=dict(colors=colors), labels=Gender_18['type'], values=Gender_18['Gender'], name=&quot;2018&quot;, scalegroup='one', text=np.array(Gender_18['Gender'].sum()), title=&quot;2018&quot;, titleposition='bottom center'), 1, 2)fig.add_trace(go.Pie(marker=dict(colors=colors), labels=Gender_17['type'], values=Gender_17['Gender'], name=&quot;2017&quot;, scalegroup='one', text=np.array(Gender_17['Gender'].sum()), title=&quot;2017&quot;, titleposition='bottom center'), 1, 1)fig.update_traces(hole=.0, hoverinfo=&quot;label+percent+name&quot;, textinfo='label+percent+value')fig.update_layout(title='&lt;b&gt;World Gender&lt;/b&gt;',title_font_size=23, margin = dict(t=300, l=100, r=0, b=200), height=700, width=1000)fig.update_layout(legend=dict( orientation=&quot;v&quot;, yanchor=&quot;bottom&quot;, y=1.3, xanchor=&quot;right&quot;, x=1))fig.add_annotation(dict(font=dict(size=14), x=0.85, y=-0.5, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() - Male (1004-&gt;2037 : 2017-&gt;2021) double increase - Female 183-&gt;327 : 2017-&gt;2021 increased 1.8 times - Others (8-&gt;64 : 2017-&gt;2021) 8x increase [Compare the high and low points] It can be seen that the number of female respondents and the ratio of male respondents hardly change, which is a difference compared to World data. It can be seen that the degree of gender freedom in East Asia has increased relatively. Compared to World data, it can be seen that in 2021 (1.87: 2.6= Wo: Ea), compared to 2017 (1.96: 0.7 = Ea), which was relatively conservative. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#data preprocessinggender21= df21_Ea.loc[:, ['Q3', 'Q2', 'year']].rename(columns={'Q3':'Country', 'Q2':'Gender'})gender20= df20_Ea.loc[:, ['Q3', 'Q2', 'year']].rename(columns={'Q3':'Country', 'Q2':'Gender'})gender19= df19_Ea.loc[:, ['Q3', 'Q2', 'year']].rename(columns={'Q3':'Country', 'Q2':'Gender'})gender18= df18_Ea.loc[:, ['Q3', 'Q1', 'year']].rename(columns={'Q3':'Country', 'Q1':'Gender'})gender17= df17_Ea.loc[:, ['Country', 'GenderSelect', 'year']].rename(columns={'index':'type', 'GenderSelect':'Gender'})Gender5y= pd.concat([gender17, gender18, gender19, gender20, gender21])Gender5y= (Gender5y.replace(['Prefer not to say', 'Prefer to self-describe', 'Nonbinary', 'A different identity'], 'Others') .replace(['Man', 'Woman'], ['Male', 'Female']) .groupby(['year', 'Gender']) .size() .reset_index() .rename(columns = {0:&quot;Count&quot;}))gen17_5y = Gender5y[Gender5y['year'] == &quot;2017&quot;].reset_index(drop = True)gen18_5y = Gender5y[Gender5y['year'] == &quot;2018&quot;].reset_index(drop = True)gen19_5y = Gender5y[Gender5y['year'] == &quot;2019&quot;].reset_index(drop = True)gen20_5y = Gender5y[Gender5y['year'] == &quot;2020&quot;].reset_index(drop = True)gen21_5y = Gender5y[Gender5y['year'] == &quot;2021&quot;].reset_index(drop = True)Gen5y_ = pd.concat([gen17_5y, gen18_5y, gen19_5y, gen20_5y, gen21_5y], ignore_index = True)Gen5y_= pd.pivot(Gen5y_, index = &quot;year&quot;, columns = &quot;Gender&quot;, values = &quot;Count&quot;).reset_index()Gen5y_Gen5y_['year'].unique()#graphfig = go.Figure()fig.add_trace(go.Bar( x = Gen5y_['year'], y = Gen5y_['Male'].tolist(), name = 'Male',marker_color='#88BFBA', text=Gen5y_['Male'].tolist(), textposition='outside'))fig.add_trace(go.Bar( x = Gen5y_['year'], y = Gen5y_['Female'].tolist(), name = 'Female',marker_color='#D9946C', text=Gen5y_['Female'].tolist(), textposition='outside'))fig.add_trace(go.Bar( x = Gen5y_['year'], y = Gen5y_['Others'].tolist(), name = 'Others',marker_color='#CDD9A3', text=Gen5y_['Others'].tolist(), textposition='outside'))fig.update_layout(barmode=&quot;group&quot;) fig.update_layout(title='&lt;b&gt;Gender by year&lt;/b&gt;',title_font_size=22, margin = dict(t=200, l=100, r=10, b=200), height=700, width=700, xaxis_title=None, yaxis_title=None)fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.5, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() 3.1.3 Job transformation 21' World Vs East Asia Age Ratio: Bar plot Not Employed : More than 30% in both East Asia and the world, the highest. Because “Students” is included. Data Scientist : High percentage in the world and East Asia. Relatively low proportion in East Asia. = Absolute lack of numbers We would like to move forward by selecting a **data scientist** with insufficient numbers in East Asia. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132#data preprocessingData_Analyst =['Data Analyst','Data Miner,Information technology','Data Miner', 'Predictive Modeler','Information technology, networking, or system administration', 'A business discipline (accounting, economics, finance, etc.)', 'Business Analyst', 'Humanities', 'Statistician', 'Mathematics or statistics', 'Medical or life sciences (biology, chemistry, medicine, etc.)', 'Physics or astronomy', 'Social sciences (anthropology, psychology, sociology, etc.)', 'Environmental science or geology', 'Humanities (history, literature, philosophy, etc.)']Data_Scientist =['Data Scientist', 'Research Scientist', 'Researcher', 'Machine Learning Engineer', 'Scientist/Researcher']Developer=['Developer Relations/Advocacy','Data Engineer','Engineer','Engineering (non-computer focused)', 'Programmer','Software Engineer', 'Computer Scientist','Computer science (software engineering, etc.)', 'Fine arts or performing arts','Product Manager', 'Software Developer/Software Engineer', 'Product/Project Manager','Program/Project Manager','DBA/Database Engineer']Not_Employed =['Currently not employed', 'Not employed', 'Student']Others = ['I never declared a major', 'Other']df21job_Ea = df21_Ea.loc[:,['Q3','Q5']].rename(columns={'Q5':'2021'}).fillna('Other')df20job_Ea = df20_Ea.loc[:,['Q3','Q5']].rename(columns={'Q5':'2020'}).fillna('Other')df19job_Ea = df19_Ea.loc[:,['Q3','Q5']].rename(columns={'Q5':'2019'}).fillna('Other')df18job_Ea = df18_Ea.loc[:,['Q3','Q5']].rename(columns={ 'Q5':'2018'}).fillna('Other')df17job_Ea = df17_Ea.loc[:,['Country','CurrentJobTitleSelect']].rename(columns={'CurrentJobTitleSelect':'2017'}).fillna('Other')df21job_Ea.value_counts('2021')df21job_Ea['JOB']=[&quot;Data Analyst&quot; if x in Data_Analyst else &quot;Data Scientist&quot; if x in Data_Scientist # Data Scientist else &quot;Developer&quot; if x in Developer else &quot;NotEmployed&quot; if x in Not_Employed else &quot;Others&quot; for x in df21job_Ea['2021']]df21job_Ea.value_counts('JOB')df20job_Ea.value_counts('2020')df20job_Ea['JOB']=[&quot;Data Analyst&quot; if x in Data_Analyst else &quot;Data Scientist&quot; if x in Data_Scientist else &quot;Developer&quot; if x in Developer else &quot;NotEmployed&quot; if x in Not_Employed else &quot;Other&quot; for x in df20job_Ea['2020']]df20job_Ea[['2020','JOB']]df19job_Ea.value_counts('2019')df19job_Ea['JOB']=[&quot;Data Analyst&quot; if x in Data_Analyst else &quot;Data Scientist&quot; if x in Data_Scientist else &quot;Developer&quot; if x in Developer else &quot;NotEmployed&quot; if x in Not_Employed else &quot;Other&quot; for x in df19job_Ea['2019']]df19jobTest = df19job_Ea.loc[df19job_Ea.JOB == 'Other']df19jobTest['2019'].value_counts()df18job_Ea.value_counts('2018')df18job_Ea['JOB']=[&quot;Data Analyst&quot; if x in Data_Analyst else &quot;Data Scientist&quot; if x in Data_Scientist else &quot;Developer&quot; if x in Developer else &quot;NotEmployed&quot; if x in Not_Employed else &quot;Other&quot; for x in df18job_Ea['2018']]df18jobTest = df18job_Ea.loc[df18job_Ea.JOB == 'Other']df18jobTest['2018'].value_counts()df17job_Ea.value_counts('2017')df17job_Ea['JOB']=[&quot;Data Analyst&quot; if x in Data_Analyst else &quot;Data Scientist&quot; if x in Data_Scientist else &quot;Developer&quot; if x in Developer else &quot;NotEmployed&quot; if x in Not_Employed else &quot;Other&quot; for x in df17job_Ea['2017']]df17jobTest = df17job_Ea.loc[df17job_Ea.JOB == 'Other']df17jobTest['2017'].value_counts()df21jobTest = df21job_Ea.loc[df21job_Ea.JOB == 'Other']df21jobTest['2021'].head()df21job_Ea.value_counts('JOB')dfjob21 =df21job_Ea.groupby(['Q3','JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;}).rename(columns={'Q3':'country'})dfjob20 =df20job_Ea.groupby(['Q3','JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;}).rename(columns={'Q3':'country'})dfjob19 =df19job_Ea.groupby(['Q3','JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;}).rename(columns={'Q3':'country'})dfjob18 =df18job_Ea.groupby(['Q3','JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;}).rename(columns={'Q3':'country'})dfjob17 =df17job_Ea.groupby(['Country','JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;}).rename(columns={'Country':'country'})df21_Ea_job =df21job_Ea.groupby(['JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;})df20_Ea_job =df20job_Ea.groupby(['JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;})df19_Ea_job =df19job_Ea.groupby(['JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;})df18_Ea_job =df18job_Ea.groupby(['JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;})df17_Ea_job =df17job_Ea.groupby(['JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;})df21_DA=df21[df21['Q5'].isin(Data_Analyst)]df21_DS=df21[df21['Q5'].isin(Data_Scientist)]df21_D=df21[df21['Q5'].isin(Developer)]df21_N=df21[df21['Q5'].isin(Not_Employed)]df21_O=df21[df21['Q5'].isin(Others)]World_ = np.array([df21_DA['Q5'].count(), df21_DS['Q5'].count(), df21_D['Q5'].count(), df21_N['Q5'].count(), df21_O['Q5'].count()]) East_Asia_ = df21_Ea_job['Count'].to_numpy()World =((World_/World_.sum())*100).round(1)East_Asia =((East_Asia_/East_Asia_.sum())*100).round(1)y = df21_Ea_job.JOB.to_numpy()fig = go.Figure(data=[ go.Bar(y=y, x=World, orientation='h', name=&quot;World&quot;, base=0, hovertemplate='&lt;b&gt;World&lt;/b&gt;: %{x}%&lt;br&gt;', marker_color='#979DA6', text=World, textposition='outside'), go.Bar(y=y, x=-East_Asia, orientation='h', name=&quot;East Asia&quot;, base=0, hovertemplate='&lt;b&gt;East Asia&lt;/b&gt;: %{x}%&lt;br&gt;', marker_color='#F2D64B', text=East_Asia, textposition='outside')])fig.update_layout(barmode='stack')fig.update_layout(title='&lt;b&gt;World vs EastAsia&lt;/b&gt;',title_font_size=22, margin = dict(t=200, l=100, r=50, b=200), height=700, width=750, xaxis_title=None, yaxis_title=None)fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.update_layout(legend=dict( orientation=&quot;h&quot;, yanchor=&quot;bottom&quot;, y=1.1, xanchor=&quot;right&quot;, x=1))fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.5, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() World Job Ratio: Heat Map The trend of increasing each job except Others. Data Scientist has a high proportion, and the trend is to increase further in 2022. East Asia Job Ratio: Heat Map East Asia : Increasing the ratio of data scientist. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113#data preprocessingdf21job= df21.loc[:,['region','Q5']].rename(columns={'Q5':'2021'}).fillna('Others')df20job= df20.loc[:,['region','Q5']].rename(columns={'Q5':'2020'}).fillna('Others')df19job= df19.loc[:,['region','Q5']].rename(columns={'Q5':'2019'}).fillna('Others')df18job= df18.loc[:,['region','Q6']].rename(columns={ 'Q6':'2018'}).fillna('Others')df17job= df17.loc[:,['region','CurrentJobTitleSelect']].rename(columns={'CurrentJobTitleSelect':'2017'}).fillna('Others')df21job['JOB']=[&quot;Data Analyst&quot; if x in Data_Analyst else &quot;Data Scientist&quot; if x in Data_Scientist # Data Scientist else &quot;Developer&quot; if x in Developer else &quot;NotEmployed&quot; if x in Not_Employed else &quot;Others&quot; for x in df21job['2021']]df20job['JOB']=[&quot;Data Analyst&quot; if x in Data_Analyst else &quot;Data Scientist&quot; if x in Data_Scientist else &quot;Developer&quot; if x in Developer else &quot;NotEmployed&quot; if x in Not_Employed else &quot;Others&quot; for x in df20job['2020']]df19job['JOB']=[&quot;Data Analyst&quot; if x in Data_Analyst else &quot;Data Scientist&quot; if x in Data_Scientist else &quot;Developer&quot; if x in Developer else &quot;NotEmployed&quot; if x in Not_Employed else &quot;Others&quot; for x in df19job['2019']]df18job['JOB']=[&quot;Data Analyst&quot; if x in Data_Analyst else &quot;Data Scientist&quot; if x in Data_Scientist else &quot;Developer&quot; if x in Developer else &quot;NotEmployed&quot; if x in Not_Employed else &quot;Others&quot; for x in df18job['2018']]df17job['JOB']=[&quot;Data Analyst&quot; if x in Data_Analyst else &quot;Data Scientist&quot; if x in Data_Scientist else &quot;Developer&quot; if x in Developer else &quot;NotEmployed&quot; if x in Not_Employed else &quot;Others&quot; for x in df17job['2017']]df21_job =df21job.groupby(['JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;})df20_job =df20job.groupby(['JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;})df19_job =df19job.groupby(['JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;})df18_job =df18job.groupby(['JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;})df17_job =df17job.groupby(['JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;})merge11=pd.merge(df21_job,df20_job, how='outer',on='JOB')merge21=pd.merge(df19_job,df18_job, how='outer',on='JOB')merge31=pd.merge(merge11,merge21, how='outer',on='JOB')merge_Wo=(pd.merge(merge31,df17_job, how='outer',on='JOB') .rename(columns = {'Count_x_x':'2021','Count_y_x':'2020','Count_x_y':'2019','Count_y_y':'2018','Count':'2017'}).fillna(0) .reindex(columns = ['JOB','2017','2018','2019','2020','2021' ]))df21job_Ea = df21job[df21job['region'] == 'EastAsia'].loc[:,['region','JOB']].rename(columns={'region':'EastAsia'})df20job_Ea = df20job[df20job['region'] == 'EastAsia'].loc[:,['region','JOB']].rename(columns={'region':'EastAsia'})df19job_Ea = df19job[df19job['region'] == 'EastAsia'].loc[:,['region','JOB']].rename(columns={'region':'EastAsia'})df18job_Ea = df18job[df18job['region'] == 'EastAsia'].loc[:,['region','JOB']].rename(columns={'region':'EastAsia'})df17job_Ea = df17job[df17job['region'] == 'EastAsia'].loc[:,['region','JOB']].rename(columns={'region':'EastAsia'})df21job_Ea =df21job_Ea.groupby(['JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;})df20job_Ea =df20job_Ea.groupby(['JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;})df19job_Ea =df19job_Ea.groupby(['JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;})df18job_Ea =df18job_Ea.groupby(['JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;})df17job_Ea =df17job_Ea.groupby(['JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;})merge1=pd.merge(df21job_Ea,df20job_Ea, how='outer',on='JOB')merge2=pd.merge(df19job_Ea,df18job_Ea, how='outer',on='JOB')merge3=pd.merge(merge1,merge2, how='outer',on='JOB')merge=(pd.merge(merge3,df17job_Ea, how='outer',on='JOB') .rename(columns = {'Count_x_x':'2021','Count_y_x':'2020','Count_x_y':'2019','Count_y_y':'2018','Count':'2017'}).fillna(0) .reindex(columns = ['JOB','2017','2018','2019','2020','2021' ]))#graphz1=((merge_Wo.iloc[:,[1,2,3,4,5]].to_numpy()/merge_Wo.iloc[:,[1,2,3,4,5]].to_numpy().sum())*100).round(1)z2=((merge.iloc[:,[1,2,3,4,5]].to_numpy()/merge.iloc[:,[1,2,3,4,5]].to_numpy().sum())*100).round(1)x=['2017-year','2018-year','2019-year','2020-year','2021-year']y1=merge_Wo['JOB'].tolist()y2=merge['JOB'].tolist()fig1 = ff.create_annotated_heatmap(z1, x = x, y = y1, colorscale='sunset')fig2 = ff.create_annotated_heatmap(z2, x = x, y = y2, colorscale='sunset')for annot in fig2['layout']['annotations']: annot['xref'] = 'x2' fig = make_subplots(rows=1, cols=2)fig.add_trace(fig1.data[0], row=1, col=1)fig.add_trace(fig2.data[0], row=1, col=2)fig.update_layout(fig1.layout, title='&lt;b&gt; World vs EastAsia&lt;/b&gt;',title_font_size=22, margin = dict(t=200, l=100, r=10, b=200), height=700, width=1150, coloraxis=dict(showscale=True, colorscale='sunset'))fig.update_traces(hovertemplate='&lt;b&gt;Job&lt;/b&gt;: %{y}&lt;br&gt;'+ '&lt;b&gt;Year&lt;/b&gt;: %{x}&lt;br&gt;'+ '&lt;b&gt;Percent&lt;/b&gt;: %{z}%')fig.layout.annotations += fig2.layout.annotationsfig.add_annotation(dict(font=dict(size=14), x=0.9, y=-0.25, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() 3.1.4 Age transformation > Age change in World and East Asia by year: Stacked scatter plot In the case of Age data, there is no 2017 data. 70% of the World respondents said 20s to 30s. 70% of East Asia respondents said 20s to 30s. The number of respondents increases, but the ratio seems to have stabilized. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224#data preprocessing#WorldAge21_W = df21.loc[:,['Q3','Q1', 'year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'age'}).fillna('etc')Age20_W = df20.loc[:,['Q3','Q1','year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'age'}).fillna('etc')Age19_W = df19.loc[:,['Q3','Q1','year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'age'}).fillna('etc')Age18_W = df18.loc[:,['Q3','Q2','year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q2':'age'}).fillna('etc')Age5y_W= pd.concat([Age21_W, Age20_W, Age19_W, Age18_W])Age5y_W= (Age5y_W.replace(['60-69', '70+', '70-79', '80+'], '60+') .replace(['22-24', '25-29'], '22-29') .replace(['30-34', '35-39'], '30-39') .replace(['40-44', '45-49'], '40-49') .replace(['50-54', '55-59'], '50-59') .groupby(['year', 'age']) .size() .reset_index() .rename(columns = {0:&quot;Count&quot;}))Age21_percent_W = Age5y_W[Age5y_W['year'] == &quot;2021&quot;].reset_index(drop = True)Age21_percent_W['percentage'] = Age21_percent_W[&quot;Count&quot;] / Age21_percent_W[&quot;Count&quot;].sum()Age21_percent_W['%'] = np.round(Age21_percent_W['percentage'] * 100, 1)Age20_percent_W = Age5y_W[Age5y_W['year'] == &quot;2020&quot;].reset_index(drop = True)Age20_percent_W['percentage'] = Age20_percent_W[&quot;Count&quot;] / Age20_percent_W[&quot;Count&quot;].sum()Age20_percent_W['%'] = np.round(Age20_percent_W['percentage'] * 100, 1)Age19_percent_W = Age5y_W[Age5y_W['year'] == &quot;2019&quot;].reset_index(drop = True)Age19_percent_W['percentage'] = Age19_percent_W[&quot;Count&quot;] / Age19_percent_W[&quot;Count&quot;].sum()Age19_percent_W['%'] = np.round(Age19_percent_W['percentage'] * 100, 1)Age18_percent_W = Age5y_W[Age5y_W['year'] == &quot;2018&quot;].reset_index(drop = True)Age18_percent_W['percentage'] = Age18_percent_W[&quot;Count&quot;] / Age18_percent_W[&quot;Count&quot;].sum()Age18_percent_W['%'] = np.round(Age18_percent_W['percentage'] * 100, 1)Age5y_percent_W = pd.concat([Age18_percent_W, Age19_percent_W, Age20_percent_W, Age21_percent_W], ignore_index = True)Age5y_percent_W= pd.pivot(Age5y_percent_W, index = &quot;year&quot;, columns = 'age', values = &quot;%&quot;).reset_index()Age5y_percent_WAge21 = df21_Ea.loc[:,['Q3','Q1', 'year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'age'}).fillna('etc')Age20 = df20_Ea.loc[:,['Q3','Q1','year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'age'}).fillna('etc')Age19 = df19_Ea.loc[:,['Q3','Q1','year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'age'}).fillna('etc')Age18 = df18_Ea.loc[:,['Q3','Q2','year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q2':'age'}).fillna('etc')Age5y= pd.concat([Age21, Age20, Age19, Age18])Age5y= (Age5y.replace(['60-69', '70+', '70-79', '80+'], '60+') .replace(['22-24', '25-29'], '22-29') .replace(['30-34', '35-39'], '30-39') .replace(['40-44', '45-49'], '40-49') .replace(['50-54', '55-59'], '50-59') .groupby(['year', 'age']) .size() .reset_index() .rename(columns = {0:&quot;Count&quot;}))#EastAsiaAge21_percent = Age5y[Age5y['year'] == &quot;2021&quot;].reset_index(drop = True)Age21_percent['percentage'] = Age21_percent[&quot;Count&quot;] / Age21_percent[&quot;Count&quot;].sum()Age21_percent['%'] = np.round(Age21_percent['percentage'] * 100, 1)Age21_percentAge20_percent = Age5y[Age5y['year'] == &quot;2020&quot;].reset_index(drop = True)Age20_percent['percentage'] = Age20_percent[&quot;Count&quot;] / Age20_percent[&quot;Count&quot;].sum()Age20_percent['%'] = np.round(Age20_percent['percentage'] * 100, 1)Age20_percentAge19_percent = Age5y[Age5y['year'] == &quot;2019&quot;].reset_index(drop = True)Age19_percent['percentage'] = Age19_percent[&quot;Count&quot;] / Age19_percent[&quot;Count&quot;].sum()Age19_percent['%'] = np.round(Age19_percent['percentage'] * 100, 1)Age19_percentAge18_percent = Age5y[Age5y['year'] == &quot;2018&quot;].reset_index(drop = True)Age18_percent['percentage'] = Age18_percent[&quot;Count&quot;] / Age18_percent[&quot;Count&quot;].sum()Age18_percent['%'] = np.round(Age18_percent['percentage'] * 100, 1)Age18_percentAge5y_percent = pd.concat([Age18_percent, Age19_percent, Age20_percent, Age21_percent], ignore_index = True)Age5y_percent= pd.pivot(Age5y_percent, index = &quot;year&quot;, columns = 'age', values = &quot;%&quot;).reset_index()Age5y_percentAge5y_percent_order = Age5y_percent_W['year'].tolist()Age5y_order = Age5y_W['age'].unique().tolist()#graph1fig = go.Figure()fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent_W['18-21'].tolist(), mode = &quot;lines&quot;, name = '18-21', line = dict(width = 1), stackgroup = &quot;one&quot;, marker_color='#F2798F'))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent_W['22-29'].tolist(), mode = &quot;lines&quot;, name = &quot;20s&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;, marker_color='#88BFBA'))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent_W['30-39'].tolist(), mode = &quot;lines&quot;, name = &quot;30s&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;, marker_color='#CDD9A3'))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent_W['40-49'].tolist(), mode = &quot;lines&quot;, name = &quot;40s&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;, marker_color='#F28705'))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent_W['50-59'].tolist(), mode = &quot;lines&quot;, name = &quot;50s&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;, marker_color='#D9946C'))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent_W['60+'].tolist(), mode = &quot;lines&quot;, name = &quot;60s&lt;&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;, marker_color='#F2D64B'))fig.update_traces(hovertemplate='&lt;b&gt;Percent&lt;/b&gt;: %{y}%&lt;br&gt;'+ '&lt;b&gt;Year&lt;/b&gt;: %{x}&lt;br&gt;')fig.update_layout(yaxis_range = (0, 100), height=500, width=700, title_text=&quot;&lt;b&gt;World&lt;/b&gt;&quot;, title_font_size=20, title_x=0.5)fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.2, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show()#graph2Age5y_percent_order = Age5y_percent['year'].tolist()Age5y_order = Age5y['age'].unique().tolist()fig = go.Figure()fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent['18-21'].tolist(), mode = &quot;lines&quot;, name = '18-21', line = dict(width = 1), stackgroup = &quot;one&quot;, marker_color='#F2798F'))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent['22-29'].tolist(), mode = &quot;lines&quot;, name = &quot;20s&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;, marker_color='#88BFBA'))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent['30-39'].tolist(), mode = &quot;lines&quot;, name = &quot;30s&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;, marker_color='#CDD9A3'))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent['40-49'].tolist(), mode = &quot;lines&quot;, name = &quot;40s&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;, marker_color='#F28705'))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent['50-59'].tolist(), mode = &quot;lines&quot;, name = &quot;50s&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;, marker_color='#D9946C'))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent['60+'].tolist(), mode = &quot;lines&quot;, name = &quot;60s&lt;&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;, marker_color='#F2D64B'))fig.update_traces(hovertemplate='&lt;b&gt;Percent&lt;/b&gt;: %{y}%&lt;br&gt;'+ '&lt;b&gt;Year&lt;/b&gt;: %{x}&lt;br&gt;')fig.update_layout(yaxis_range = (0, 100), height=500, width=700, title_text=&quot;&lt;b&gt;East Asia&lt;/b&gt;&quot;, title_font_size=20, title_x=0.5)fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.2, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() 17'East Asia Age Ratio: Heat Map East Asia : 50% or more. Those in their 20s and 30s. Korea: Those in their 20s are the highest. The number of respondents in their 50s and older is also large. Taiwan : The number of respondents in their 30s and older is relatively small. China: 70% or more of respondents in their 30s or younger. Related to life expectancy? Japan: Like an aging country, all ages are evenly distributed. Even if you’re older, there are many respondents to Kaggle. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#data processingdf21Age_Ea = df21_Ea.loc[:,['Q3','Q1']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'2021'}).fillna('etc')df21Age_Ea=(df21Age_Ea.replace(['60-69', '70+', '70-79', '80+'], '60+') .replace(['22-24', '25-29'], '22-29') .replace(['30-34', '35-39'], '30-39') .replace(['40-44', '45-49'], '40-49') .replace(['50-54', '55-59'], '50-59'))# 연령-지역 %dfKo_Age21= df21Age_Ea[df21Age_Ea['East_Asia']=='South Korea']dfKo_Age21_per=dfKo_Age21['2021'].value_counts().to_frame().reset_index()dfKo_Age21_per['South Korea']=((dfKo_Age21_per['2021'] / len(dfKo_Age21))*100).round(2)dfTw_Age21= df21Age_Ea[df21Age_Ea['East_Asia']=='Taiwan']dfTw_Age21_per=dfTw_Age21['2021'].value_counts().to_frame().reset_index()dfTw_Age21_per['Taiwan']=((dfTw_Age21_per['2021'] / len(dfTw_Age21))*100).round(2)dfTw_Age21_perdfCh_Age21= df21Age_Ea[df21Age_Ea['East_Asia']=='China']dfCh_Age21_per=dfCh_Age21['2021'].value_counts().to_frame().reset_index()dfCh_Age21_per['China']=((dfCh_Age21_per['2021'] / len(dfCh_Age21))*100).round(2)dfCh_Age21_perdf21Age_Ea.head()dfJp_Age21= df21Age_Ea[df21Age_Ea['East_Asia']=='Japan']dfJp_Age21_per=dfJp_Age21['2021'].value_counts().to_frame().reset_index()dfJp_Age21_per['Japan']=((dfJp_Age21_per['2021'] / len(dfJp_Age21))*100).round(2)dfJp_Age21_permerge1= pd.merge(dfKo_Age21_per,dfTw_Age21_per, on='index', how='outer')merge2= pd.merge(dfCh_Age21_per,dfJp_Age21_per, on='index', how='outer')merge= pd.merge(merge1,merge2, on='index', how='outer').fillna(0).sort_values(by=['index'],ascending=True)#graphx1=['South Korea','Taiwan','China','Japan']y1=merge.sort_values(by=['index'], ascending=True)['index'].tolist()z1=merge.iloc[:,[2,4,6,8]].to_numpy()fig = go.Figure(data=go.Heatmap( z=z1, x=x1, y=y1, hoverongaps = True, opacity=1.0, xgap=2.5, ygap=2.5))fig = ff.create_annotated_heatmap(z1, x = x1, y = y1, colorscale='sunset')fig.update_layout(height=500, width=600, title_text=&quot;&lt;b&gt;East Asia Age (2021)&lt;/b&gt;&quot;, title_font_size=20, title_x=0.5)fig.update_traces(hovertemplate='&lt;b&gt;Age&lt;/b&gt;: %{y}&lt;br&gt;'+ '&lt;b&gt;Country&lt;/b&gt;: %{x}&lt;br&gt;'+ '&lt;b&gt;Percent&lt;/b&gt;: %{z}%')fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.2, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() 17’East Asia’s age ratio: Box plot 2017: Data is not a section but an individual number. If you divide the interval, you can add it to the previous graph. It was data that I could draw a bar plot, so I drew it. You can see a 100-year-old in China, but they don’t remove missing values on purpose. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# 연도별 나이 df21Age_Ea = df21_Ea.loc[:,['Q3','Q1']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'2021'}).fillna('etc')df20Age_Ea = df20_Ea.loc[:,['Q3','Q1']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'2020'}).fillna('etc')df19Age_Ea = df19_Ea.loc[:,['Q3','Q1']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'2019'}).fillna('etc')df18Age_Ea = df18_Ea.loc[:,['Q3','Q2']].reset_index().rename(columns={'Q3':'East_Asia', 'Q2':'2018'}).fillna('etc')df17Age_Ea = df17_Ea.loc[:,['Country','Age']].reset_index().rename(columns={'Country':'East_Asia', 'Age':'2017'}).fillna('etc')#data frame 정리dfAge21 =df21Age_Ea.groupby(['East_Asia','2021']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfAge20 =df20Age_Ea.groupby(['East_Asia','2020']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfAge19 =df19Age_Ea.groupby(['East_Asia','2019']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfAge18 =df18Age_Ea.groupby(['East_Asia','2018']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfAge17 =(df17Age_Ea.groupby(['East_Asia','2017']) .size().reset_index().rename(columns = {0:&quot;Count&quot;}))#graphfig = go.Figure()x = ['China','Japan','South Korea','Taiwan']fig.add_trace(go.Box( y=dfAge17['2017'][dfAge17['East_Asia']==&quot;Japan&quot;].to_numpy(), name='Japan', marker=dict(color='#CDD9A3')))fig.add_trace(go.Box(y=dfAge17['2017'][dfAge17['East_Asia']==&quot;China&quot;].to_numpy(), name='China', marker=dict(color='#88BFBA')))fig.add_trace(go.Box(y=dfAge17['2017'][dfAge17['East_Asia']==&quot;South Korea&quot;].to_numpy(), name='South Korea', marker=dict(color='#F2798F')))fig.add_trace(go.Box(y=dfAge17['2017'][dfAge17['East_Asia']==&quot;Taiwan&quot;].to_numpy(), name='Taiwan', marker=dict(color='#F28705' ),))fig.update_layout(yaxis = dict(range=[0, 120]))fig.update_layout(yaxis_range = (0, 110), height=600, width=700, title_text=&quot;&lt;b&gt;Age in East Asia (2017)&lt;/b&gt;&quot;, title_font_size=20, margin = dict(t=100, l=50, r=50, b=100), title_x=0.5)fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.update_layout(legend=dict( orientation=&quot;v&quot;, yanchor=&quot;bottom&quot;, y=0.8, xanchor=&quot;right&quot;, x=1))fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.2, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() 3.1.5 Degree transformation World job ratio in each country: pie plot World: 90% or higher Bachelor’s degree East Asia: 85% bachelor’s degree or higher 12345678910111213141516171819202122232425262728293031323334353637383940414243#data preprocessingdegree_wo = (df21['Q4'] .replace(['No formal education past high school', 'Some college/university study without earning a bachelor’s degree'],'~college') .replace(['Doctoral degree', 'Professional doctorate'],'Doctoral degree~') .value_counts().to_frame())degree_ea = (df21_Ea['Q4'] .replace(['No formal education past high school', 'Some college/university study without earning a bachelor’s degree'],'~college') .replace(['Doctoral degree', 'Professional doctorate'],'Doctoral degree~') .value_counts().to_frame())#graphcolors = ['#F2798F','#88BFBA', '#CDD9A3', '#F28705', '#D9946C']fig = make_subplots(rows=1, cols=2, specs=[[{'type':'pie'}, {'type':'pie'}]], subplot_titles=(&quot;World&quot;, &quot;East Asia&quot;))fig.add_trace(go.Pie(marker=dict(colors=colors), labels=degree_wo.index, values=degree_wo['Q4'].to_numpy(), name=&quot;World&quot;), 1, 1)fig.add_trace(go.Pie(marker=dict(colors=colors), labels=degree_ea.index, values=degree_ea['Q4'].to_numpy(), name=&quot;East Asia&quot;), 1, 2)fig.update_traces(hole=.0, hoverinfo=&quot;label+percent+name&quot;)fig.update_layout(title='&lt;b&gt;World vs East Asia&lt;/b&gt;',title_font_size=22, margin = dict(t=200, l=30, r=0, b=200), height=700, width=700)fig.update_layout(legend=dict( orientation=&quot;h&quot;, yanchor=&quot;bottom&quot;, y=1.1, xanchor=&quot;right&quot;, x=1.0))fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.5, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() Percentage of East Asia degrees by year: sunburst plot The highest percentage of respondents with master’s degrees per year 12345678910111213141516171819202122232425262728293031323334353637383940414243444546#data preprocessingdf21_Ea_degree=(df21_Ea['Q4'].replace(['No formal education past high school', 'Some college/university study without earning a bachelor’s degree'],'~college') .replace(['Doctoral degree','Professional doctorate'],'Doctoral degree~') .value_counts().to_frame().rename(columns={'Q4':'2021'}))df20_Ea_degree=(df20_Ea['Q4'].replace(['No formal education past high school', 'Some college/university study without earning a bachelor’s degree'],'~college') .replace(['Doctoral degree', 'Professional degree'],'Doctoral degree~') .value_counts().to_frame().rename(columns={'Q4':'2020'}))df19_Ea_degree=(df19_Ea['Q4'].replace(['No formal education past high school','Some college/university study without earning a bachelor’s degree'],'~college') .replace(['Doctoral degree', 'Professional degree'],'Doctoral degree~') .value_counts().to_frame().rename(columns={'Q4':'2019'}))df18_Ea_degree=(df18_Ea['Q4'].replace(['No formal education past high school', 'Some college/university study without earning a bachelor’s degree'],'~college') .replace(['Doctoral degree', 'Professional degree'],'Doctoral degree~') .value_counts().to_frame().rename(columns={'Q4':'2018'}))df17_Ea_degree=(df17_Ea['FormalEducation'] .replace(['No formal education past high school', 'Some college/university study without earning a bachelor’s degree'],'~college') .replace(['Doctoral degree', 'Professional degree'],'Doctoral degree~') .value_counts().to_frame() .rename(columns={'FormalEducation':'2017'} ,index = {'I did not complete any formal education past high school':'No formal education past high school','Master\\'s degree':'Master’s degree','Bachelor\\'s degree':'Bachelor’s degree','Some college/university study without earning a bachelor\\'s degree':'Some college/university study without earning a bachelor’s degree'}) ) concat1 = pd.concat([df21_Ea_degree,df20_Ea_degree],axis=1, join='outer') concat2 = pd.concat([df19_Ea_degree,df18_Ea_degree],axis=1, join='outer') concat3 = pd.concat([concat1,concat2],axis=1, join='outer') df21_Ea_degree_yearly_=concat3.join(df17_Ea_degree).fillna(0).transpose() #.transpose() 행 열 바꾸기df21_Ea_degree_yearly=df21_Ea_degree_yearly_.stack().to_frame().reset_index().rename(columns={'level_0':'year','level_1':'degree',0:'value'})df21_Ea_degree_yearly#graphfig = px.sunburst(df21_Ea_degree_yearly, path=['year','degree'], values=df21_Ea_degree_yearly['value'].tolist())fig.update_layout( margin = dict(t=10, l=10, r=10, b=10),colorway=(&quot;#F2798F&quot;,&quot;#88BFBA&quot;,&quot;#CDD9A3&quot;,'#F28705','#D9946C'))fig.update_layout(title='&lt;b&gt; Degree&lt;/b&gt;',title_font_size=25, margin = dict(t=100, l=100, r=50, b=100), height=700, width=700)fig.update_traces(hovertemplate='&lt;b&gt;Name&lt;/b&gt;: %{id}&lt;br&gt;'+ '&lt;b&gt;Count&lt;/b&gt;: %{value}&lt;br&gt;'+ '&lt;b&gt;Parent&lt;/b&gt;: %{parent}') fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.2, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() Plus we could see the advantages of Plotly in this graph. Matplotlib draws a static graph, but Plotly can dynamically click and move, and it supports zooming out, zooming in, and downloading graphs. Because all of our graphs are made of plotly, the viewer can represent or remove items in the graph if desired. With a click East Asia Degree Ratio: Bar plot 40% of master’s degrees or higher, and respondents have a high educational background. China and Japan have similar trends to East Asia and the World. The number of people itself is large, so a representative trend seems to appear here. However, it is noteworthy that the two countries have the same tendency. Korea: It is the only country among the four countries with a high degree of education below Ph.D., bachelor’s degree, and junior college. Only masters are low. (Polarization of education?) Taiwan: 1st place in master’s ratio (55%), 2nd place in Ph.D. or higher (13.8%). = The highest level of education. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091#data preprocessingdf21Edu_Ea = df21_Ea.loc[:,['Q3','Q4']].reset_index().rename(columns={'Q3':'East_Asia', 'Q4':'Dgree'}).fillna('etc')df21Edu_Ea =(df21Edu_Ea.replace({'I prefer not to answer':'etc'}).replace(['No formal education past high school', 'Some college/university study without earning a bachelor’s degree'],'~college') .replace(['Doctoral degree', 'Professional doctorate'],'Doctoral degree~'))df21Edu_Ea= (df21Edu_Ea .groupby(['East_Asia', 'Dgree']) .size() .reset_index() .rename(columns = {0:&quot;Count&quot;}))# 연령-지역 %dfKo_Edu21= df21Edu_Ea[df21Edu_Ea['East_Asia']=='South Korea']dfKo_Edu21['%']=((dfKo_Edu21['Count'] / dfKo_Edu21['Count'].sum()*100)).round(2)dfKo_Edu21=dfKo_Edu21.sort_values(by='%', ascending=False)dfTw_Edu21= df21Edu_Ea[df21Edu_Ea['East_Asia']=='Taiwan']dfTw_Edu21['%']=((dfTw_Edu21['Count'] / dfTw_Edu21['Count'].sum())*100).round(2)dfTw_Edu21=dfTw_Edu21.sort_values(by='%', ascending=False)dfCh_Edu21= df21Edu_Ea[df21Edu_Ea['East_Asia']=='China']dfCh_Edu21['%']=((dfCh_Edu21['Count'] / dfCh_Edu21['Count'].sum())*100).round(2)dfCh_Edu21=dfCh_Edu21.sort_values(by='%', ascending=False)dfJp_Edu21= df21Edu_Ea[df21Edu_Ea['East_Asia']=='Japan']dfJp_Edu21['%']=((dfJp_Edu21['Count'] / dfJp_Edu21['Count'].sum())*100).round(2)dfJp_Edu21=dfJp_Edu21.sort_values(by='%', ascending=False)# #data 완성# dfEdu_21_per = pd.concat([dfKo_Edu21, dfTw_Edu21, dfCh_Edu21, dfJp_Edu21], ignore_index = True)# dfEdu_21_per= pd.pivot(dfEdu_21_per, index = &quot;Dgree&quot;, columns = 'East_Asia', values = &quot;%&quot;).reset_index()# dfEdu_21_per#graphfig = make_subplots(rows = 1, cols = 4, shared_yaxes=True, vertical_spacing = 0.05)fig.add_trace(go.Bar(x = dfCh_Edu21['Dgree'], y = dfCh_Edu21['%'], text = dfCh_Edu21['%'].astype(str) + &quot;%&quot;, textposition='outside', name='China', marker_color='#88BFBA'), row = 1, col = 1)fig.add_trace(go.Bar(x = dfJp_Edu21['Dgree'], y = dfJp_Edu21['%'], text = dfJp_Edu21['%'].astype(str) + &quot;%&quot;, textposition='outside', name='Japan', marker_color='#CDD9A3'), row = 1, col = 2)fig.add_trace(go.Bar(x = dfKo_Edu21['Dgree'], y = dfKo_Edu21['%'], text = dfKo_Edu21['%'].astype(str) + &quot;%&quot;, textposition='outside', name='South Korea', marker_color='#F28705'), row = 1, col = 3)fig.add_trace(go.Bar(x = dfTw_Edu21['Dgree'], y = dfTw_Edu21['%'], text = dfTw_Edu21['%'].astype(str) + &quot;%&quot;, textposition='outside', name='Taiwan', marker_color='#D9946C'), row = 1, col = 4)fig.update_layout(showlegend=True,title='&lt;b&gt;Degree in East Asia&lt;/b&gt;',title_font_size=22, margin = dict(t=200, l=100, r=50, b=200), height=700, width=700)fig.update_traces(hovertemplate='&lt;b&gt;Percent&lt;/b&gt;: %{y}%&lt;br&gt;'+ '&lt;b&gt;Degree&lt;/b&gt;: %{x}&lt;br&gt;')fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.update_layout(legend=dict( orientation=&quot;h&quot;, yanchor=&quot;bottom&quot;, y=1.1, xanchor=&quot;right&quot;, x=1))fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.5, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() 3.1.6 Experience transformation Trends in World & East Asia Career: Stacked Scatter plot - < 2 years: 50% of the total. - 3-5 years: Decrease in the world, maintain East Asia ratio - 2021 'etc data' disappeared. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120#Exp data 전처리# Exp 뽑아오기Exp21_Wo = df21.loc[:,['Q3','Q6', 'year']].reset_index().rename(columns={'Q3':'Country', 'Q6':'Exp'}).fillna('etc')Exp20_Wo = df20.loc[:,['Q3','Q6','year']].reset_index().rename(columns={'Q3':'Country', 'Q6':'Exp'}).fillna('etc')Exp19_Wo = df19.loc[:,['Q3','Q15','year']].reset_index().rename(columns={'Q3':'Country', 'Q15':'Exp'}).fillna('etc')Exp18_Wo = df18.loc[:,['Q3','Q8','year']].reset_index().rename(columns={'Q3':'Country', 'Q8':'Exp'}).fillna('etc')Exp17_Wo = df17.loc[:,['Country','Tenure', 'year']].reset_index().rename(columns={'Country':'Country', 'Tenure':'Exp'}).fillna('etc')Exp21_Wo= Exp21_Wo.replace({'I have never written code': '&lt; 1 years', '1-3 years': '1-2 years'}).replace(['10-20 years', '20+ years'], '10+ years' )Exp20_Wo= Exp20_Wo.replace({'I have never written code': '&lt; 1 years'}).replace(['10-20 years', '20+ years'], '10+ years' )Exp19_Wo= Exp19_Wo.replace({'I have never written code': '&lt; 1 years'}).replace(['10-20 years', '20+ years'], '10+ years' )Exp18_Wo= (Exp18_Wo.replace({'0-1': '&lt; 1 years', '1-2': '1-2 years', '5-10':'5-10 years'}) .replace(['2-3', '3-4', '4-5'],'3-5 years') .replace(['10-15', '15-20','20-25', '30 +','25-30'],'10+ years'))Exp17_Wo=(Exp17_Wo.replace({'More than 10 years':'10+ years', '1 to 2 years':'1-2 years', 'Less than a year':'&lt; 1 years', '3 to 5 years':'3-5 years', &quot;I don't write code to analyze data&quot;:'&lt; 1 years', '6 to 10 years':'5-10 years'})) #data 정제(한꺼번에 이름바꾸기)Exp5y_Wo= pd.concat([Exp17_Wo, Exp18_Wo, Exp19_Wo, Exp20_Wo, Exp21_Wo]).reset_index()Exp5y_Wo=(Exp5y_Wo.groupby(['year', 'Exp']) .size() .reset_index() .rename(columns = {0:&quot;Count&quot;}))#percent data 넣기Exp21_per_W= Exp5y_Wo[Exp5y_Wo['year'] == &quot;2021&quot;].reset_index(drop = True)Exp21_per_W['percentage'] = Exp21_per_W[&quot;Count&quot;] / Exp21_per_W[&quot;Count&quot;].sum()Exp21_per_W['%'] = np.round(Exp21_per_W['percentage'] * 100, 1)Exp20_per_W = Exp5y_Wo[Exp5y_Wo['year'] == &quot;2020&quot;].reset_index(drop = True)Exp20_per_W['percentage'] = Exp20_per_W[&quot;Count&quot;] / Exp20_per_W[&quot;Count&quot;].sum()Exp20_per_W['%'] = np.round(Exp20_per_W['percentage'] * 100, 1)Exp19_per_W = Exp5y_Wo[Exp5y_Wo['year'] == &quot;2019&quot;].reset_index(drop = True)Exp19_per_W['percentage'] = Exp19_per_W[&quot;Count&quot;] / Exp19_per_W[&quot;Count&quot;].sum()Exp19_per_W['%'] = np.round(Exp19_per_W['percentage'] * 100, 1)Exp18_per_W = Exp5y_Wo[Exp5y_Wo['year'] == &quot;2018&quot;].reset_index(drop = True)Exp18_per_W['percentage'] = Exp18_per_W[&quot;Count&quot;] / Exp18_per_W[&quot;Count&quot;].sum()Exp18_per_W['%'] = np.round(Exp18_per_W['percentage'] * 100, 1)Exp17_per_W = Exp5y_Wo[Exp5y_Wo['year'] == &quot;2017&quot;].reset_index(drop = True)Exp17_per_W['percentage'] = Exp17_per_W[&quot;Count&quot;] / Exp17_per_W[&quot;Count&quot;].sum()Exp17_per_W['%'] = np.round(Exp17_per_W['percentage'] * 100, 1)#data 완성Exp5y_per_W = pd.concat([Exp17_per_W, Exp18_per_W, Exp19_per_W, Exp20_per_W, Exp21_per_W], ignore_index = True)Exp5y_per_W= pd.pivot(Exp5y_per_W, index = &quot;year&quot;, columns = 'Exp', values = &quot;%&quot;).reset_index()Exp5y_per_W.fillna('0')Exp5y_percent_order = Exp5y_per_W['year'].tolist()fig = go.Figure()fig.add_trace(go.Scatter( x = Exp5y_percent_order, y = Exp5y_per_W['&lt; 1 years'].tolist(), mode = &quot;lines&quot;, name = '&lt; 1 years', line = dict(width = 0.5), stackgroup = &quot;one&quot;, marker_color='#F2798F'))fig.add_trace(go.Scatter( x = Exp5y_percent_order, y = Exp5y_per_W['1-2 years'].tolist(), mode = &quot;lines&quot;, name = '1-2 years', line = dict(width = 0.5), stackgroup = &quot;one&quot;, marker_color='#88BFBA'))fig.add_trace(go.Scatter( x = Exp5y_percent_order, y = Exp5y_per_W['3-5 years'].tolist(), mode = &quot;lines&quot;, name = '3-5 years', line = dict(width = 0.5), stackgroup = &quot;one&quot;, marker_color='#CDD9A3'))fig.add_trace(go.Scatter( x = Exp5y_percent_order, y = Exp5y_per_W['5-10 years'].tolist(), mode = &quot;lines&quot;, name = '5-10 years', line = dict(width = 0.5), stackgroup = &quot;one&quot;, marker_color='#F28705'))fig.add_trace(go.Scatter( x = Exp5y_percent_order, y = Exp5y_per_W['10+ years'].tolist(), mode = &quot;lines&quot;, name = '10+ years', line = dict(width = 0.5), stackgroup = &quot;one&quot;, marker_color='#D9946C'))fig.add_trace(go.Scatter( x = Exp5y_percent_order, y = Exp5y_per_W['etc'].tolist(), mode = &quot;lines&quot;, name = 'etc', line = dict(width = 1), stackgroup = &quot;one&quot;, marker_color='#F2D64B'))fig.update_traces(hovertemplate='&lt;b&gt;Percent&lt;/b&gt;: %{y}%&lt;br&gt;')fig.update_layout(yaxis_range = (0, 100), title_font_size=20, title_text=&quot;&lt;b&gt;Experience in world&lt;/b&gt;&quot;, height=500, width=700, title_x=0.5)fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.2, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135#data preprocessingExp21 = df21_Ea.loc[:,['Q3','Q6', 'year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q6':'Exp'}).fillna('etc')Exp20 = df20_Ea.loc[:,['Q3','Q6','year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q6':'Exp'}).fillna('etc')Exp19 = df19_Ea.loc[:,['Q3','Q15','year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q15':'Exp'}).fillna('etc')Exp18 = df18_Ea.loc[:,['Q3','Q8','year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q8':'Exp'}).fillna('etc')Exp17 = df17_Ea.loc[:,['Country','Tenure', 'year']].reset_index().rename(columns={'Country':'East_Asia', 'Tenure':'Exp'}).fillna('etc')Exp21Uni=['3-5 years', '&lt; 1 years', '1-3 years', '10-20 years', 'I have never written code', '5-10 years', '20+ years']Exp20Uni= ['3-5 years', '&lt; 1 years', '5-10 years', '1-2 years', 'etc', '20+ years', '10-20 years', 'I have never written code']Exp19Uni=['1-2 years', '5-10 years', '&lt; 1 years', 'I have never written code', '3-5 years', '10-20 years', '20+ years', 'etc']Exp18Uni=['0-1', '2-3', '1-2', '5-10', '3-4', '10-15', '15-20', '4-5', '20-25', '30 +', 'etc', '25-30']Exp17Uni=['More than 10 years', '1 to 2 years', 'etc', 'Less than a year', '3 to 5 years', &quot;I don't write code to analyze data&quot;, '6 to 10 years']Exp21= Exp21.replace({'I have never written code': '&lt; 1 years', '1-3 years': '1-2 years'}).replace(['10-20 years', '20+ years'], '10+ years' )Exp20= Exp20.replace({'I have never written code': '&lt; 1 years'}).replace(['10-20 years', '20+ years'], '10+ years' )Exp19= Exp19.replace({'I have never written code': '&lt; 1 years'}).replace(['10-20 years', '20+ years'], '10+ years' )Exp18= (Exp18.replace({'0-1': '&lt; 1 years', '1-2': '1-2 years', '5-10':'5-10 years'}) .replace(['2-3', '3-4', '4-5'],'3-5 years') .replace(['10-15', '15-20','20-25', '30 +','25-30'],'10+ years'))Exp17=(Exp17.replace({'More than 10 years':'10+ years', '1 to 2 years':'1-2 years', 'Less than a year':'&lt; 1 years', '3 to 5 years':'3-5 years', &quot;I don't write code to analyze data&quot;:'&lt; 1 years', '6 to 10 years':'5-10 years'})) Exp5y= pd.concat([Exp17, Exp18, Exp19, Exp20, Exp21]).reset_index()Exp5y=(Exp5y.groupby(['year', 'Exp']) .size() .reset_index() .rename(columns = {0:&quot;Count&quot;}))Exp21_percent = Exp5y[Exp5y['year'] == &quot;2021&quot;].reset_index(drop = True)Exp21_percent['percentage'] = Exp21_percent[&quot;Count&quot;] / Exp21_percent[&quot;Count&quot;].sum()Exp21_percent['%'] = np.round(Exp21_percent['percentage'] * 100, 1)Exp21_percentExp20_percent = Exp5y[Exp5y['year'] == &quot;2020&quot;].reset_index(drop = True)Exp20_percent['percentage'] = Exp20_percent[&quot;Count&quot;] / Exp20_percent[&quot;Count&quot;].sum()Exp20_percent['%'] = np.round(Exp20_percent['percentage'] * 100, 1)Exp20_percentExp19_percent = Exp5y[Exp5y['year'] == &quot;2019&quot;].reset_index(drop = True)Exp19_percent['percentage'] = Exp19_percent[&quot;Count&quot;] / Exp19_percent[&quot;Count&quot;].sum()Exp19_percent['%'] = np.round(Exp19_percent['percentage'] * 100, 1)Exp19_percentExp18_percent = Exp5y[Exp5y['year'] == &quot;2018&quot;].reset_index(drop = True)Exp18_percent['percentage'] = Exp18_percent[&quot;Count&quot;] / Exp18_percent[&quot;Count&quot;].sum()Exp18_percent['%'] = np.round(Exp18_percent['percentage'] * 100, 1)Exp18_percentExp17_percent = Exp5y[Exp5y['year'] == &quot;2017&quot;].reset_index(drop = True)Exp17_percent['percentage'] = Exp17_percent[&quot;Count&quot;] / Exp17_percent[&quot;Count&quot;].sum()Exp17_percent['%'] = np.round(Exp17_percent['percentage'] * 100, 1)Exp17_percent#graphExp5y_percent = pd.concat([Exp17_percent, Exp18_percent, Exp19_percent, Exp20_percent, Exp21_percent], ignore_index = True)Exp5y_percent= pd.pivot(Exp5y_percent, index = &quot;year&quot;, columns = 'Exp', values = &quot;%&quot;).reset_index()Exp5y_percent.fillna('0')Exp5y_percent_order = Exp5y_percent['year'].tolist()fig = go.Figure()fig.add_trace(go.Scatter( x = Exp5y_percent_order, y = Exp5y_percent['&lt; 1 years'].tolist(), mode = &quot;lines&quot;, name = '&lt; 1 years', line = dict(width = 0.5), stackgroup = &quot;one&quot;, marker_color='#F2798F'))fig.add_trace(go.Scatter( x = Exp5y_percent_order, y = Exp5y_percent['1-2 years'].tolist(), mode = &quot;lines&quot;, name = '1-2 years', line = dict(width = 0.5), stackgroup = &quot;one&quot;, marker_color='#88BFBA'))fig.add_trace(go.Scatter( x = Exp5y_percent_order, y = Exp5y_percent['3-5 years'].tolist(), mode = &quot;lines&quot;, name = '3-5 years', line = dict(width = 0.5), stackgroup = &quot;one&quot;, marker_color='#CDD9A3'))fig.add_trace(go.Scatter( x = Exp5y_percent_order, y = Exp5y_percent['5-10 years'].tolist(), mode = &quot;lines&quot;, name = '5-10 years', line = dict(width = 0.5), stackgroup = &quot;one&quot;, marker_color='#F28705'))fig.add_trace(go.Scatter( x = Exp5y_percent_order, y = Exp5y_percent['10+ years'].tolist(), mode = &quot;lines&quot;, name = '10+ years', line = dict(width = 0.5), stackgroup = &quot;one&quot;, marker_color='#D9946C'))fig.add_trace(go.Scatter( x = Exp5y_percent_order, y = Exp5y_percent['etc'].tolist(), mode = &quot;lines&quot;, name = 'etc', line = dict(width = 0.5), stackgroup = &quot;one&quot;, marker_color='#F2D64B'))fig.update_traces(hovertemplate='&lt;b&gt;Percent&lt;/b&gt;: %{y}%&lt;br&gt;')fig.update_layout(yaxis_range = (0, 100), title_text=&quot;&lt;b&gt;Experience in East Asia&lt;/b&gt;&quot;, height=500, width=700, title_font_size=20, title_x=0.5)fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.2, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() 3.1.7 Salary transformation World & East Asia Annual salary: Bar-H plot $ 200,000 ~ : World (2.9%) is more than 50% compared to East Asia (1.3%) $ ~250,000 : World (59.2%) is less than East Asia (50.3%) = East Asia’s annual salary gap between rich and poor is less. $ 25,000~60,000: The highest section in East Asia at 24%. = The annual salary section that we aim for. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103#data preprocessingdf21_salary_=df21['Q25'].value_counts().to_frame().rename(index={'$0-999':'&lt;999','&gt;$1,000,000':'1,000,000~','$500,000-999,999':'500,000-999,999'}).fillna(0)df21_Ea_salary_=df21_Ea['Q25'].value_counts().to_frame().rename(index={'$0-999':'&lt;999','&gt;$1,000,000':'1,000,000~','$500,000-999,999':'500,000-999,999'}).fillna(0)#퍼센트df21_salary__=(df21_salary_['Q25']/(df21_salary_['Q25'].sum())*100).round(1).to_frame().rename(columns={'Q25':'World'})df21_Ea_salary__=(df21_Ea_salary_['Q25']/(df21_Ea_salary_['Q25'].sum())*100).round(1).to_frame().rename(columns={'Q25':'EA'})#그룹화df21_salary=(df21_salary__.rename(index= {'1,000-1,999':'1,000-7,499', '2,000-2,999':'1,000-7,499', '3,000-3,999':'1,000-7,499', '4,000-4,999':'1,000-7,499', '5,000-7,499':'1,000-7,499'}) .rename(index={'7,500-9,999':'7,500-24,999', '10,000-14,999':'7,500-24,999', '15,000-19,999':'7,500-24,999', '20,000-24,999':'7,500-24,999' }) .rename(index={'25,000-29,999':'25,000-59,999', '30,000-39,999':'25,000-59,999', '40,000-49,999':'25,000-59,999', '50,000-59,999':'25,000-59,999'}) .rename(index={'60,000-69,999':'60,000-99,999', '70,000-79,999':'60,000-99,999', '80,000-89,999':'60,000-99,999', '90,000-99,999':'60,000-99,999'}) .rename(index={'100,000-124,999':'100,000-199,999', '125,000-149,999':'100,000-199,999', '150,000-199,999':'100,000-199,999'}) .rename(index={'200,000-249,999':'200,000-1,000,000~', '250,000-299,999':'200,000-1,000,000~', '300,000-499,999':'200,000-1,000,000~', '500,000-999,999':'200,000-1,000,000~', '1,000,000~':'200,000-1,000,000~'}) .reset_index().groupby('index').sum() .reindex(index = ['&lt;999', '1,000-7,499', '7,500-24,999', '25,000-59,999', '60,000-99,999', '100,000-199,999', '200,000-1,000,000~']))df21_Ea_salary=(df21_Ea_salary__.rename(index= {'1,000-1,999':'1,000-7,499', '2,000-2,999':'1,000-7,499', '3,000-3,999':'1,000-7,499', '4,000-4,999':'1,000-7,499', '5,000-7,499':'1,000-7,499'}) .rename(index={'7,500-9,999':'7,500-24,999', '10,000-14,999':'7,500-24,999', '15,000-19,999':'7,500-24,999', '20,000-24,999':'7,500-24,999'}) .rename(index={'25,000-29,999':'25,000-59,999', '30,000-39,999':'25,000-59,999', '40,000-49,999':'25,000-59,999', '50,000-59,999':'25,000-59,999'}) .rename(index={'60,000-69,999':'60,000-99,999', '70,000-79,999':'60,000-99,999', '80,000-89,999':'60,000-99,999', '90,000-99,999':'60,000-99,999'}) .rename(index={'100,000-124,999':'100,000-199,999', '125,000-149,999':'100,000-199,999', '150,000-199,999':'100,000-199,999'}) .rename(index={'200,000-249,999':'200,000-1,000,000~', '250,000-299,999':'200,000-1,000,000~', '300,000-499,999 ':'200,000-1,000,000~', '500,000-999,999':'200,000-1,000,000~', '1,000,000~':'200,000-1,000,000~'}) .reset_index().groupby('index').sum() .reindex(index = ['&lt;999', '1,000-7,499', '7,500-24,999', '25,000-59,999', '60,000-99,999', '100,000-199,999', '200,000-1,000,000~']))#graphWorld = df21_salary['World'].valuesEast_Asia = df21_Ea_salary['EA'].valuesy = df21_salary.indexfig = go.Figure(data=[ go.Bar(y=y, x=World, orientation='h', name=&quot;World&quot;, base=0, hovertemplate='&lt;b&gt;World&lt;/b&gt;: %{x}%&lt;br&gt;', marker_color='#979DA6'), go.Bar(y=y, x=-East_Asia, orientation='h', name=&quot;East Asia&quot;, base=0, hovertemplate='&lt;b&gt;East Asia&lt;/b&gt;: %{x}%&lt;br&gt;', marker_color='#F2D64B') ])fig.update_layout(barmode='stack')fig.update_layout( margin=dict(l=200, r=0, t=200, b=100), autosize=False, title_text=&quot;&lt;b&gt; Salary in East Asia vs World&lt;/b&gt;&quot;, height=600, width=700, title_font_size=20, title_x=0.5)fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.update_layout(legend=dict( orientation=&quot;h&quot;, yanchor=&quot;bottom&quot;, y=1.1, xanchor=&quot;right&quot;, x=1))fig.show() World experience and annual salary: Heat Map Relatively **positive correlation.** Even with 5-10 years of experience, more than 45% has an annual salary of less than $20,000 With more than 10 years of experience, more than 30% receive an annual salary of $100,000. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990#data preprocessingSalExp21= df21.loc[:, ['region', 'Q25', 'Q6']].rename(columns={'Q6':'Exp', 'Q25':'Salary'})SalExp21=(SalExp21 .replace(['0-999','$0-999','0'], '&lt; 999') .replace({'&gt;$1,000,000':'200,000~'}) .replace(['1,000-1,999','2,000-2,999','3,000-3,999', '4,000-4,999','5,000-7,499','7,500-9,999','10,000-14,999', '15,000-19,999'],'1,000-20,000') .replace(['20,000-24,999''25,000-29,999','30,000-39,999', '40,000-49,999', '50,000-59,999'],'20,000-59,999') .replace(['60,000-69,999', '70,000-79,999', '80,000-89,999', '90,000-99,999'], '60,000-99,999') .replace(['100,000-124,999', '300,000-499,999', '125,000-149,999', '125,000-149,999', '150,000-199,999'],'100,000-199,999') .replace(['200,000-249,999', '250,000-299,999', '1,000,000','$500,000-999,999'], '200,000~') .replace({'I have never written code': '&lt; 1 years'}) .replace(['10-20 years', '20+ years'], '10+ years' ) )sal_order=['&lt; 999', '1,000-20,000', '20,000-59,999', '60,000-99,999','100,000-199,999', '200,000~']Exp21_order=['&lt; 1 years', '1-3 years','3-5 years', '5-10 years', '10+ years' ]SalExp21_Ea = SalExp21[SalExp21['region'] == &quot;EastAsia&quot;].reset_index(drop = True)SalExp21_Ea=(SalExp21_Ea.groupby(['Exp', 'Salary']) .size() .unstack().fillna(0).astype('int64'))SalExp21_Wo = SalExp21[SalExp21['region'] == &quot;World&quot;].reset_index(drop = True)SalExp21_Wo=(SalExp21_Wo.groupby(['Exp', 'Salary']) .size() .unstack().fillna(0).astype('int64'))SalExp21_Wo#graph#Worldz = SalExp21_Woz = z[sal_order]z = z.reindex(Exp21_order)z_data = z.apply(lambda x:np.round(x/x.sum()*100, 2), axis = 1).to_numpy() # convert to correlation matrixx = sal_ordery = Exp21_orderfig = ff.create_annotated_heatmap(z_data, x = x, y = y, colorscale = &quot;sunset&quot;)fig.update_layout( title_text=&quot;&lt;b&gt;Experience and salary in World&lt;/b&gt;&quot;, height=700, width=700, title_font_size=20, title_x=0.5, margin=dict(l=100, r=100, t=200, b=100))fig.add_annotation(dict(font=dict(size=14), x=0.85, y=-0.1, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show()#East Asiaz = SalExp21_Eaz = z[sal_order]z = z.reindex(Exp21_order)z_data = z.apply(lambda x:np.round(x/x.sum(), 2), axis = 1).to_numpy() # convert to correlation matrixx = sal_ordery = Exp21_orderfig = ff.create_annotated_heatmap(z_data, x = x, y = y, colorscale = &quot;sunset&quot;)fig.update_layout(title_text=&quot;&lt;b&gt;Experience and salary in East Asia&lt;/b&gt;&quot;, height=700, width=700, title_font_size=20, title_x=0.5, margin=dict(l=100, r=100, t=200, b=100))fig.add_annotation(dict(font=dict(size=14), x=0.85, y=-0.1, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() World & East Asia Degree/Annual salary: Heat Map \\$ ~20,000 : Regardless of degree, about 40% of the annual salary is $ 20,000 or less. Guess it’s the ratio that comes from a student. $ 25,000-100,000 : Earned more than 40% with a bachelor’s degree alone in East Asia (World: less than 20%) $ 200,000~ : Even with a doctorate or higher, it is difficult to obtain it from East Asia. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798#data preprocessingSalary21= df21.loc[:, ['region', 'Q25', 'year']].rename(columns={'Q3':'Country', 'Q25':'Salary'})salary21_Index=['&lt; 999', '1,000-20,000', '20,000-59,999', '60,000-99,999','100,000-199,999', '200,000~']Salary21=(Salary21 .replace(['0-999','$0-999','0'], '&lt; 999') .replace({'&gt;$1,000,000':'200,000~'}) .replace(['1,000-1,999','2,000-2,999','3,000-3,999', '4,000-4,999','5,000-7,499','7,500-9,999','10,000-14,999', '15,000-19,999'],'1,000-20,000') .replace(['20,000-24,999''25,000-29,999','30,000-39,999', '40,000-49,999', '50,000-59,999'],'20,000-59,999') .replace(['60,000-69,999', '70,000-79,999', '80,000-89,999','90,000-99,999'], '60,000-99,999') .replace(['100,000-124,999', '300,000-499,999', '125,000-149,999', '125,000-149,999', '150,000-199,999'],'100,000-199,999') .replace(['200,000-249,999', '250,000-299,999','1,000,000','$500,000-999,999'], '200,000~')).fillna('0')sal_order=['&lt; 999', '1,000-20,000', '20,000-59,999', '60,000-99,999','100,000-199,999', '200,000~']Salary21=(Salary21.groupby(['region', 'Salary']) .size() .reset_index() .rename(columns = {0:&quot;Count&quot;}))Salary21_Ea = Salary21[Salary21['region'] == &quot;EastAsia&quot;].reset_index(drop = True)Salary21_Ea['%']=((Salary21_Ea['Count'] / Salary21_Ea['Count'].sum())*100).round(2)Salary21_Wo = Salary21[Salary21['region'] == &quot;World&quot;].reset_index(drop = True)Salary21_Wo['%']=((Salary21_Wo['Count'] / Salary21_Wo['Count'].sum())*100).round(2)Dgr_Sal_21= df21.loc[:, ['region', 'Q25', 'Q4']].rename(columns={'Q4':'Dgree', 'Q25':'Salary'})Dgr_Sal_21 = (Dgr_Sal_21.replace(['0-999','$0-999','0'], '&lt; 999') .replace({'&gt;$1,000,000':'200,000~'}) .replace(['1,000-1,999','2,000-2,999','3,000-3,999', '4,000-4,999','5,000-7,499','7,500-9,999','10,000-14,999', '15,000-19,999'],'1,000-20,000') .replace(['20,000-24,999''25,000-29,999','30,000-39,999', '40,000-49,999', '50,000-59,999'],'20,000-59,999') .replace(['60,000-69,999', '70,000-79,999', '80,000-89,999', '90,000-99,999'], '60,000-99,999') .replace(['100,000-124,999', '300,000-499,999', '125,000-149,999', '125,000-149,999','150,000-199,999'],'100,000-199,999') .replace(['200,000-249,999', '250,000-299,999','1,000,000','$500,000-999,999'], '200,000~') .replace({'I prefer not to answer':'etc'}) .replace(['No formal education past high school', 'Some college/university study without earning a bachelor’s degree'],'~college') .replace(['Doctoral degree', 'Professional doctorate'],'Doctoral degree~'))#EastAsia 뽑기Dgr_Sal_21_Ea= Dgr_Sal_21[Dgr_Sal_21['region'] == &quot;EastAsia&quot;].reset_index(drop = True)Dgr_Sal_21_Ea = Dgr_Sal_21_Ea.groupby(['Dgree', 'Salary']).size().unstack().fillna(0).astype('int64')dgree_order=[ '~college','Bachelor’s degree', 'Master’s degree', 'Doctoral degree~', 'etc']#graph#Worldz = Dgr_Sal_21.groupby(['Dgree', 'Salary']).size().unstack().fillna(0).astype('int64')z = z[sal_order]z = z.reindex(dgree_order)z_data = z.apply(lambda x:np.round(x/x.sum()*100, 2), axis = 1).to_numpy() # convert to correlation matrixx = sal_ordery = dgree_orderfig = ff.create_annotated_heatmap(z_data, x = x, y = y, colorscale = &quot;sunset&quot;)fig.update_layout( title_text=&quot;&lt;b&gt; Degree-Salary in World&lt;/b&gt;&quot;, height=700, width=700, title_font_size=20, title_x=0.5, margin=dict(l=150, r=100, t=200, b=50))fig.update_traces(hovertemplate='&lt;b&gt;Degree&lt;/b&gt;: %{y}&lt;br&gt;'+ '&lt;b&gt;Salary&lt;/b&gt;: %{x}&lt;br&gt;'+ '&lt;b&gt;Percent&lt;/b&gt;: %{z}%')fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.1, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show()#East Asiaz = Dgr_Sal_21_Eaz = z[sal_order]z = z.reindex(dgree_order)z_data = z.apply(lambda x:np.round(x/x.sum()*100, 2), axis = 1).to_numpy() # convert to correlation matrixx = sal_ordery = dgree_orderfig = ff.create_annotated_heatmap(z_data, x = x, y = y, colorscale = &quot;sunset&quot;)fig.update_layout(title_text=&quot;&lt;b&gt; Degree-Salary in East Asia&lt;/b&gt;&quot;, height=700, width=700, title_font_size=20, title_x=0.5, margin=dict(l=150, r=100, t=200, b=50))fig.update_traces(hovertemplate='&lt;b&gt;Degree&lt;/b&gt;: %{y}&lt;br&gt;'+ '&lt;b&gt;Salary&lt;/b&gt;: %{x}&lt;br&gt;'+ '&lt;b&gt;Percent&lt;/b&gt;: %{z}%')fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.1, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() 3.1.8 Language transformation World & East Asia Programming Language: Bar plot - Python: 80% of the world and 85% of East Asia use it. We've been working on the project as python, so I hope we can continue to learn python and become experienced Data Scientists! 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081#data preprocessing#worldprogramming_list = [&quot;Python&quot;, &quot;R&quot;, &quot;SQL&quot;, &quot;Java&quot;, &quot;C&quot;, &quot;Bash&quot;, &quot;Javascript&quot;, &quot;C++&quot;]programming_df = pd.Series(programming_list)df_2019 = df19[df19['Q19'].isin(programming_df)]df_2020 = df20[df20['Q8'].isin(programming_df)]df_2021 = df21[df21['Q8'].isin(programming_df)]df19Lag = df_2019.loc[:, ['region', 'Q5', 'Q19', 'year']]df19Lag = df19Lag.rename(columns = {'Q19': 'Language'}, inplace = False) # To match with other datasetsdf20Lag = df_2020.loc[:, ['region', 'Q5', 'Q8', 'year']].rename(columns = {'Q8': 'Language'}, inplace = False)df21Lag = df_2021.loc[:, ['region', 'Q5', 'Q8', 'year']].rename(columns = {'Q8': 'Language'}, inplace = False)df3y_Lag = pd.concat([df19Lag, df20Lag, df21Lag])df3y_Lag = df3y_Lag.groupby(['year', 'Language']).size().reset_index().rename(columns = {0:&quot;Count&quot;})df3y_Lag# 2019dfLang_19 = df3y_Lag[df3y_Lag['year'] == &quot;2019&quot;].reset_index(drop = True)dfLang_19['percentage'] = dfLang_19[&quot;Count&quot;] / dfLang_19[&quot;Count&quot;].sum()dfLang_19['%'] = np.round(dfLang_19['percentage'] * 100, 1)# 2020dfLang_20 = df3y_Lag[df3y_Lag['year'] == &quot;2020&quot;].reset_index(drop = True)dfLang_20['percentage'] = dfLang_20[&quot;Count&quot;] / dfLang_20[&quot;Count&quot;].sum()dfLang_20['%'] = np.round(dfLang_20['percentage'] * 100, 1)# 2021dfLang_21 = df3y_Lag[df3y_Lag['year'] == &quot;2021&quot;].reset_index(drop = True)dfLang_21['percentage'] = dfLang_21[&quot;Count&quot;] / dfLang_21[&quot;Count&quot;].sum()dfLang_21['%'] = np.round(dfLang_21['percentage'] * 100, 1)dfLang_19=dfLang_19.sort_values(by='%', ascending=False)dfLang_20=dfLang_20.sort_values(by='%', ascending=False)dfLang_21=dfLang_21.sort_values(by='%', ascending=False)#graphfig = go.Figure()fig.add_trace(go.Bar(x = dfLang_19['Language'], y = dfLang_19['%'], name = &quot;2019&quot;, text = dfLang_19['%'].astype(str) + &quot;%&quot;, textposition='auto', marker_color='#CDD9A3'))fig.add_trace(go.Bar(x = dfLang_20['Language'], y = dfLang_20['%'], name = &quot;2020&quot;, text = dfLang_20['%'].astype(str) + &quot;%&quot;, textposition='auto', marker_color='#F28705'))fig.add_trace(go.Bar(x = dfLang_21['Language'], y = dfLang_21['%'], name = &quot;2021&quot;, text = dfLang_21['%'].astype(str) + &quot;%&quot;, textposition='auto', marker_color='#88BFBA'))fig.update_layout(title='&lt;b&gt;Language in World&lt;/b&gt;',title_font_size=20, margin = dict(t=100, l=100, r=50, b=100), height=600, width=700, xaxis_title=None, yaxis_title=None)fig.update_traces(hovertemplate='&lt;b&gt;Percent&lt;/b&gt;: %{y}%&lt;br&gt;'+ '&lt;b&gt;Language&lt;/b&gt;: %{x}&lt;br&gt;')fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.update_layout(legend=dict( orientation=&quot;v&quot;, yanchor=&quot;bottom&quot;, y=0.8, xanchor=&quot;right&quot;, x=1))fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.2, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#data prprocessing#Eadf_2019 = df19_Ea[df19_Ea['Q19'].isin(programming_df)]df_2020 = df20_Ea[df20_Ea['Q8'].isin(programming_df)]df_2021 = df21_Ea[df21_Ea['Q8'].isin(programming_df)]df19Lag = df_2019.loc[:, ['region', 'Q5', 'Q19', 'year']]df19Lag = df19Lag.rename(columns = {'Q19': 'Language'}, inplace = False) # To match with other datasetsdf20Lag = df_2020.loc[:, ['region', 'Q5', 'Q8', 'year']].rename(columns = {'Q8': 'Language'}, inplace = False)df21Lag = df_2021.loc[:, ['region', 'Q5', 'Q8', 'year']].rename(columns = {'Q8': 'Language'}, inplace = False)df3y_Lag = pd.concat([df19Lag, df20Lag, df21Lag])df3y_Lag = df3y_Lag.groupby(['year', 'Language']).size().reset_index().rename(columns = {0:&quot;Count&quot;})df3y_Lag# 2019dfLang_19 = df3y_Lag[df3y_Lag['year'] == &quot;2019&quot;].reset_index(drop = True)dfLang_19['percentage'] = dfLang_19[&quot;Count&quot;] / dfLang_19[&quot;Count&quot;].sum()dfLang_19['%'] = np.round(dfLang_19['percentage'] * 100, 1)# 2020dfLang_20 = df3y_Lag[df3y_Lag['year'] == &quot;2020&quot;].reset_index(drop = True)dfLang_20['percentage'] = dfLang_20[&quot;Count&quot;] / dfLang_20[&quot;Count&quot;].sum()dfLang_20['%'] = np.round(dfLang_20['percentage'] * 100, 1)# 2021dfLang_21 = df3y_Lag[df3y_Lag['year'] == &quot;2021&quot;].reset_index(drop = True)dfLang_21['percentage'] = dfLang_21[&quot;Count&quot;] / dfLang_21[&quot;Count&quot;].sum()dfLang_21['%'] = np.round(dfLang_21['percentage'] * 100, 1)dfLang_19=dfLang_19.sort_values(by='%', ascending=False)dfLang_20=dfLang_20.sort_values(by='%', ascending=False)dfLang_21=dfLang_21.sort_values(by='%', ascending=False)#graphfig = go.Figure()fig.add_trace(go.Bar(x = dfLang_19['Language'], y = dfLang_19['%'], name = &quot;2019&quot;, text = dfLang_19['%'].astype(str) + &quot;%&quot;, textposition='auto', marker_color='#CDD9A3'))fig.add_trace(go.Bar(x = dfLang_20['Language'], y = dfLang_20['%'], name = &quot;2020&quot;, text = dfLang_20['%'].astype(str) + &quot;%&quot;, textposition='auto', marker_color='#F28705'))fig.add_trace(go.Bar(x = dfLang_21['Language'], y = dfLang_21['%'], name = &quot;2021&quot;, text = dfLang_21['%'].astype(str) + &quot;%&quot;, textposition='auto', marker_color='#88BFBA'))fig.update_layout(title='&lt;b&gt;Language in EastAsia&lt;/b&gt;',title_font_size=20, margin = dict(t=100, l=100, r=50, b=100), height=600, width=700, xaxis_title=None, yaxis_title=None)fig.update_traces(hovertemplate='&lt;b&gt;Percent&lt;/b&gt;: %{text}')fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.2, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() 3.2 Position of Data Scientist in East Asia 123456789101112131415161718192021222324# data preprocessingdf21_Ea_DS = df21_Ea[df21_Ea['Q5'].isin(Data_Scientist)].fillna(0)salary_order= ['&lt;999', '1,000-19,999', '20,000-59,999', '60,000-99,999','100,000-199,999', '200,000~']dgree_order=[ '~college','Bachelor’s degree', 'Master’s degree', 'Doctoral degree~', 'etc']df21_Ea_DS=(df21_Ea_DS #salary .replace({'$0-999':'&lt;999','&gt;$1,000,000':'1,000,000~','$500,000-999,999':'500,000-999,999'}) .replace(['1,000-1,999','2,000-2,999','3,000-3,999', '4,000-4,999','5,000-7,499','7,500-9,999','10,000-14,999', '15,000-19,999'],'1,000-19,999') .replace(['20,000-24,999','25,000-29,999','30,000-39,999', '40,000-49,999', '50,000-59,999'],'20,000-59,999') .replace(['60,000-69,999', '70,000-79,999', '80,000-89,999', '90,000-99,999'], '60,000-99,999') .replace(['100,000-124,999','125,000-149,999','150,000-199,999'],'100,000-199,999') .replace(['200,000-249,999', '250,000-299,999', '300,000-499,999','500,000-999,999', '1,000,000~'], '200,000~') #degree .replace({'I prefer not to answer':'etc'}) .replace(['No formal education past high school','Some college/university study without earning a bachelor’s degree'],'~college') .replace(['Doctoral degree', 'Professional doctorate'],'Doctoral degree~') )sal_order= ['&lt;999', '1,000-19,999', '20,000-59,999', '60,000-99,999','100,000-199,999', '200,000~']dgree_order=[ '~college','Bachelor’s degree', 'Master’s degree', 'Doctoral degree~', 'etc'] 3.2.1 Salary 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162df21_Ea_DS_= df21_Ea_DS.loc[:,['Q5','Q25']].reset_index().rename(columns={'Q5':'Data_Scientist', 'Q25':'Salary'}).fillna('etc')df21_Ea_DS_= (df21_Ea_DS_.groupby(['Data_Scientist', 'Salary']).size() .reset_index() .rename(columns = {0:&quot;Count&quot;}))#Data Scientistdf21_Ea_DS_Ds = df21_Ea_DS_[df21_Ea_DS_['Data_Scientist'] == &quot;Data Scientist&quot;].reset_index(drop = True)df21_Ea_DS_Ds['%']=((df21_Ea_DS_Ds['Count'] / df21_Ea_DS_Ds['Count'].sum())*100).round(2)#Machine Learning Engineerdf21_Ea_DS_Mle = df21_Ea_DS_[df21_Ea_DS_['Data_Scientist'] == &quot;Machine Learning Engineer&quot;].reset_index(drop = True)df21_Ea_DS_Mle['%']=((df21_Ea_DS_Mle['Count'] / df21_Ea_DS_Mle['Count'].sum())*100).round(2)#Research Scientistdf21_Ea_DS_Rs = df21_Ea_DS_[df21_Ea_DS_['Data_Scientist'] == &quot;Research Scientist&quot;].reset_index(drop = True)df21_Ea_DS_Rs['%']=((df21_Ea_DS_Rs['Count'] / df21_Ea_DS_Rs['Count'].sum())*100).round(2)df21_Ea_DS_Rsdf21_Ea_DS_salary = pd.concat([df21_Ea_DS_Ds, df21_Ea_DS_Mle, df21_Ea_DS_Rs], ignore_index = True)df21_Ea_DS_salary= pd.pivot(df21_Ea_DS_salary, index = &quot;Salary&quot;, columns = 'Data_Scientist', values = &quot;%&quot;).reset_index().fillna('0')df21_Ea_DS_salary= df21_Ea_DS_salary.set_index(&quot;Salary&quot;).reindex(sal_order)#graphfig = go.Figure()fig.add_trace(go.Bar(x = df21_Ea_DS_salary.index, y = df21_Ea_DS_salary['Data Scientist'], name = &quot;Data Scientist&quot;, text = df21_Ea_DS_salary['Data Scientist'].astype(str) + &quot;%&quot;, textposition='auto', marker_color='#F2798F'))fig.add_trace(go.Bar(x = df21_Ea_DS_salary.index, y = df21_Ea_DS_salary['Machine Learning Engineer'], name = &quot;Machine Learning Engineer&quot;, text = df21_Ea_DS_salary['Machine Learning Engineer'].astype(str) + &quot;%&quot;, textposition='auto', marker_color='#CDD9A3'))fig.add_trace(go.Bar(x = df21_Ea_DS_salary.index, y = df21_Ea_DS_salary['Research Scientist'], name = &quot;Research Scientist&quot;, text = df21_Ea_DS_salary['Research Scientist'].astype(str) + &quot;%&quot;, textposition='auto', marker_color='#88BFBA'))fig.update_layout(barmode='stack', showlegend=True, height=600, width=700, title_text=&quot;&lt;b&gt;Data Scientist's Salary in East Asia&lt;/b&gt;&quot;, title_x=0.5, title_font_size=20, margin=dict(l=100, r=100, t=100, b=100))fig.update_traces(hovertemplate='&lt;b&gt;Percent&lt;/b&gt;: %{y}%&lt;br&gt;'+ '&lt;b&gt;Salary&lt;/b&gt;: %{x}$&lt;br&gt;')fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.update_layout(legend=dict( orientation=&quot;v&quot;, yanchor=&quot;bottom&quot;, y=0.8, xanchor=&quot;right&quot;, x=1.2))fig.show() 3.2.2 Salary-Experience 12345678910111213141516171819202122232425df21Ea_DS_ExSal = df21_Ea_DS.loc[:,['Q6','Q25']].reset_index().rename(columns={'Q25':'Salary', 'Q6':'Exp'}).fillna('etc')df21Ea_DS_ExSal= (df21Ea_DS_ExSal.groupby(['Exp', 'Salary']).size().unstack().fillna(0).astype('int64'))Exp_order=['&lt; 1 years','1-3 years','3-5 years', '5-10 years', '10-20 years', '20+ years', 'I have never written code']df21Ea_DS_ExSalz = df21Ea_DS_ExSalz = z[sal_order]z = z.reindex(Exp_order)z_data = z.apply(lambda x:np.round(x/x.sum()*100, 2), axis = 1).to_numpy() # convert to correlation matrixx = sal_ordery = Exp_orderfig = ff.create_annotated_heatmap(z_data, x = x, y = y, colorscale = &quot;sunset&quot;)fig.update_layout(title_text=&quot;&lt;b&gt; Data Scientist's Experience &amp; Salary &lt;/b&gt;&quot;,title_font_size=20, height=700, width=700, title_x=0.5, margin=dict(l=100, r=100, t=200, b=100))fig.update_traces(hovertemplate='&lt;b&gt;Salary&lt;/b&gt;: %{y}&lt;br&gt;'+ '&lt;b&gt;Experience&lt;/b&gt;: %{x}&lt;br&gt;'+ '&lt;b&gt;Percent&lt;/b&gt;: %{z}%')fig.show() 3.2.3 Degree 1234567891011121314151617181920212223df21_Ea_degree = df21_Ea_DS['Q4'].value_counts().to_frame()degree = df21_Ea_degree.indexvalues = df21_Ea_degree['Q4'].tolist()colors = ['#F2798F','#88BFBA', '#CDD9A3', '#F28705', '#D9946C']fig = go.Figure(data=[go.Bar(name='Degree', x=degree, y=values ,orientation='v', marker_color=colors, text=values, textposition='outside')])fig.update_layout(title_text=&quot;&lt;b&gt;Data Scientist's Degree (2021)&lt;/b&gt;&quot;, title_font_size=20, height=600, width=700, title_x=0.5, margin=dict(l=100, r=100, t=200, b=100))fig.update_traces(hovertemplate='&lt;b&gt;Count&lt;/b&gt;: %{y}&lt;br&gt;'+ '&lt;b&gt;Degree&lt;/b&gt;: %{x}&lt;br&gt;')fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.2, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() 3.2.4 Salary-Degree 1234567891011121314151617181920212223242526272829303132df21Ea_DS_EduSal= df21_Ea_DS.loc[:, ['Q4', 'Q25']].rename(columns={'Q4':'Edu', 'Q25':'Salary'})df21Ea_DS_EduSal['Edu'].unique()Edu_order=['~college', 'Bachelor’s degree','Master’s degree', 'Doctoral degree~', 'etc']df21Ea_DS_EduSal= (df21Ea_DS_EduSal.groupby(['Edu', 'Salary']).size().unstack().fillna(0).astype('int64'))df21Ea_DS_EduSalz = df21Ea_DS_EduSalz = z[sal_order]z = z.reindex(Edu_order)z_data = z.apply(lambda x:np.round(x/x.sum()*100, 2), axis = 1).to_numpy() # convert to correlation matrixx = sal_ordery = Edu_orderfig = ff.create_annotated_heatmap(z_data, x = x, y = y, colorscale = &quot;sunset&quot;)fig.update_layout(title_text=&quot;&lt;b&gt; Data Scientist's Degree &amp; Salary &lt;/b&gt;&quot;, title_font_size=20, height=700, width=700, title_x=0.5, margin=dict(l=150, r=100, t=200, b=50))fig.update_traces(hovertemplate='&lt;b&gt;Degree&lt;/b&gt;: %{y}&lt;br&gt;'+ '&lt;b&gt;Salary&lt;/b&gt;: %{x}&lt;br&gt;'+ '&lt;b&gt;Percent&lt;/b&gt;: %{z}%')fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.1, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() 3.2.5 Language 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677#data preprocessingdf20_Ea_DS = df20_Ea[df20_Ea['Q5'].isin(Data_Scientist)]df19_Ea_DS =df19_Ea[df19_Ea['Q5'].isin(Data_Scientist)]df19Ea_DSLag = df19_Ea_DS.loc[:, [ 'Q5', 'Q19', 'year']]df19Ea_DSLag = df19Ea_DSLag.rename(columns = {'Q19': 'Language'}, inplace = False) # To match with other datasetsdf20Ea_DSLag = df20_Ea_DS.loc[:, [ 'Q5', 'Q8', 'year']].rename(columns = {'Q8': 'Language'}, inplace = False)df21Ea_DSLag = df21_Ea_DS.loc[:, [ 'Q5', 'Q8', 'year']].rename(columns = {'Q8': 'Language'}, inplace = False)df3y_Ds_Lag = pd.concat([df19Ea_DSLag, df20Ea_DSLag, df21Ea_DSLag])df3y_Ds_Lag = df3y_Ds_Lag.groupby(['year', 'Language']).size().reset_index().rename(columns = {0:&quot;Count&quot;})df3y_Ds_Lag# 2019dfLang_Ds_19 = df3y_Ds_Lag[df3y_Ds_Lag['year'] == &quot;2019&quot;].reset_index(drop = True)dfLang_Ds_19['percentage'] = dfLang_Ds_19[&quot;Count&quot;] / dfLang_Ds_19[&quot;Count&quot;].sum()dfLang_Ds_19['%'] = np.round(dfLang_Ds_19['percentage'] * 100, 1)# 2020dfLang_Ds_20 = df3y_Ds_Lag[df3y_Ds_Lag['year'] == &quot;2020&quot;].reset_index(drop = True)dfLang_Ds_20['percentage'] = dfLang_Ds_20[&quot;Count&quot;] / dfLang_Ds_20[&quot;Count&quot;].sum()dfLang_Ds_20['%'] = np.round(dfLang_Ds_20['percentage'] * 100, 1)# 2021dfLang_Ds_21 = df3y_Ds_Lag[df3y_Ds_Lag['year'] == &quot;2021&quot;].reset_index(drop = True)dfLang_Ds_21['percentage'] = dfLang_Ds_21[&quot;Count&quot;] / dfLang_Ds_21[&quot;Count&quot;].sum()dfLang_Ds_21['%'] = np.round(dfLang_Ds_21['percentage'] * 100, 1)dfLang_Ds_19=dfLang_Ds_19.sort_values(by='%', ascending=False)dfLang_Ds_20=dfLang_Ds_20.sort_values(by='%', ascending=False)dfLang_Ds_21=dfLang_Ds_21.sort_values(by='%', ascending=False)#graphfig = go.Figure()fig.add_trace(go.Bar(x = dfLang_Ds_19['Language'], y = dfLang_Ds_19['%'], name = &quot;2019&quot;, text = dfLang_Ds_19['%'].astype(str) + &quot;%&quot;, textposition='auto', marker_color='#CDD9A3'))fig.add_trace(go.Bar(x = dfLang_Ds_20['Language'], y = dfLang_Ds_20['%'], name = &quot;2020&quot;, text = dfLang_Ds_20['%'].astype(str) + &quot;%&quot;, textposition='auto', marker_color='#F28705'))fig.add_trace(go.Bar(x = dfLang_Ds_21['Language'], y = dfLang_Ds_21['%'], name = &quot;2021&quot;, text = dfLang_Ds_21['%'].astype(str) + &quot;%&quot;, textposition='auto', marker_color='#88BFBA'))fig.update_layout(title='&lt;b&gt; The language used by the data scientist&lt;/b&gt;',title_font_size=22, margin = dict(t=120, l=100, r=10, b=150), height=600, width=700)fig.update_traces(hovertemplate='&lt;b&gt;Percent&lt;/b&gt;: %{y}%&lt;br&gt;'+ '&lt;b&gt;Language&lt;/b&gt;: %{x}&lt;br&gt;')fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.update_layout(legend=dict( orientation=&quot;h&quot;, yanchor=&quot;bottom&quot;, y=0.8, xanchor=&quot;right&quot;, x=1))fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.2, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() 1234567891011121314151617181920ds_pc=(df21_Ea_DS.loc[:, ['Q5','Q25','Q6','Q4','Q8']] .replace({'I have never written code': '&lt; 1 years', '1-3 years': '1-2 years'}) .replace(['10-20 years', '20+ years'], '10+ years' ) .replace([0,'&lt;999']) )fig = px.parallel_categories(ds_pc, labels={'Q5':'Job', 'Q25':'Salary', 'Q6':'Experience', 'Q4':'Degree', 'Q8':'Language'})fig.update_layout(hovermode = 'x')fig.update_layout(title='&lt;b&gt; Data Scientist&lt;/b&gt;',title_font_size=20, margin = dict(t=120, l=100, r=10, b=150), height=600, width=700)fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.2, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() 4. Ref. Ref. 동아시아 지역 https://ko.wikipedia.org/wiki/%EB%8F%99%EC%95%84%EC%8B%9C%EC%95%84 동아시아 인구 https://ko.wikipedia.org/wiki/%EC%95%84%EC%8B%9C%EC%95%84%EC%9D%98_%EC%9D%B8%EA%B5%AC 세계 인구 https://ko.wikipedia.org/wiki/%EC%84%B8%EA%B3%84_%EC%9D%B8%EA%B5%AC https://ko.wikipedia.org/wiki/%EC%9D%B8%EA%B0%84_%EA%B0%9C%EB%B0%9C_%EC%A7%80%EC%88%98#2020%EB%85%84 동아시아 인간개발지수 https://namu.wiki/w/%EB%8F%99%EC%95%84%EC%8B%9C%EC%95%84 Data Scientist란 https://dataprofessional.tistory.com/126 https://terms.naver.com/entry.naver?docId=1691563&amp;cid=42171&amp;categoryId=42183 Kaggle이란 https://ko.wikipedia.org/wiki/%EC%BA%90%EA%B8%80 Python이란 https://ko.wikipedia.org/wiki/%ED%8C%8C%EC%9D%B4%EC%8D%AC Kaggle competition Ref. https://www.kaggle.com/miguelfzzz/the-typical-kaggle-data-scientist-in-2021 https://www.kaggle.com/desalegngeb/how-popular-is-kaggle-in-africa flaricon: Icons made by Freepik from www.flaticon.com 5. close 안녕하세요 한국에 사는 YH입니다. python을 배운지 한달이 채 안되서 명이 한 팀이 되어 이번 대회에 참가 하게 되었습니다. 많이 부족하지만 여기까지 읽어 주셔서 감사합니다. 아직은 너무너무 부족한 제출물 이지만, 앞으로 열심히 해서 케글 대회에서 1등하는 그 날까지 지켜봐 주세요 ^^! 혹시 코멘트로 다 전하지 못하셨던 말이 있으시다면, 저의 github blog에 방문하여 도움을 주세요! 별거 없지만 놀러오세요 ;-) Hello, I’m YH and I live in Korea.Less than a month after learning python, people became a team and participated in this competition. It’s not enough, but thank you for reading it up to here. It’s still not enough, but please watch until the day we win first place at the Kaggle competition ^^! If there’s anything you haven’t said in the comments, please visit my github blog and help me! It’s nothing special, but come and play. ;-)","link":"/2021/11/28/kgg/KaggleCompetition_ver33byYH_Finall/"},{"title":"How Popular is kaggle in Africa?","text":"1. Introduction1.1 Introduction800 사용자가 사용하고 있는 글로벌 온라인 커뮤니티 케글. 194개의 나라에서 사용중. 2017년부터 머신러닝과 data 과학자들을 대상으로 설문을 했는데, 본 저자는 아프리카 나라들의 참여를 알아보고자 한다. Historical overview 이용 1.2 Table of Contents How does Africa compares with rest of the world? (Region(Q3)) 응답자 수(Africa/전체, 2021): bar-H (Region(Q3)) Africa에서 kgg사용증가 : bar Which African countries are kaggle-aware? (Q3) kgg을 사용하는 Africa나라 (2021): plotly_choroplethMap (Q3) kgg 사옹 증가 나라별 in Africa : heatmap (Q3) 조사에 참여한 나라 비율 in Africa (2021): Pie-Chart(donut) Demography : Age and Gender (Q1) age / (Q2) gender (Africa/전체, 2021): bar /bar-H (Q2, Q2, Q2, Q1, Genderselect) African 여성 비율 : bar Education, Jobs and Experience (Q4)학력/ (Q5)직업/ (Q6)경력 (Africa/전체, 2021): bar-H Programming Languages &amp; IDE’s (Q7-13)프로그래밍 언어 선호 (Africa/전체, 2021): heatmap (Q9-13)IDE (Africa/전체, 2021): bar-H (Q8)추천 프로그래밍 언어 (Africa/전체, 2021): bar-H Machine Learning: Experience, Framework and Algorithms (Q15)Muchine Learning 경력 (Africa/전체, 2021): bar-H (Q16-18)Muchine Learning 플랫폼 (Africa/전체, 2021): bar-H (Q17-12)Muchine Learning 알고리즘 (Africa/전체, 2021): bar-H (Q18-7)computer vision Methods (Africa/전체, 2021): Heatmap (Q19-6)자연언어 처리방법 (Africa/전체, 2021): Heatmap (Q14-12)visualizations Library (Africa/전체, 2021): Heatmap Computing Resources (Q11)hardware platform (Africa/전체, 2021): bar-H (Q13)TPU 사용빈도 (Africa/전체, 2021): bar-H (Q12-6)특별한 하드웨어 (Africa/전체, 2021): Heatmap (Q27_A 12)온라인 플렛폼 (Africa/전체, 2021): bar-H (Q28)클라우드 플렛폼 (Africa/전체, 2021): bar-H (29_A, 5)클라우드 만들때 쓰는 기본 resource? (Africa/전체, 2021): bar-H (32_A, 12빅데이터 만들때 (Africa/전체, 2021): bar-H (Q33)가장많이 쓰는 big data 생성 프로그램 (Africa/전체, 2021): bar-H Employment and role at work (Q20) 최근 고용주 (Africa/전체, 2021): bar-H (Q24)일할때 중요한 활동 (Africa/전체, 2021): Heatmap (Q25) 급여 피라미드 (Africa/전체, 2021): dual bar-H Learning Platform and Media (Q40) data science 배우는 플랫폼 (Africa/전체, 2021): bar-H (Q42-12) 가장 좋아하는 DS Topics 미디어 소스 (Africa/전체, 2021): Heatmap Summary Reference 특정 연도를 넣지 않은 부분은 historical data 2. Import2.1 dataFrame &amp; visualization Module1234import numpy as npimport pandas as pdimport seaborn as snsimport matplotlib.pylab as plt Numpy dataframe Pandas dataframe seabornSeaborn은 Metplotlib를 기본으로 생상테마와 통계용 차트 기능을 추가한 시각화 패키지 기본적인 시각화 기능은 Matplotlib, 통계는 Statsmodels에 의존한다. Ref. seaborn tutorial/En matplotlibmatplotlib.pyplot 모듈은 명령어 스타일로 동작하는 함수의 Library. 함수를 이용하여 그래프를 만들고, grid를 조정하고, Label도 꾸미는 등을 할 수 있다. Ref. matplotlib.pyplot 2.2 plotly1234567891011121314import plotly.io as pioimport plotly.express as pximport plotly.graph_objects as goimport plotly.figure_factory as fffrom plotly.subplots import make_subplots #특정한 차트를 만들기 위해 넣어줌from plotly.offline import init_notebook_mode, iplot #offLine에서도 돌아갈 수 있게 해줌init_notebook_mode(connected=True)pio.templates.default = &quot;none&quot;# import plotly.offline as py# py.offline.init_notebook_mode() Plotlyplotly Livrary 전체를 pio로 받아옴.그 중에서 px : plotly-express go : graph_objects ff : figure_factory px가 존재하기 이전 go로 생성하기 어려운 특정 유형의 플롯을 생성 할 수 있는 전용 함수가 있다. 를 받아 왔다. plotly에서 G를 그리는 방법은 2가지가 있는다. px : 템플릿을 통해 제작 go : 그래프를 하나하나 설정하며 제작 plotly_python plolty-tutorial-guide/Ko offline plotly offline plotly 1234567import osfor dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename))import warningswarnings.filterwarnings(&quot;ignore&quot;) data input 하는 방법.여러개의 csv file을 경로를 지정 해주어 한번에 넣어주는 code 12345df17= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2017/multipleChoiceResponses.csv&quot;, encoding=&quot;ISO-8859-1&quot;)df18= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2018/multipleChoiceResponses.csv&quot;, )df19= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2019/multiple_choice_responses.csv&quot;, )df20= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2020/kaggle_survey_2020_responses.csv&quot;, )df21= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2021/kaggle_survey_2021_responses.csv&quot;, ) 연도 별로 dataFrame을 씌워서 csv file을 dfyy객체에 Loading해 준다.","link":"/2021/11/10/kgg/Kgg_Africa/"},{"title":"kaggle in Africa_Fig","text":"1. IntroductionHelper functions 1.1 horizontal bar graphsGraph의 code 의 경우 해당 data와 연동해서 한꺼번에 보기로 한다. &lt; plot의 종류 &gt; plotly_hBar (df, q, title, height=400,l=250,r=50,b=50,t=100,) plotly_vBar(df, q, title, l=50,r=50,b=50,t=100) head_count(df, question_num, parts): head_count function copied from df_with_percentages(df, q, n, region) plot_barH_percent(df1, df2, title, l=150, r=50, b=50, t=100) annotated_heatmap(df_w, df_a, title, width=850) categorical_scatter(df1, df2, title, l=150, r=50, b=50, t=100) annotated_heatmap_Trans(df_w, df_a, title, width=850, height=750, l=150) head_count_suf(df, question_num, part, n) df_with_percentages_suf(df, q, part, n, region) 위의 코드의 경우 쓰여지지 않은 코드도 있는 것 같지만, 일단 List UP 해 놓음. 1.2 grouping african countries1.2.1 연도 별 Africa 국가 이름 df123456africa17 = ['Nigeria','Kenya', 'South Africa', 'Egypt']africa18 = ['Nigeria','Kenya', 'South Africa', 'Egypt', 'Tunisia', 'Morocco'] africa19 = ['Nigeria','Kenya', 'South Africa', 'Egypt', 'Tunisia', 'Morocco', 'Algeria']africa20 = ['Nigeria','Kenya', 'South Africa', 'Egypt', 'Tunisia', 'Morocco', 'Ghana']africa21 = ['Nigeria','Kenya', 'South Africa', 'Egypt', 'Tunisia', 'Morocco', 'Algeria', 'Ghana', 'Uganda', 'Ethiopia'] 아마도 직접 수기로 찾은 것 같다. 1.2.2 국가 이름 확인 pd.isin(): List에 존재하는 요소가 대상 dataFrame, series에 존재 여부를 Boolean type으로 반환. 123456789101112131415161718192021africa = ['Nigeria', 'Egypt', 'South Africa', 'Algeria', 'Tunisia', 'Morocco', 'Kenya', 'Uganda', 'Ghana', 'Ethiopia']df21_africa = df21[df21['Q3'].isin(africa)]df21_world = df21[~df21['Q3'].isin(africa )]df21['region']=[&quot;Africa&quot; if x in africa else &quot;World&quot; for x in df21['Q3']]df20_africa = df20[df20['Q3'].isin(africa)]df20_world = df20[~df20['Q3'].isin(africa )]df20['region']=[&quot;Africa&quot; if x in africa else &quot;World&quot; for x in df20['Q3']]df19_africa = df19[df19['Q3'].isin(africa)]df19_world = df19[~df19['Q3'].isin(africa)]df19['region']=[&quot;Africa&quot; if x in africa else &quot;World&quot; for x in df19['Q3']]df18_africa = df18[df18['Q3'].isin(africa)]df18_world = df18[~df18['Q3'].isin(africa)]df18['region']=[&quot;Africa&quot; if x in africa else &quot;World&quot; for x in df18['Q3']]df17_africa = df17[df17['Country'].isin(africa)]df17_world = df17[~df17['Country'].isin(africa )]df17['region']=[&quot;Africa&quot; if x in africa else &quot;World&quot; for x in df17['Country']] ‘africa’라는 배열을 만들어 df를 새로 정의 17’~21’까지 같은 내용이므로 21’의 내용만으로 정리 0&gt; df21 data 확인 1&gt; df21[‘Q3’]의 내용은 당신의 나라는 어디 입니까? 2&gt; 따라서 “ df21[‘Q3’].isin(africa) “ 코드의 의미는 Q3의 대답이 africa 이면 True 반환. 3&gt; 결론적으로 Q3의 대답이 Africa[]인 모든 대답을 추출 하게 된다. 4&gt; 반대로 dfworld의 경우 ~ ( not )을 사용하여 Q3이 false인 data frame을 추출 할 수 있는것. 1.2.3 region column을 추가1df21['region']=[&quot;Africa&quot; if x in africa else &quot;World&quot; for x in df21['Q3']] df21 dataframe에 Region이라는 column 을 추가해 보자. region 컬럼에 들어갈 값은 List의 끝까지 반복하되, 만약 df21[‘Q3’]의 값이 africa에 해당하면 “Africa”, 그 밖의 경우는 world를 입력해라. data science를 잘 하려면, python 문법도 잘 알아야 할 듯.","link":"/2021/11/10/kgg/Kgg_Africa_Fig/"},{"title":"kaggle in Africa_barH(1-1)","text":"1. Figure Helper functionsKgg_Africa 에서는 python 문법 중에서 함수를 만드는 def 을 이용하여 plot들을 정의 해 놓았다. def 함수명(매개변수): &lt;수행할 문장1&gt; &lt;수행할 문장2&gt; ... ref. python_Function/Ko. 1.1 horizontal bar graphs다음 results plot 을 뜯어보며 bar-H를 해석 해 보자. How does Africa compares with rest of the world? (Region(Q3)) 응답자 수(Africa/전체, 2021): bar-H 먼저, hBar는 다음과 같이 정의 되었다. 그동암 bar-H에대한 많은 부분을 공부 했으므로 간단히 함수를 중심으로 뜯어 보자. plotly.express.histogram 1234567891011121314151617181920212223242526272829303132333435363738394041424344def plotly_hBar(df, q, title, height=400,l=250,r=50,b=50,t=100,): fig = px.histogram(df.iloc[1:], y=q, orientation='h', width=700, height=height, histnorm='percent', color='region', color_discrete_map={ &quot;Africa&quot;: &quot;gold&quot;, &quot;World&quot;: &quot;salmon&quot; }, opacity=0.6 ) fig.update_layout(title=title, font_family=&quot;San Serif&quot;, bargap=0.2, barmode='group', titlefont={'size': 28}, paper_bgcolor='#F5F5F5', plot_bgcolor='#F5F5F5', legend=dict( orientation=&quot;v&quot;, y=1, yanchor=&quot;top&quot;, x=1.250, xanchor=&quot;right&quot;,) ).update_yaxes(categoryorder='total ascending') fig.update_traces(marker_line_color='black', marker_line_width=1.5) fig.update_layout(yaxis_title=None,yaxis_linewidth=2.5, autosize=False, margin=dict( l=l, r=r, b=b, t=t, ), ) fig.update_xaxes(showgrid=False) fig.update_yaxes(showgrid=False) fig.show() def plotly_hBar(df, q, title, height=400,l=250,r=50,b=50,t=100,) 함수 plotly_hBar의 정의 df, q, title등의 변수를 선언하고 값을 정해줌. fig 정의123456789101112fig = px.histogram(df.iloc[1:], y=q, orientation='h', width=700, height=height, histnorm='percent', color='region', color_discrete_map={ &quot;Africa&quot;: &quot;gold&quot;, &quot;World&quot;: &quot;salmon&quot; }, opacity=0.6 ) plotly.express.histogram() plotly의 express Library를 이용하여 histogram을 그려본다. df.iloc[1:] dataframe으로 iloc을 이용하여 컬럼을 가져옴 1행에서부터 끝까지 y= q, 나중에 q변수만 정해서 넣어주면 G가 그려진다. orientation= ‘h’, orientation이 h일땐, x orientation이 v일땐, y 를 하라고 공식문서에 써있는데 왜 얘는 이랬는지 알 수 없음 + Histogram plot ???. height = ‘height’, plot의 높이 지정 height=400이라고 함수 정의때 이미 지정 됨. color = ‘region’, 색은 region이라는 변수가 어떤것이냐에 따라 달라짐 color_discrete_map={“Africa”: “gold”, “World”: “salmon”}, dictionary처럼 Indexing 해 줌. opacity = 0.6 불 투명함의 정도 (0~1, flot) color_discrete_map 과 color_discrete_sequence 의 차이 dict with str keys and str values (default {}) , (list of str) plotly.express fig.update_layout()1234567891011121314fig.update_layout(title=title, font_family=&quot;San Serif&quot;, bargap=0.2, barmode='group', titlefont={'size': 28}, paper_bgcolor='#F5F5F5', plot_bgcolor='#F5F5F5', legend=dict( orientation=&quot;v&quot;, y=1, yanchor=&quot;top&quot;, x=1.250, xanchor=&quot;right&quot;,) ).update_yaxes(categoryorder='total ascending') fig.update_layout() :","link":"/2021/11/10/kgg/Kgg_Africa_bar-H/"},{"title":"East Asia, Data Scientist (kaggle in East-Asia)","text":"Data scientist in East Asia Data scientist로써 East Asia 에서 살아 남아보자 !Data ImportEast asia data에서 DS(Data scientist)뽑아내기12345678910111213141516171819202122232425262728# 21년 EastAsia 의 Data Scientist# 21년 EastAsia의 Data Scientist는 'Data Scientist', 'Machine Learning Engineer', 'Scientist/Researcher'# Data_Scientist =['Data Scientist', 'Research Scientist', 'Researcher','Machine Learning Engineer', 'Scientist/Researcher']df21_Ea_DS = df21_Ea[df21_Ea['Q5'].isin(Data_Scientist)]#21년 EastAsia의 Data Scientist 설문조사 응답자 리스트 dgree_order=[ '~college','Bachelor’s degree', 'Master’s degree', 'Doctoral degree~', 'etc']sal_order=['&lt; 999', '1,000-7,499', '7,500-24,999', '25,000-59,999', '60,000-99,999','100,000-199,999', '200,000~']df21_Ea_DS=(df21_Ea_DS.replace(['0-999','$0-999','0'], '&lt; 999') .replace({'&gt;$1,000,000':'200,000~'}) .replace(['1,000-1,999','2,000-2,999','3,000-3,999', '4,000-4,999','5,000-7,499'],'1,000-7,499') .replace(['7,500-9,999','10,000-14,999', '15,000-19,999', '20,000-24,999'],'7,500-24,999') .replace(['25,000-29,999','30,000-39,999', '40,000-49,999', '50,000-59,999'],'25,000-59,999') .replace(['60,000-69,999', '70,000-79,999', '80,000-89,999', '90,000-99,999'], '60,000-99,999') .replace(['100,000-124,999', '300,000-499,999', '125,000-149,999', '125,000-149,999', '150,000-199,999'],'100,000-199,999') .replace(['200,000-249,999', '250,000-299,999', '1,000,000','$500,000-999,999'], '200,000~') .replace({'I prefer not to answer':'etc'}).replace(['No formal education past high school', 'Some college/university study without earning a bachelor’s degree'],'~college') .replace(['Doctoral degree', 'Professional doctorate'],'Doctoral degree~')) replace.() replace.()를 data를 뽑아 내면서 사용 하면, 편하다. 다음 project 부터는 그렇게 사용하자! 순차적으로 적용 하더라도 replace.()는 맨 앞에 사용하자. data를 정제할 때 구획을 어디서 나누느냐는 presentation에 중요한 구성 요소이다.(강조할 부분이 바뀐다.) Ds의 연봉 뽑아내기123456789101112131415161718192021222324252627df21_Ea_DS_= df21_Ea_DS.loc[:,['Q5','Q25']].reset_index().rename(columns={'Q5':'Data_Scientist', 'Q25':'Salary'}).fillna('etc')df21_Ea_DS_= (df21_Ea_DS_.groupby(['Data_Scientist', 'Salary']).size() .reset_index() .rename(columns = {0:&quot;Count&quot;}) )#Data Scientistdf21_Ea_DS_Ds = df21_Ea_DS_[df21_Ea_DS_['Data_Scientist'] == &quot;Data Scientist&quot;].reset_index(drop = True)df21_Ea_DS_Ds['%']=((df21_Ea_DS_Ds['Count'] / df21_Ea_DS_Ds['Count'].sum())*100).round(2)# Salary21_Ea=Salary21_Ea.sort_values(by='%', ascending=False)#Machine Learning Engineerdf21_Ea_DS_Mle = df21_Ea_DS_[df21_Ea_DS_['Data_Scientist'] == &quot;Machine Learning Engineer&quot;].reset_index(drop = True)df21_Ea_DS_Mle['%']=((df21_Ea_DS_Mle['Count'] / df21_Ea_DS_Mle['Count'].sum())*100).round(2)#Research Scientistdf21_Ea_DS_Rs = df21_Ea_DS_[df21_Ea_DS_['Data_Scientist'] == &quot;Research Scientist&quot;].reset_index(drop = True)df21_Ea_DS_Rs['%']=((df21_Ea_DS_Rs['Count'] / df21_Ea_DS_Rs['Count'].sum())*100).round(2)df21_Ea_DS_Rsdf21_Ea_DS_salary = pd.concat([df21_Ea_DS_Ds, df21_Ea_DS_Mle, df21_Ea_DS_Rs], ignore_index = True)df21_Ea_DS_salary= pd.pivot(df21_Ea_DS_salary, index = &quot;Salary&quot;, columns = 'Data_Scientist', values = &quot;%&quot;).reset_index().fillna('0')df21_Ea_DS_salary= df21_Ea_DS_salary.set_index(&quot;Salary&quot;).reindex(sal_order) 뽑아낸 data를 합쳐서 하나의 표로 만든다. Excel에서 하는게 더 편하고 익숙하지만,python을 능숙 하게 다룰 수 잇는 언젠가가 오지 않을까 싶다. Ds의 연봉 bar graph 만들기123456789101112131415161718192021222324252627282930313233343536fig = go.Figure()fig.add_trace(go.Bar(x = df21_Ea_DS_salary.index, y = df21_Ea_DS_salary['Data Scientist'], name = &quot;Data Scientist&quot;, text = df21_Ea_DS_salary['Data Scientist'].astype(str) + &quot;%&quot;, textposition='auto'))fig.add_trace(go.Bar(x = df21_Ea_DS_salary.index, y = df21_Ea_DS_salary['Machine Learning Engineer'], name = &quot;Machine Learning Engineer&quot;, text = df21_Ea_DS_salary['Machine Learning Engineer'].astype(str) + &quot;%&quot;, textposition='auto'))fig.add_trace(go.Bar(x = df21_Ea_DS_salary.index, y = df21_Ea_DS_salary['Research Scientist'], name = &quot;Research Scientist&quot;, text = df21_Ea_DS_salary['Research Scientist'].astype(str) + &quot;%&quot;, textposition='auto'))fig.update_layout(barmode='stack', showlegend=True, margin=dict(pad=20), height=500, yaxis_title=None, xaxis_title=None, title_text=&quot;&lt;b&gt;21년 East Asia의 Data Scientist의 연봉&lt;/b&gt;&quot;, title_x=0.5, font=dict(size=17, color='#000000'), title_font_size=35)fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.show() [25,000-59,999] 이 구간이 East asia의 data scientist 들의 빈도가 가장 높은 연봉 구간이다. [7,500-24,999] 이 구간을 없애버리고 싶지만 (편입), 우선은 그냥 두기로 한다. HeatMap을 그려보자East asia의 DS들의 연봉과 경력간의 관계를 알아보고자 한다.12345678910111213141516171819202122df21Ea_DS_ExSal = df21_Ea_DS.loc[:,['Q6','Q25']].reset_index().rename(columns={'Q25':'Salary', 'Q6':'Exp'}).fillna('etc')df21Ea_DS_ExSal= (df21Ea_DS_ExSal.groupby(['Exp', 'Salary']).size().unstack().fillna(0).astype('int64'))# df21Ea_DS_ExSal['Exp'].unique()Exp_order=['&lt; 1 years','1-3 years','3-5 years', '5-10 years', '10-20 years', '20+ years', 'I have never written code']df21Ea_DS_ExSalz = df21Ea_DS_ExSalz = z[sal_order]z = z.reindex(Exp_order)z_data = z.apply(lambda x:np.round(x/x.sum()*100, 2), axis = 1).to_numpy() # convert to correlation matrixx = sal_ordery = Exp_orderfig = ff.create_annotated_heatmap(z_data, x = x, y = y, colorscale = &quot;sunset&quot;)fig.update_layout( title_text=&quot;&lt;b&gt;Data Scientist의 경력과 연봉 &lt;/b&gt;&quot;, height=700, width=800, title_x=0.5, margin=dict(l=200, r=100, t=200, b=100))fig.show() data Scientist의 경력과 연봉 상관관계를 Heatmap으로 그렸다. 정말 재미있는 사실은 [7,500-24,999], [60,000-99,999] 등의 구간이 비어 보인다. 혹시 연봉이 반올림되는 구간이 아닐까 생각한다. 다음에 연봉 구획을 다시 나눈다면 이런 부분을 신경쓰면서 나누어야 할 듯. [&lt;999] 구간은 생각보다 비율이 높은 걸을 알 수 있는데 이는 survey의 오류인듯 하다. 연봉인데 월급으로 생각했다던가… 1234567891011121314df21_Ea_degree = df21_Ea_DS['Q4'].value_counts().to_frame()degree = df21_Ea_degree.indexvalues = df21_Ea_degree['Q4'].tolist()fig = go.Figure(data=[ go.Bar(name='Degree', x=degree, y=values ,orientation='v')])fig.update_layout( title_text=&quot;&lt;b&gt;21년 East Asia의 Data Scientist의 학력&lt;/b&gt;&quot;, )fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.show() 1234567891011121314151617181920212223df21Ea_DS_EduSal= df21_Ea_DS.loc[:, ['Q4', 'Q25']].rename(columns={'Q4':'Edu', 'Q25':'Salary'})df21Ea_DS_EduSal['Edu'].unique()Edu_order=['~college', 'Bachelor’s degree','Master’s degree', 'Doctoral degree~', 'etc']df21Ea_DS_EduSal= (df21Ea_DS_EduSal.groupby(['Edu', 'Salary']).size().unstack().fillna(0).astype('int64'))df21Ea_DS_EduSalz = df21Ea_DS_EduSalz = z[sal_order]z = z.reindex(Edu_order)z_data = z.apply(lambda x:np.round(x/x.sum()*100, 2), axis = 1).to_numpy() # convert to correlation matrixx = sal_ordery = Edu_orderfig = ff.create_annotated_heatmap(z_data, x = x, y = y, colorscale = &quot;sunset&quot;)fig.update_layout( title_text=&quot;&lt;b&gt;Data Scientist의 경력과 연봉 &lt;/b&gt;&quot;, height=700, width=800, title_x=0.5, margin=dict(l=200, r=100, t=200, b=100))fig.show() East Asia의 ds들의 연봉은 거의 [25000-60000] 구간에 들어잇는 것 같다. 학위랑은 많이 상관 없어 보이며 심지어 200,000~$를 받는 학사학력자가 있다. 몹시 바람직하다. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455df20_Ea_DS = df20_Ea[df20_Ea['Q5'].isin(Data_Scientist)]df19_Ea_DS =df19_Ea[df19_Ea['Q5'].isin(Data_Scientist)]df19Ea_DSLag = df19_Ea_DS.loc[:, [ 'Q5', 'Q19', 'year']]df19Ea_DSLag = df19Ea_DSLag.rename(columns = {'Q19': 'Language'}, inplace = False) # To match with other datasetsdf20Ea_DSLag = df20_Ea_DS.loc[:, [ 'Q5', 'Q8', 'year']].rename(columns = {'Q8': 'Language'}, inplace = False)df21Ea_DSLag = df21_Ea_DS.loc[:, [ 'Q5', 'Q8', 'year']].rename(columns = {'Q8': 'Language'}, inplace = False)df3y_Ds_Lag = pd.concat([df19Ea_DSLag, df20Ea_DSLag, df21Ea_DSLag])df3y_Ds_Lag = df3y_Ds_Lag.groupby(['year', 'Language']).size().reset_index().rename(columns = {0:&quot;Count&quot;})df3y_Ds_Lag# 2019dfLang_Ds_19 = df3y_Ds_Lag[df3y_Ds_Lag['year'] == &quot;2019&quot;].reset_index(drop = True)dfLang_Ds_19['percentage'] = dfLang_Ds_19[&quot;Count&quot;] / dfLang_Ds_19[&quot;Count&quot;].sum()dfLang_Ds_19['%'] = np.round(dfLang_Ds_19['percentage'] * 100, 1)# 2020dfLang_Ds_20 = df3y_Ds_Lag[df3y_Ds_Lag['year'] == &quot;2020&quot;].reset_index(drop = True)dfLang_Ds_20['percentage'] = dfLang_Ds_20[&quot;Count&quot;] / dfLang_Ds_20[&quot;Count&quot;].sum()dfLang_Ds_20['%'] = np.round(dfLang_Ds_20['percentage'] * 100, 1)# 2021dfLang_Ds_21 = df3y_Ds_Lag[df3y_Ds_Lag['year'] == &quot;2021&quot;].reset_index(drop = True)dfLang_Ds_21['percentage'] = dfLang_Ds_21[&quot;Count&quot;] / dfLang_Ds_21[&quot;Count&quot;].sum()dfLang_Ds_21['%'] = np.round(dfLang_Ds_21['percentage'] * 100, 1)dfLang_Ds_19=dfLang_Ds_19.sort_values(by='%', ascending=False)dfLang_Ds_20=dfLang_Ds_20.sort_values(by='%', ascending=False)dfLang_Ds_21=dfLang_Ds_21.sort_values(by='%', ascending=False)fig = go.Figure()fig.add_trace(go.Bar(x = dfLang_Ds_19['Language'], y = dfLang_Ds_19['%'], name = &quot;2019&quot;, text = dfLang_Ds_19['%'].astype(str) + &quot;%&quot;, textposition='auto'))fig.add_trace(go.Bar(x = dfLang_Ds_20['Language'], y = dfLang_Ds_20['%'], name = &quot;2020&quot;, text = dfLang_Ds_20['%'].astype(str) + &quot;%&quot;, textposition='auto'))fig.add_trace(go.Bar(x = dfLang_Ds_21['Language'], y = dfLang_Ds_21['%'], name = &quot;2021&quot;, text = dfLang_Ds_21['%'].astype(str) + &quot;%&quot;, textposition='auto'))fig.show() 이 plot은 강사쌤의 도움을 많이 받았다. 2017, 2018년도도 넣고 싶었으나 data 찾는데 너무 시간이 많이 걸리는 것이 대회 마감이 얼마 남지 않은 이 시점에서 바람직 하지 못한 계획이라는 생각이 들어 이쯤에서 만족 하기로 했다. 비록 이 대회에서 우승 하지 못하겠지만,나에게 있어 이번 대회는 의미가 크다. 내 경력에는 큰 의미가 없을지언정 ㅎㅎ 123456789101112131415161718192021222324252627282930313233import numpy as npimport pandas as pdimport seaborn as snsimport matplotlib.pylab as pltimport plotly.io as pioimport plotly.express as pximport plotly.graph_objects as goimport plotly.figure_factory as fffrom plotly.subplots import make_subplotsfrom plotly.offline import init_notebook_mode, iplotinit_notebook_mode(connected=True)pio.templates.default = &quot;none&quot;# import plotly.offline as py# py.offline.init_notebook_mode()import osfor dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename))import warningswarnings.filterwarnings(&quot;ignore&quot;)#joypy를 쓰고 싶다면, sns에 싣어야 하는데 고민.#pip install joypy# import joypy# fig,axes = joypy.joyplot(df_duration, by='year',color=palette18, alpha=0.8)# plt.title('Time taken to complete the survey (in seconds)', size=14, fontweight='bold')# plt.show()# https://ichi.pro/ko/joypyleul-sayonghayeo-joy-plot-mandeulgi-113466494282576 123456789ds_pc=df21_Ea_DS.loc[:, ['Q5','Q25','Q6','Q4','Q8']]fig = px.parallel_categories(ds_pc, labels={'Q5':'Job', 'Q25':'Salary', 'Q6':'Experience', 'Q4':'Degree', 'Q8':'Language'})fig.update_layout(hovermode = 'x')fig.update_layout(title='&lt;b&gt; Data Scientist&lt;/b&gt;',title_font_size=20, margin = dict(t=120, l=100, r=10, b=150), height=600, width=700)fig.show()","link":"/2021/11/23/kgg/Kgg_EastAsia_DataScientist/"},{"title":"Subplots in python (kaggle in East-Asia)","text":"World Vs East Asia ##python을 이용한 plotly Library로 plot 그리기 subplots 를 이용하여 다중 그래프를 그려 보자. python Library Import1234567891011121314151617181920212223import numpy as npimport pandas as pdimport seaborn as snsimport matplotlib.pylab as pltimport plotly.io as pioimport plotly.express as pximport plotly.graph_objects as goimport plotly.figure_factory as fffrom plotly.subplots import make_subplotsfrom plotly.offline import init_notebook_mode, iplotinit_notebook_mode(connected=True)pio.templates.default = &quot;none&quot;# import plotly.offline as py# py.offline.init_notebook_mode()import osfor dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename))import warningswarnings.filterwarnings(&quot;ignore&quot;) data import123456df17= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2017/multipleChoiceResponses.csv&quot;, encoding=&quot;ISO-8859-1&quot;)df18= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2018/multipleChoiceResponses.csv&quot;, )df19= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2019/multiple_choice_responses.csv&quot;, )df20= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2020/kaggle_survey_2020_responses.csv&quot;, )df21= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2021/kaggle_survey_2021_responses.csv&quot;, ) data frame 전처리123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354total17 = ( df17['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index() )total18 = ( df18['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index() )total19 = ( df19['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index() )total20 = ( df20['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index() )total21 = ( df21['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index() ) plot 그리기make_subplots 로 여러개의 그래프를 한 plot에 담아 봅시다. pie그래프를 연도별로 담아 봅시다. 1234567891011121314151617181920212223242526272829303132333435# Create subplots: use 'domain' type for Pie subplot##subplots가 담길 행렬을 만들어 줍니다. rows와 cols를 맞춰서 제목도 달아 줍니다. fig = make_subplots(rows=1, cols=5, specs=[[{'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}]], subplot_titles=(&quot;2017&quot;, &quot;2018&quot;, &quot;2019&quot;, &quot;2020&quot;, &quot;2021&quot;))## scalegroup = one으로 설정 하게 되면, data의 크기에 따라 pie의 크기가 결정됩니다. fig.add_trace(go.Pie(labels=total21['type'], values=total21['respodents'], name=&quot;2021&quot;, scalegroup='one'), 1, 1)fig.add_trace(go.Pie(labels=total20['type'], values=total20['respodents'], name=&quot;2020&quot;, scalegroup='one'), 1, 2)fig.add_trace(go.Pie(labels=total19['type'], values=total19['respodents'], name=&quot;2019&quot;, scalegroup='one'), 1, 3)fig.add_trace(go.Pie(labels=total18['type'], values=total18['respodents'], name=&quot;2018&quot;, scalegroup='one'), 1, 4)fig.add_trace(go.Pie(labels=total17['type'], values=total17['respodents'], name=&quot;2017&quot;, scalegroup='one'), 1, 5)# Use `hole` to create a donut-like pie chartfig.update_traces(hole=.2, hoverinfo=&quot;label+percent+name&quot;)fig.update_layout( title_text=&quot;&lt;b&gt;World vs EastAsia&lt;/b&gt;&quot;, # Add annotations in the center of the donut pies. )fig.show()","link":"/2021/11/15/kgg/Kgg_EastAsia_KggHistorical/"},{"title":"Stacked Bar (kaggle in East-Asia)","text":"World Vs East Asia ##python을 이용한 plotly Library로 plot 그리기 subplots 를 이용하여 다중 그래프를 그려 보자. python Library Import1234567891011121314151617181920212223import numpy as npimport pandas as pdimport seaborn as snsimport matplotlib.pylab as pltimport plotly.io as pioimport plotly.express as pximport plotly.graph_objects as goimport plotly.figure_factory as fffrom plotly.subplots import make_subplotsfrom plotly.offline import init_notebook_mode, iplotinit_notebook_mode(connected=True)pio.templates.default = &quot;none&quot;# import plotly.offline as py# py.offline.init_notebook_mode()import osfor dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename))import warningswarnings.filterwarnings(&quot;ignore&quot;) data import123456df17= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2017/multipleChoiceResponses.csv&quot;, encoding=&quot;ISO-8859-1&quot;)df18= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2018/multipleChoiceResponses.csv&quot;, )df19= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2019/multiple_choice_responses.csv&quot;, )df20= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2020/kaggle_survey_2020_responses.csv&quot;, )df21= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2021/kaggle_survey_2021_responses.csv&quot;, ) data frame 전처리i) Q3를 기준으로 EastAsia에 속하는 나라만 연도별로 뽑아냅니다. 1234567891011121314151617181920212223242526272829303132df21_Ea=df21[df21['Q3'].isin(EastAsia21)]Ea21= ( df21_Ea['Q3'].value_counts().to_frame() .reset_index().rename(columns={'index':'Country', 'Q3':'21'}))df20_Ea=df20[df20['Q3'].isin(EastAsia)]Ea20= ( df20_Ea['Q3'].replace('Republic of Korea','South Korea') .value_counts().to_frame().reset_index() .rename(columns={'index':'Country', 'Q3':'20'}))df19_Ea=df19[df19['Q3'].isin(EastAsia)]Ea19= (df19_Ea['Q3'].replace('Republic of Korea','South Korea') .value_counts().to_frame().reset_index() .rename(columns={'index':'Country', 'Q3':'19'}))df18_Ea=df18[df18['Q3'].isin(EastAsia)]Ea18= (df18_Ea['Q3'].replace('Republic of Korea','South Korea') .value_counts().to_frame().reset_index() .rename(columns={'index':'Country', 'Q3':'18'}))Ea18.value_counts()#df18 열에 taiwan = 0을 추가 해야 합니다. df17_Ea = df17[df17['Country'].isin(EastAsia)]Ea17= (df17_Ea['Country'].replace(&quot;People 's Republic of China&quot;,'China') .value_counts().to_frame().reset_index() .rename(columns={'index':'Country', 'Country':'17'})) ii) data를 합쳐서 하나의 dataframe으로 만들음. 이 과정에서 pd.merge()를 사용 해 주었기 때문에 18’ taiwan data가 Nan으로 추가 되었다. 123456789fig = go.Figure(data=[ go.Bar(name='2017', x=df5years['Country'], y=df5years['17']), go.Bar(name='2018', x=df5years['Country'], y=df5years['18']), go.Bar(name='2019', x=df5years['Country'], y=df5years['19']), go.Bar(name='2020', x=df5years['Country'], y=df5years['20']), go.Bar(name='2021', x=df5years['Country'], y=df5years['21'])])fig.show() 123#Change the bar modefig.update_layout(barmode='stack', title='연도별 동아시아 Kaggle 사용자수' ) stacked bar로 할까말까 고민중. dictation 할 때 까지만 해도 bar 그래프 그리는 것이 뭐그리 어렵겠나? 했다. 그냥 복사 붙여넣기로 만드려고 했는데 그게 참 안되네 ㅂㄷㅂㄷ 123456789101112131415df5years_ =df5years.transpose()df5years_= df5years_.iloc[1:]fig2 = go.Figure(data=[ go.Bar(name='China', x=years, y=df5years_[0]), go.Bar(name='Japan', x=years, y=df5years_[1]), go.Bar(name='Taiwan', x=years, y=df5years_[2]), go.Bar(name='South Korea', x=years, y=df5years_[3]),])fig2.show() 축 reverse로 할까말까 고민중. 어떤게 더 잘 보여 줄 수 있을까 … ㅜㅜ","link":"/2021/11/15/kgg/Kgg_EastAsia_stackedBar/"},{"title":"kaggle HeatMap","text":"HeatMappython문법의 plotly Library를 이용하여 Heatmap을 알아보자 HeatMap의 Gradation 색을 바꾸고 싶다면, colorscales 을 참고 해 보자. HeatMap 112345678import plotly.figure_factory as ffz=[[1, 90, 30, 50, 1], [20, 1, 60, 80, 30], [30, 60, 1, 50, 20]]x=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']y=['Morning', 'Afternoon', 'Evening']fig = ff.create_annotated_heatmap(z, x = x, y = y, colorscale = &quot;Viridis&quot;)fig.show() Input data를 맞춰 줘야한다. x, y = List z= 배열 HeatMap 21234567891011121314151617181920212223242526272829import plotly.graph_objects as gofrom functools import reducefrom itertools import productz=[[1, 90, 30, 50, 1], [20, 1, 60, 80, 30], [30, 60, 1, 50, 20]]x=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']y=['Morning', 'Afternoon', 'Evening']def get_anno_text(z_value): annotations=[] a, b = len(z_value), len(z_value[0]) flat_z = reduce(lambda x,y: x+y, z_value) # z_value.flat if you deal with numpy coords = product(range(a), range(b)) for pos, elem in zip(coords, flat_z): annotations.append({'font': {'color': '#FFFFFF'}, 'showarrow': False, 'text': str(elem), 'x': pos[1], 'y': pos[0]}) return annotationsfig = go.Figure(data=go.Heatmap( z=z, x=x, y=y, hoverongaps = True))fig.update_layout(annotations = get_anno_text(z))fig.show() HeatMap 31234567891011121314151617181920212223import plotly.graph_objects as goz = df.groupby(['Q4', 'Q1']).size().unstack().fillna(0).astype('int16')# convert to correlation matrixz2 = z.apply(lambda x:x/x.sum(), axis = 1)x = z2.columnsy = z2.indexfig = go.Figure(data=go.Heatmap( z=z2.to_numpy(), #dataframe을 넘파이(배열)로 바꿔줌: List형태 x=x, y=y, type=&quot;heatmap&quot;, colorscale = &quot;Viridis&quot;, hoverongaps = False))fig.update_layout( title='Degree ~ Gender', xaxis_nticks=36)fig.show() z2 = z.apply(lambda x:x/x.sum(), axis = 1) 여기서 이 한줄로 인해 dataFrame이 Heatmap에 들어 갈 수 있는 상관관계G를 만들 수 있는 상태로 변환. data 자료형을 맞춰줘야 한다. (x, y, z) HeatMap Ref.Ref.","link":"/2021/11/16/kgg/Kgg_HeatMap/"},{"title":"kaggle in Korea(data둘러보기)","text":"#How popular is kaggle in South Korea? data 정제하기123456789101112131415161718192021222324df21_Ko = df21[df21['Q3'] == 'South Korea']df21_Wo = df21[~(df21['Q3'] == 'South Korea')]df21['region']=[&quot;Korea&quot; if x == 'South Korea' else &quot;World&quot; for x in df21['Q3']]df21['region'].value_counts()df20_Ko = df20[df20['Q3'] == 'South Korea']df20_Wo = df20[~(df20['Q3'] == 'South Korea')]df20['region']=[&quot;Korea&quot; if x == 'South Korea' else &quot;World&quot; for x in df20['Q3']]df20['region'].value_counts()df19_Ko = df19[df19['Q3'] == 'South Korea']df19_Wo = df19[~(df19['Q3'] == 'South Korea')]df19['region']=[&quot;Korea&quot; if x == 'South Korea' else &quot;World&quot; for x in df19['Q3']]df19['region'].value_counts()df18_Ko = df18[df18['Q3'] == 'South Korea']df18_Wo = df18[~(df18['Q3'] == 'South Korea')]df18['region']=[&quot;Korea&quot; if x == 'South Korea' else &quot;World&quot; for x in df18['Q3']]df18['region'].value_counts()df17_Ko = df17[df17['Country'] == 'South Korea']df17_Wo = df17[~(df17['Country'] == 'South Korea')]df17['region']=[&quot;Korea&quot; if x == 'South Korea' else &quot;World&quot; for x in df17['Country']]df17['region'].value_counts() &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD World 25615Korea 359Name: region, dtype: int64 World 19847Korea 190Name: region, dtype: int64 World 19536Korea 182Name: region, dtype: int64 World 23672Korea 188Name: region, dtype: int64 World 16522Korea 194Name: region, dtype: int64======= 2021 World 25615 Korea 359 Name: region, dtype: int64 2020 World 19847 Korea 190 Name: region, dtype: int64 2019 World 19536 Korea 182 Name: region, dtype: int64 2018 World 23672 Korea 188 Name: region, dtype: int64 2017 World 16522 Korea 194 Name: region, dtype: int64 trouble shootingdata 정제를 하다 보니 전체 data에서 korea가 1% 밖에 되지 않아 data set을 더 추가 하기로 했다. ##동아시아 Ref. East Asia 동아시아 East Asia에는 대한민국, 일본, 중국, 타이완, 몽골, 북조선 총 6개의 국가가 속해 있다. 알 수 없지만, 18년도엔 타이완이 없다. 12345678910111213141516171819202122232425262728293031323334## East Asia에는 대한민국, 일본, 중국, 타이완, 몽골, 북조선 총 6개의 국가가 속해 있다. ## 알 수 없지만, 18년도엔 타이완이 없다. EastAsia17 = ['China',&quot;People 's Republic of China&quot;, 'Taiwan', 'South Korea', 'Japan']EastAsia18= ['China', 'South Korea', 'Japan', 'Republic of Korea'] EastAsia19 = ['China','Taiwan', 'South Korea', 'Japan', 'Republic of Korea']EastAsia20 = ['China','Taiwan', 'South Korea','Republic of Korea', 'Japan']EastAsia21 = ['China','Taiwan', 'South Korea', 'Japan']EastAsia = ['Republic of Korea','China','Taiwan', 'South Korea', 'Japan', &quot;People 's Republic of China&quot; ]df21_Ea = df21[df21['Q3'].isin(EastAsia)]df21_Wo = df21[~df21['Q3'].isin(EastAsia )]df21['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df21['Q3']]df20_Ea = df20[df20['Q3'].isin(EastAsia)]df20_Wo = df20[~df20['Q3'].isin(EastAsia )]df20['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df20['Q3']]df19_Ea = df19[df19['Q3'].isin(EastAsia)]df19_Wo = df19[~df19['Q3'].isin(EastAsia )]df19['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df19['Q3']]df18_Ea = df18[df18['Q3'].isin(EastAsia)]df18_Wo = df18[~df18['Q3'].isin(EastAsia )]df18['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df18['Q3']]df17_Ea = df17[df17['Country'].isin(EastAsia)]df17_Wo = df17[~df17['Country'].isin(EastAsia )]df17['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df17['Country']]#df21['region'].to_frame().value_counts().to_frame().rename(columns={'region': '21y', '' : 'count'}) 21년도 를 .value_counts()로 뽑아 냈다. 1%대는 아니지만, 이제 10%대 data를 뽑아 냈다. 이것이 어떤 의미가 있을지 모르겠지만, 일단 주말동안 이 data로 궁금한 것을 Graph로 만들어 보자. 12345678910111213141516171819202122232425262728293031323334353637383940# % 계산을 위해 len() 을 통해 data 생성.Ea21 = len(df21_Ea)Wo21 = len(df21) - len(df21_Ea)Ea20 = len(df20_Ea)Wo20 = len(df20) - len(df20_Ea)Ea19 = len(df19_Ea)Wo19 = len(df19) - len(df19_Ea)Ea18 = len(df18_Ea)Wo18 = len(df18) - len(df18_Ea)Ea17 = len(df17_Ea)Wo17 = len(df17) - len(df17_Ea)def percent (a, b): result =a/(a+b)*100 return resultdef percentR (b, a): result =a/(a+b)*100 return resultcountry = ['East Asia', 'Rest of the World']years = ['2017', '2018', '2019', '2020', '2021']fig = go.Figure(data=[ go.Bar(name='Rest of the World', x=years, y=[percentR(Ea17, Wo17), percentR(Ea18, Wo18), percentR(Ea19, Wo19), percentR(Ea20, Wo20), percentR(Ea21, Wo21)]), go.Bar(name='East Asia', x=years, y=[percent(Ea17, Wo17), percent(Ea18, Wo18), percent(Ea19, Wo19), percent(Ea20, Wo20), percent(Ea21, Wo21)])])fig.update_layout(barmode='stack')fig.show() 일단, plot은 뽑아 보았는데 이래도 되나 싶다 ^^ 하하 노동력을 더해서 대륙 별로 뽑던지 해야겠다 ㅂㄷㅂㄷ 072bc76c34c0f369e5fe7e814291408da6a83ffc","link":"/2021/11/12/kgg/Kgg_Korea/"},{"title":"[Kgg]TPS data import(02)_Dec. 2021_","text":"origin of dictation 이전 포스팅 : Kgg_TPS 01 Description of competition 유투브, 인프런 강의 : self branding data analysis : 자격증 : ADsP 데이터 분석 준 전문가","link":"/2021/12/20/kgg/Kgg_TPS_data_import(02)/"},{"title":"[object Object]","text":"#Plotly Tutorial For Kaggle Survey Competitions 진도가 너무 더디게 나가서 Teacher’s code를 조금더 뜯어 본후 python for문이나 if문을 조금 더 잘 쓸 수 있을기를 바란다.","link":"/2021/11/15/kgg/Kgg_Teacher/"},{"title":"kgg compete data 정리","text":"data Import1234567891011121314151617181920212223242526272829import numpy as npimport pandas as pdimport seaborn as snsimport matplotlib.pylab as pltimport plotly.io as pioimport plotly.express as pximport plotly.graph_objects as goimport plotly.figure_factory as fffrom plotly.subplots import make_subplotsfrom plotly.offline import init_notebook_mode, iplotinit_notebook_mode(connected=True)pio.templates.default = &quot;none&quot;# import plotly.offline as py# py.offline.init_notebook_mode()import osfor dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename))import warningswarnings.filterwarnings(&quot;ignore&quot;)df17= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2017/multipleChoiceResponses.csv&quot;, encoding=&quot;ISO-8859-1&quot;)df18= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2018/multipleChoiceResponses.csv&quot;, )df19= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2019/multiple_choice_responses.csv&quot;, )df20= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2020/kaggle_survey_2020_responses.csv&quot;, )df21= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2021/kaggle_survey_2021_responses.csv&quot;, ) 질문 뽑는 법18년도의 질문 frame을 확인 해 보자 . 12questions = df18.iloc[0, :].Tquestions 질문 제거하기이후 data에 질문이 들어가면 안되기 때문에 질문을 제거 해 준다. 12345df17= df17.iloc[1:, :].replace(&quot;People 's Republic of China&quot;,'China')df18= df18.iloc[1:, :].replace('Republic of Korea','South Korea')df19= df19.iloc[1:, :].replace('Republic of Korea','South Korea')df20= df20.iloc[1:, :].replace('Republic of Korea','South Korea')df21= df21.iloc[1:, :] 연도 추가하기이후 연도별 data를 뽑기 위해서 data set에 연도를 추가 해 준다. 12345df21['year'] = '2021'df20['year'] = '2020'df19['year'] = '2019'df18['year'] = '2018'df17['year'] = '2017' 동아시아와 세계 나누기1df21['Q1'].unique() [Q3]에 나라가 있다. 나라를 확인하여 East Asia만 region column을 새로 만들어 분류 해 준다. 1234567891011121314151617181920212223242526272829EastAsia17 = ['China',&quot;People 's Republic of China&quot;, 'Taiwan', 'South Korea', 'Japan']EastAsia18= ['China', 'South Korea', 'Japan', 'Republic of Korea'] EastAsia19 = ['China','Taiwan', 'South Korea', 'Japan', 'Republic of Korea']EastAsia20 = ['China','Taiwan', 'South Korea','Republic of Korea', 'Japan']EastAsia21 = ['China','Taiwan', 'South Korea', 'Japan']EastAsia = ['Republic of Korea','China','Taiwan', 'South Korea', 'Japan', &quot;People 's Republic of China&quot; ]df21_Ea = df21[df21['Q3'].isin(EastAsia)]df21_Wo = df21[~df21['Q3'].isin(EastAsia)]df21['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df21['Q3']]df20_Ea = df20[df20['Q3'].isin(EastAsia)]df20_Wo = df20[~df20['Q3'].isin(EastAsia)]df20['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df20['Q3']]df19_Ea = df19[df19['Q3'].isin(EastAsia)]df19_Wo = df19[~df19['Q3'].isin(EastAsia)]df19['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df19['Q3']]df18_Ea = df18[df18['Q3'].isin(EastAsia)]df18_Wo = df18[~df18['Q3'].isin(EastAsia)]df18['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df18['Q3']]df17_Ea = df17[df17['Country'].isin(EastAsia)]df17_Wo = df17[~df17['Country'].isin(EastAsia)]df17['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df17['Country']] lng()을 이용하여 % 그래프 그리기1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# 수치 bar g = 사용자 수 비교.Ea21 = len(df21_Ea)Wo21 = len(df21) - len(df21_Ea)Ea20 = len(df20_Ea)Wo20 = len(df20) - len(df20_Ea)Ea19 = len(df19_Ea)Wo19 = len(df19) - len(df19_Ea)Ea18 = len(df18_Ea)Wo18 = len(df18) - len(df18_Ea)Ea17 = len(df17_Ea)Wo17 = len(df17) - len(df17_Ea)years = ['2017','2018','2019','2020', '2021']def percent (a, b): result =a/(a+b)*100 result = np.round(result) return resultdef percentR (b, a): result =a/(a+b)*100 result = np.round(result) return resultpercent = [percent(Ea17, Wo17), percent(Ea18, Wo18), percent(Ea19, Wo19), percent(Ea20, Wo20), percent(Ea21, Wo21)]# percentR = [percentR(Ea17, Wo17), percentR(Ea18, Wo18), percentR(Ea19, Wo19), # percentR(Ea20, Wo20), percentR(Ea21, Wo21)]fig = go.Figure()fig.add_trace(go.Bar(x=years, y=[len(df17), len(df18), len(df19), len(df20), len(df21)], base=[-len(df17), -len(df18), -len(df19), -len(df20), -len(df21)], marker_color='#88BFBA', name='World' ))fig.add_trace(go.Bar(x=years, y=[Ea17, Ea18, Ea19, Ea20, Ea21], base=0, marker_color='#D9946C', name='East Asia', text= percent, texttemplate='%{text} %', textposition='outside', hovertemplate='&lt;b&gt;KaggleUser&lt;/b&gt;: %{x}&lt;br&gt;'+ '&lt;b&gt;Count&lt;/b&gt;: %{y}', textfont_size=14 ))fig.show() 다음과 같은 G가 그려 진다. East asia와 world비교 _ kgg 사용자수123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687#data 정제하기total17 = ( df17['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index())total18 = ( df18['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index())total19 = ( df19['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index())total20 = ( df20['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index())total21 = ( df21['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index())colors = ['#88BFBA','#F28705']# Create subplots: use 'domain' type for Pie subplotfig = make_subplots(rows=1, cols=5, specs=[[{'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}]], subplot_titles=(&quot;2017&quot;, &quot;2018&quot;, &quot;2019&quot;, &quot;2020&quot;, &quot;2021&quot;))fig.add_trace(go.Pie(marker=dict(colors=colors),labels=total21['type'], values=total21['respodents'], name=&quot;2021&quot;, scalegroup='one'), 1, 1)fig.add_trace(go.Pie(marker=dict(colors=colors),labels=total20['type'], values=total20['respodents'], name=&quot;2020&quot;, scalegroup='one'), 1, 2)fig.add_trace(go.Pie(marker=dict(colors=colors),labels=total19['type'], values=total19['respodents'], name=&quot;2019&quot;, scalegroup='one'), 1, 3)fig.add_trace(go.Pie(marker=dict(colors=colors),labels=total18['type'], values=total18['respodents'], name=&quot;2018&quot;, scalegroup='one'), 1, 4)fig.add_trace(go.Pie(marker=dict(colors=colors),labels=total17['type'], values=total17['respodents'], name=&quot;2017&quot;, scalegroup='one'), 1, 5)# Use `hole` to create a donut-like pie chartfig.update_traces(hole=.0, hoverinfo=&quot;label+percent+name&quot;, textfont_size=15,)fig.update_layout(showlegend=False, margin=dict(pad=20), height=100, yaxis_title=None, xaxis_title=None, title_text=&quot;&lt;b&gt;World vs EastAsia&lt;/b&gt;&quot;, title_font_size=22, font=dict(size=17, color='#000000'), autosize=True)fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.show() subplot을 이용하여 원그래프 5개를 한꺼번에 묶어서 출력 할 수 있다. World map그리기1234567891011121314151617181920212223242526272829303132333435def world_map(locations,counts,title): data = [ dict( type = 'choropleth', locations = locations, z = counts, colorscale = 'Blues', locationmode = 'country names', autocolorscale = False, reversescale = True, marker = dict( line = dict(color = '#F7F7F7', width = 1.5)), colorbar = dict(autotick = True, legth = 3, len=0.75, title = 'respodents', max = 1000, min = 0) ) ] layout = dict( title = title, titlefont={'size': 28, 'family': 'san serif'}, width=750, height=475, paper_bgcolor='#F7F7F7', geo = dict( showframe = True, showcoastlines = True, fitbounds=&quot;locations&quot;, ) ) fig = dict(data=data, layout=layout) iplot(fig, validate=False, filename='world-map') z = df21_Ea['Q3'].value_counts() ## 메서드 호출world_map(locations=z.index, counts=z.values, title= '&lt;b&gt; EastAsia Countries (2021 survey) &lt;b&gt;') bar mode를 stack으로 하여 G를 그릴 수 있다.12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061years = ['2017', '2018', '2019', '2020', '2021']df21_Ea = df21[df21['Q3'].isin(EastAsia21)]Ea21= ( df21_Ea['Q3'].value_counts().to_frame() .reset_index().rename(columns={'index':'Country', 'Q3':'21'}))df20_Ea=df20[df20['Q3'].isin(EastAsia)]Ea20= ( df20_Ea['Q3'].replace('Republic of Korea','South Korea') .value_counts().to_frame().reset_index() .rename(columns={'index':'Country', 'Q3':'20'}))df19_Ea=df19[df19['Q3'].isin(EastAsia)]Ea19= (df19_Ea['Q3'].replace('Republic of Korea','South Korea') .value_counts().to_frame().reset_index() .rename(columns={'index':'Country', 'Q3':'19'}))df18_Ea=df18[df18['Q3'].isin(EastAsia)]Ea18= (df18_Ea['Q3'].replace('Republic of Korea','South Korea') .value_counts().to_frame().reset_index() .rename(columns={'index':'Country', 'Q3':'18'}))Ea18.value_counts()#df18 열에 taiwan = 0을 추가 해야 합니다. df17_Ea = df17[df17['Country'].isin(EastAsia)]Ea17= (df17_Ea['Country'].replace(&quot;People 's Republic of China&quot;,'China') .value_counts().to_frame().reset_index() .rename(columns={'index':'Country', 'Country':'17'}))#data를 합쳐서 하나의 dataframe으로 만들어 줌.df5years = pd.merge(Ea17, Ea18, on='Country', how='outer')df5year =pd.merge(Ea19,Ea20, on='Country', how='outer')df5year=pd.merge(df5year, Ea21, on='Country', how='outer')df5years = pd.merge(df5years, df5year, on='Country', how='outer')fig = go.Figure(data=[ go.Bar(name='2017', x=df5years['Country'], y=df5years['17']), go.Bar(name='2018', x=df5years['Country'], y=df5years['18']), go.Bar(name='2019', x=df5years['Country'], y=df5years['19']), go.Bar(name='2020', x=df5years['Country'], y=df5years['20']), go.Bar(name='2021', x=df5years['Country'], y=df5years['21'])])fig.update_layout(barmode='stack', showlegend=True, margin=dict(pad=20), height=500, yaxis_title=None, xaxis_title=None, title_text=&quot;&lt;b&gt;연도별 동아시아 Kaggle 사용자수&lt;/b&gt;&quot;, title_x=0.5, font=dict(size=17, color='#000000'), title_font_size=35)fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.show()# Text : percent 오늘은 여기까지 정리","link":"/2021/11/19/kgg/Kgg_compt/"},{"title":"kaggle in East-Asia(data둘러보기)","text":"Kaggle in East Asia 0. introduction1. World Vs East Asia Kgg 응답자 수 Kgg 응답자 중 직업1. 2. East Asia3. interestings Tenure: 17y 경력? Q6(20y) Q8(18y) Q15(19y) Q6(20, 21y) FormalEducation: 17y 학력, Q4 in 18y, 19y, 20y 전공 Q5 in18y compensation for year: Q10 in 18y,19y, Q24 in 20, 1234567891011121314151617181920212223242526272829# 연도별 나이 df21Age_Ea = df21_Ea.loc[:,['Q3','Q1']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'2021'}).fillna('etc')df20Age_Ea = df20_Ea.loc[:,['Q3','Q1']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'2020'}).fillna('etc')df19Age_Ea = df19_Ea.loc[:,['Q3','Q1']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'2019'}).fillna('etc')df18Age_Ea = df18_Ea.loc[:,['Q3','Q2']].reset_index().rename(columns={'Q3':'East_Asia', 'Q2':'2018'}).fillna('etc')df17Age_Ea = df17_Ea.loc[:,['Country','Age']].reset_index().rename(columns={'Country':'East_Asia', 'Age':'2017'}).fillna('etc')#data frame 정리dfAge21_percent =df21Age_Ea.groupby(['East_Asia','2021']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfAge21_percent['percent'] =((dfAge21_percent['Count'] / len(df21Age_Ea))*100).round(2)dfAge21_percent['percent_str'] =((dfAge21_percent['Count'] / len(df21Age_Ea))*100).round(2).astype(str) + '%'# dfAge21['percent'] = ((media['Count'] / len(df))*100).round(2).astype(str) + '%'# dfAge_percent21=dfAge21.value_counts('East_Asia',normalize=True).mul(100).round(1).astype(str)dfAge21_percent# 나라별 연령대 비율 in East Asiafig = go.Figure()for country, group in dfAge21_percent.groupby('East_Asia'): fig.add_trace(go.Bar( x = group['2021'], y = group['percent'], name = country, text=group['percent_str'] ))fig.update_layout(barmode=&quot;stack&quot;, plot_bgcolor = &quot;white&quot;, title='2021_ 연령별 나라 비율 in East Asia')fig.show() 123456789101112# 연령별 나라 비율 in East Asia // 여기 name 어떻게 넣는거야 !!!! fig = go.Figure()for country, group in dfAge21_percent.groupby('2021'): fig.add_trace(go.Bar( x = group['East_Asia'], y =group['percent'], text=dfAge21_percent['percent_str'] ))fig.update_layout(barmode=&quot;stack&quot;, plot_bgcolor = &quot;white&quot;, title='2021_ 나라별 연령 비율 in East Asia')fig.show() 못해먹겠다. Metaplotly","link":"/2021/11/15/kgg/Kgg_in_EastAsia/"},{"title":"kaggle HeatMap","text":"Kaggle 을 쓰는 East Asia의 사람들 Kaggle 정의 Kaggle 설문조사_개요 그런데 우리는 EA에살아서 궁긍한 걸 찾아보자. data import1234567891011121314151617181920212223import numpy as npimport pandas as pdimport seaborn as snsimport matplotlib.pylab as pltimport plotly.io as pioimport plotly.express as pximport plotly.graph_objects as goimport plotly.figure_factory as fffrom plotly.subplots import make_subplotsfrom plotly.offline import init_notebook_mode, iplotinit_notebook_mode(connected=True)pio.templates.default = &quot;none&quot;# import plotly.offline as py# py.offline.init_notebook_mode()import osfor dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename))import warningswarnings.filterwarnings(&quot;ignore&quot;) /kaggle/input/kaggle-survey-2018/SurveySchema.csv /kaggle/input/kaggle-survey-2018/freeFormResponses.csv /kaggle/input/kaggle-survey-2018/multipleChoiceResponses.csv /kaggle/input/kaggle-survey-2020/kaggle_survey_2020_responses.csv /kaggle/input/kaggle-survey-2020/supplementary_data/kaggle_survey_2020_methodology.pdf /kaggle/input/kaggle-survey-2020/supplementary_data/kaggle_survey_2020_answer_choices.pdf /kaggle/input/kaggle-survey-2021/kaggle_survey_2021_responses.csv /kaggle/input/kaggle-survey-2021/supplementary_data/kaggle_survey_2021_methodology.pdf /kaggle/input/kaggle-survey-2021/supplementary_data/kaggle_survey_2021_answer_choices.pdf /kaggle/input/kaggle-survey-2019/survey_schema.csv /kaggle/input/kaggle-survey-2019/multiple_choice_responses.csv /kaggle/input/kaggle-survey-2019/other_text_responses.csv /kaggle/input/kaggle-survey-2019/questions_only.csv /kaggle/input/kaggle-survey-2017/freeformResponses.csv /kaggle/input/kaggle-survey-2017/schema.csv /kaggle/input/kaggle-survey-2017/RespondentTypeREADME.txt /kaggle/input/kaggle-survey-2017/multipleChoiceResponses.csv /kaggle/input/kaggle-survey-2017/conversionRates.csv 123456df17= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2017/multipleChoiceResponses.csv&quot;, encoding=&quot;ISO-8859-1&quot;)df18= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2018/multipleChoiceResponses.csv&quot;, )df19= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2019/multiple_choice_responses.csv&quot;, )df20= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2020/kaggle_survey_2020_responses.csv&quot;, )df21= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2021/kaggle_survey_2021_responses.csv&quot;, ) EastAsia VS World1. data 전처리1.1 EastAsia / World 나누기123456789101112131415161718192021222324252627282930313233343536373839404142434445#질문 제거하기, replacedf17= df17.iloc[1:, :].replace(&quot;People 's Republic of China&quot;,'China')df18= df18.iloc[1:, :].replace('Republic of Korea','South Korea')df19= df19.iloc[1:, :].replace('Republic of Korea','South Korea')df20= df20.iloc[1:, :].replace('Republic of Korea','South Korea')df21= df21.iloc[1:, :]## East Asia에는 대한민국, 일본, 중국, 타이완, 몽골, 북조선 총 6개의 국가가 속해 있다. ## 알 수 없지만, 18년도엔 타이완이 없다. EastAsia17 = ['China',&quot;People 's Republic of China&quot;, 'Taiwan', 'South Korea', 'Japan']EastAsia18= ['China', 'South Korea', 'Japan', 'Republic of Korea'] EastAsia19 = ['China','Taiwan', 'South Korea', 'Japan', 'Republic of Korea']EastAsia20 = ['China','Taiwan', 'South Korea','Republic of Korea', 'Japan']EastAsia21 = ['China','Taiwan', 'South Korea', 'Japan']EastAsia = ['Republic of Korea','China','Taiwan', 'South Korea', 'Japan', &quot;People 's Republic of China&quot; ]years = ['2017', '2018', '2019', '2020', '2021']#East Asia 뽑기df21_Ea = df21[df21['Q3'].isin(EastAsia)]df21_Wo = df21[~df21['Q3'].isin(EastAsia)]df21['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df21['Q3']]df20_Ea = df20[df20['Q3'].isin(EastAsia)]df20_Wo = df20[~df20['Q3'].isin(EastAsia)]df20['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df20['Q3']]df19_Ea = df19[df19['Q3'].isin(EastAsia)]df19_Wo = df19[~df19['Q3'].isin(EastAsia)]df19['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df19['Q3']]df18_Ea = df18[df18['Q3'].isin(EastAsia)]df18_Wo = df18[~df18['Q3'].isin(EastAsia)]df18['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df18['Q3']]df17_Ea = df17[df17['Country'].isin(EastAsia)]df17_Wo = df17[~df17['Country'].isin(EastAsia)]df17['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df17['Country']]#df21['region'].to_frame().value_counts().to_frame().rename(columns={'region': '21y', '' : 'count'})print('OK') OK 123456789101112131415161718192021222324# 나라별 data 뽑기df21_Ch= df21_Ea[df21_Ea['Q3'] == 'China']df21_Tw= df21_Ea[df21_Ea['Q3'] == 'South Korea']df21_Ko= df21_Ea[df21_Ea['Q3'] == 'Taiwan']df21_Jp= df21_Ea[df21_Ea['Q3'] == 'Japan']df20_Ch= df20_Ea[df20_Ea['Q3'] == 'China']df20_Tw= df20_Ea[df20_Ea['Q3'] == 'South Korea']df20_Ko= df20_Ea[df20_Ea['Q3'] == 'Taiwan']df20_Jp= df20_Ea[df20_Ea['Q3'] == 'Japan']df19_Ch= df19_Ea[df19_Ea['Q3'] == 'China']df19_Tw= df19_Ea[df19_Ea['Q3'] == 'South Korea']df19_Ko= df19_Ea[df19_Ea['Q3'] == 'Taiwan']df19_Jp= df19_Ea[df19_Ea['Q3'] == 'Japan']df18_Ch= df18_Ea[df18_Ea['Q3'] == 'China']df18_Tw= df18_Ea[df18_Ea['Q3'] == 'South Korea']df18_Jp= df18_Ea[df18_Ea['Q3'] == 'Japan']df17_Ch= df17_Ea[df17_Ea['Country'] == 'China']df17_Tw= df17_Ea[df17_Ea['Country'] == 'South Korea']df17_Ko= df17_Ea[df17_Ea['Country'] == 'Taiwan']df17_Jp= df17_Ea[df17_Ea['Country'] == 'Japan'] 2. Kaggle 사용자수 (W/Ea)2.1 data 전처리2.2 그래프 그리기1234567891011121314151617181920212223242526272829303132333435def world_map(locations,counts,title): data = [ dict( type = 'choropleth', locations = locations, z = counts, colorscale = 'Blues', locationmode = 'country names', autocolorscale = False, reversescale = True, marker = dict( line = dict(color = '#F7F7F7', width = 1.5)), colorbar = dict(autotick = True, legth = 3, len=0.75, title = 'respodents', max = 1000, min = 0) ) ] layout = dict( title = title, titlefont={'size': 28, 'family': 'san serif'}, width=750, height=475, paper_bgcolor='#F7F7F7', geo = dict( showframe = True, showcoastlines = True, fitbounds=&quot;locations&quot;, ) ) fig = dict(data=data, layout=layout) iplot(fig, validate=False, filename='world-map') z = df21_Ea['Q3'].value_counts() ## 메서드 호출world_map(locations=z.index, counts=z.values, title= '&lt;b&gt; EastAsia Countries (2021 survey) &lt;b&gt;') 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# 수치 bar g = 사용자 수 비교.Ea21 = len(df21_Ea)Wo21 = len(df21) - len(df21_Ea)Ea20 = len(df20_Ea)Wo20 = len(df20) - len(df20_Ea)Ea19 = len(df19_Ea)Wo19 = len(df19) - len(df19_Ea)Ea18 = len(df18_Ea)Wo18 = len(df18) - len(df18_Ea)Ea17 = len(df17_Ea)Wo17 = len(df17) - len(df17_Ea)years = ['2017','2018','2019','2020', '2021']def percent (a, b): result =a/(a+b)*100 result = np.round(result) return resultdef percentR (b, a): result =a/(a+b)*100 result = np.round(result) return resultpercent = [percent(Ea17, Wo17), percent(Ea18, Wo18), percent(Ea19, Wo19), percent(Ea20, Wo20), percent(Ea21, Wo21)]# percentR = [percentR(Ea17, Wo17), percentR(Ea18, Wo18), percentR(Ea19, Wo19), # percentR(Ea20, Wo20), percentR(Ea21, Wo21)]fig = go.Figure()fig.add_trace(go.Bar(x=years, y=[len(df17), len(df18), len(df19), len(df20), len(df21)], base=[-len(df17), -len(df18), -len(df19), -len(df20), -len(df21)], marker_color='lightslategrey', name='World', textposition='outside', hovertemplate='&lt;b&gt;KaggleUser&lt;/b&gt;: %{x}&lt;br&gt;'+ '&lt;b&gt;Count&lt;/b&gt;: %{y}' ))fig.add_trace(go.Bar(x=years, y=[Ea17, Ea18, Ea19, Ea20, Ea21], base=0, marker_color='pink', name='East Asia', text= percent, texttemplate='%{text} %', textposition='outside', hovertemplate='&lt;b&gt;KaggleUser&lt;/b&gt;: %{x}&lt;br&gt;'+ '&lt;b&gt;Count&lt;/b&gt;: %{y}', textfont_size=12 ))fig.update_layout(width=600, height=700)fig.show() 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677from plotly.subplots import make_subplotsimport plotly.graph_objects as gototal17 = ( df17['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index())total18 = ( df18['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index())total19 = ( df19['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index())total20 = ( df20['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index())total21 = ( df21['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index())# Create subplots: use 'domain' type for Pie subplotfig = make_subplots(rows=1, cols=5, specs=[[{'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}]], subplot_titles=(&quot;2017&quot;, &quot;2018&quot;, &quot;2019&quot;, &quot;2020&quot;, &quot;2021&quot;))fig.add_trace(go.Pie(labels=total21['type'], values=total21['respodents'], name=&quot;2021&quot;, scalegroup='one'), 1, 1)fig.add_trace(go.Pie(labels=total20['type'], values=total20['respodents'], name=&quot;2020&quot;, scalegroup='one'), 1, 2)fig.add_trace(go.Pie(labels=total19['type'], values=total19['respodents'], name=&quot;2019&quot;, scalegroup='one'), 1, 3)fig.add_trace(go.Pie(labels=total18['type'], values=total18['respodents'], name=&quot;2018&quot;, scalegroup='one'), 1, 4)fig.add_trace(go.Pie(labels=total17['type'], values=total17['respodents'], name=&quot;2017&quot;, scalegroup='one'), 1, 5)# Use `hole` to create a donut-like pie chartfig.update_traces(hole=.2, hoverinfo=&quot;label+percent+name&quot;)fig.update_layout( title_text=&quot;&lt;b&gt;World vs EastAsia&lt;/b&gt;&quot;, # Add annotations in the center of the donut pies. )fig.show() 가설은 시간에따라서 응답자수가 증가 할 줄 알았는데 결과를 보니 오히려 감소하는 경향을 볼 수 있다. East Asia 응답자는 2020년도(10.5%)에 가장 많았고 2121년도(7.15%)로 가장 적엇다. 0. Kaggle Gender (in Ea)0.1 data 전처리0.2 그래프 그리기123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263df21_Ea=df21[df21['Q3'].isin(EastAsia21)]Ea21= ( df21_Ea['Q3'].value_counts().to_frame() .reset_index().rename(columns={'index':'Country', 'Q3':'21'}))df20_Ea=df20[df20['Q3'].isin(EastAsia)]Ea20= ( df20_Ea['Q3'].replace('Republic of Korea','South Korea') .value_counts().to_frame().reset_index() .rename(columns={'index':'Country', 'Q3':'20'}))df19_Ea=df19[df19['Q3'].isin(EastAsia)]Ea19= (df19_Ea['Q3'].replace('Republic of Korea','South Korea') .value_counts().to_frame().reset_index() .rename(columns={'index':'Country', 'Q3':'19'}))df18_Ea=df18[df18['Q3'].isin(EastAsia)]Ea18= (df18_Ea['Q3'].replace('Republic of Korea','South Korea') .value_counts().to_frame().reset_index() .rename(columns={'index':'Country', 'Q3':'18'}))Ea18.value_counts()#df18 열에 taiwan = 0을 추가 해야 합니다. df17_Ea = df17[df17['Country'].isin(EastAsia)]Ea17= (df17_Ea['Country'].replace(&quot;People 's Republic of China&quot;,'China') .value_counts().to_frame().reset_index() .rename(columns={'index':'Country', 'Country':'17'}))#data를 합쳐서 하나의 dataframe으로 만들어 줌.df5years = pd.merge(Ea17, Ea18, on='Country', how='outer')df5year =pd.merge(Ea19,Ea20, on='Country', how='outer')df5year=pd.merge(df5year, Ea21, on='Country', how='outer')df5years = pd.merge(df5years, df5year, on='Country', how='outer')fig = go.Figure(data=[ go.Bar(name='2017', x=df5years['Country'], y=df5years['17']), go.Bar(name='2018', x=df5years['Country'], y=df5years['18']), go.Bar(name='2019', x=df5years['Country'], y=df5years['19']), go.Bar(name='2020', x=df5years['Country'], y=df5years['20']), go.Bar(name='2021', x=df5years['Country'], y=df5years['21'])])#Change the bar modefig.update_layout(barmode='stack', title='연도별 동아시아 Kaggle 사용자수' )fig.show()# Text : percent 5년간 china의 Kaggle 응답자 수가 가장 많았다. taiwan이 가장 적지만, 2018년도에 어떤이유인지 모르겠지만, china로 결과 값이 흡수 된 것 같다. south korea나 Taiwan의 응답자수는 china의 절반에 미치지 못한다. 인구수 대비 몇 % 가 많다. 3. Kaggle Gender (W/Ea)3.1 data 전처리3.2 그래프 그리기12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485Gender_17 = ( df17['GenderSelect'] .replace(['A different identity', 'Prefer to self-describe', 'Non-binary, genderqueer, or gender non-conforming'], 'Others') .fillna('Others') .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'GenderSelect':'Gender'}) .groupby('type') .sum() .reset_index())Gender_18 = ( df18['Q1'] .replace(['Prefer not to say', 'Prefer to self-describe'], 'Others') .fillna('Others') .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'Q1':'Gender'}) .groupby('type') .sum() .reset_index())Gender_19 = ( df19['Q2'] .replace(['Prefer not to say','Prefer to self-describe'],'Others') .fillna('Others') .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'Q2':'Gender'}) .groupby('type') .sum() .reset_index())Gender_20 = ( df20['Q2'] .replace(['Prefer not to say', 'Prefer to self-describe', 'Nonbinary'], 'Others') .replace(['Man', 'Woman'], ['Male', 'Female']) .fillna('Others') .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'Q2':'Gender'}) .groupby('type') .sum() .reset_index())Gender_21 = ( df21['Q2'] .replace(['Prefer not to say', 'Prefer to self-describe', 'Nonbinary'], 'Others') .replace(['Man', 'Woman'], ['Male', 'Female']) .fillna('Others') .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'Q2':'Gender'}) .groupby('type') .sum() .reset_index())# Create subplots: use 'domain' type for Pie subplotfig = make_subplots(rows=1, cols=5, specs=[[{'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}]], subplot_titles=(&quot;2017&quot;, &quot;2018&quot;, &quot;2019&quot;, &quot;2020&quot;, &quot;2021&quot;))fig.add_trace(go.Pie(labels=Gender_21['type'], values=Gender_21['Gender'], name=&quot;2021&quot;, scalegroup='one'), 1, 5)fig.add_trace(go.Pie(labels=Gender_20['type'], values=Gender_20['Gender'], name=&quot;2020&quot;, scalegroup='one'), 1, 4)fig.add_trace(go.Pie(labels=Gender_19['type'], values=Gender_19['Gender'], name=&quot;2019&quot;, scalegroup='one'), 1, 3)fig.add_trace(go.Pie(labels=Gender_18['type'], values=Gender_18['Gender'], name=&quot;2018&quot;, scalegroup='one'), 1, 2)fig.add_trace(go.Pie(labels=Gender_17['type'], values=Gender_17['Gender'], name=&quot;2017&quot;, scalegroup='one'), 1, 1)# Use `hole` to create a donut-like pie chartfig.update_traces(hole=.2, hoverinfo=&quot;label+percent+name&quot;)fig.update_layout( title_text=&quot;&lt;b&gt;World_Gender&lt;/b&gt;&quot;, # Add annotations in the center of the donut pies. )fig.show() 연도별, 지역별로 보았을때, 여성의 비율이 20% 미만으로 적은 것을 알 수 있다. 2019년 이전보다 2020년 이후가 증가 했다. (16%-&gt; 19%) 0. Kaggle job (W/Ea)0.1 data 전처리0.2 그래프 그리기123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293#data 확인Data_Analyst =['Data Analyst','Data Miner,Information technology','Data Miner', 'Predictive Modeler','Information technology, networking, or system administration', 'A business discipline (accounting, economics, finance, etc.)', 'Business Analyst', 'Humanities', 'Statistician', 'Mathematics or statistics', 'Medical or life sciences (biology, chemistry, medicine, etc.)', 'Physics or astronomy', 'Research Scientist', 'Researcher', 'Social sciences (anthropology, psychology, sociology, etc.)', 'Humanities (history, literature, philosophy, etc.)']Data_Scientist =['Data Scientist', 'Environmental science or geology', 'Machine Learning Engineer', 'Scientist/Researcher']Developer=['Developer Relations/Advocacy','Data Engineer','Engineer','Engineering (non-computer focused)', 'Programmer','Software Engineer', 'Computer Scientist','Computer science (software engineering, etc.)', 'Fine arts or performing arts','Product Manager', 'Software Developer/Software Engineer', 'Product/Project Manager','Program/Project Manager','DBA/Database Engineer']Not_Employeed =['Currently not employed', 'Not employed', 'Student']Others = ['I never declared a major', 'Other']#연도별로 뽑은 나라별 직업.df21job_Ea = df21_Ea.loc[:,['Q3','Q5']].reset_index().rename(columns={'index':'job', 'Q5':'2021'}).fillna('Other')df20job_Ea = df20_Ea.loc[:,['Q3','Q5']].reset_index().rename(columns={'index':'job', 'Q5':'2020'}).fillna('Other')df19job_Ea = df19_Ea.loc[:,['Q3','Q5']].reset_index().rename(columns={'index':'job', 'Q5':'2019'}).fillna('Other')df18job_Ea = df18_Ea.loc[:,['Q3','Q5']].reset_index().rename(columns={'index':'job', 'Q5':'2018'}).fillna('Other')df17job_Ea = df17_Ea.loc[:,['Country','CurrentJobTitleSelect']].reset_index().rename(columns={'index':'job', 'CurrentJobTitleSelect':'2017'}).fillna('Other')# 연도별 job Grouping in east asiadf21job_Ea.value_counts('2021')df21job_Ea['JOB']=[&quot;Data Analyst&quot; if x in Data_Analyst else &quot;Data Scientist&quot; if x in Data_Scientist # Data Scientist else &quot;Data Engineer&quot; if x in Developer else &quot;NotEmployeed&quot; if x in Not_Employeed else &quot;Others&quot; for x in df21job_Ea['2021']]df21job_Ea.value_counts('JOB')df20job_Ea.value_counts('2020')df20job_Ea['JOB']=[&quot;Data Analyst&quot; if x in Data_Analyst else &quot;Data Scientist&quot; if x in Data_Scientist else &quot;Data Engineer&quot; if x in Developer else &quot;NotEmployeed&quot; if x in Not_Employeed else &quot;Other&quot; for x in df20job_Ea['2020']]df20job_Ea[['2020','JOB']]df19job_Ea.value_counts('2019')df19job_Ea['JOB']=[&quot;Data Analyst&quot; if x in Data_Analyst else &quot;Data Scientist&quot; if x in Data_Scientist else &quot;Data Engineer&quot; if x in Developer else &quot;NotEmployeed&quot; if x in Not_Employeed else &quot;Other&quot; for x in df19job_Ea['2019']]#2019 data에 &quot;Other&quot; grouping이 제대로 이루어 졌는지 확인 df19jobTest = df19job_Ea.loc[df19job_Ea.JOB == 'Other']df19jobTest['2019'].value_counts()df18job_Ea.value_counts('2018')df18job_Ea['JOB']=[&quot;Data Analyst&quot; if x in Data_Analyst else &quot;Data Scientist&quot; if x in Data_Scientist else &quot;Data Engineer&quot; if x in Developer else &quot;NotEmployeed&quot; if x in Not_Employeed else &quot;Other&quot; for x in df18job_Ea['2018']]df18jobTest = df18job_Ea.loc[df18job_Ea.JOB == 'Other']df18jobTest['2018'].value_counts()df17job_Ea.value_counts('2017')df17job_Ea['JOB']=[&quot;Data Analyst&quot; if x in Data_Analyst else &quot;Data Scientist&quot; if x in Data_Scientist else &quot;Data Engineer&quot; if x in Developer else &quot;NotEmployeed&quot; if x in Not_Employeed else &quot;Other&quot; for x in df17job_Ea['2017']]df17jobTest = df17job_Ea.loc[df17job_Ea.JOB == 'Other']df17jobTest['2017'].value_counts()df21jobTest = df21job_Ea.loc[df21job_Ea.JOB == 'Other']df21jobTest['2021'].head()df21job_Ea.value_counts('JOB')#data frame 정리dfjob21 =df21job_Ea.groupby(['Q3','JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;}).rename(columns={'Q3':'country', 'JOB':'2021'})dfjob20 =df20job_Ea.groupby(['Q3','JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;}).rename(columns={'Q3':'country', 'JOB':'2020'})dfjob19 =df19job_Ea.groupby(['Q3','JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;}).rename(columns={'Q3':'country', 'JOB':'2019'})dfjob18 =df18job_Ea.groupby(['Q3','JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;}).rename(columns={'Q3':'country', 'JOB':'2018'})dfjob17 =df17job_Ea.groupby(['Country','JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;}).rename(columns={'Country':'country', 'JOB':'2017'}) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475#21년도 Bar graph 그리기fig = go.Figure()for country, group in dfjob21.groupby('country'): fig.add_trace(go.Bar( x = group['2021'], y = group['Count'], name = country ))fig.update_layout(barmode=&quot;group&quot;, plot_bgcolor = &quot;white&quot;, title='2021_나라별 직업 수', width=700, height=450 )fig.show()#20년도 Bar graph 그리기fig = go.Figure()for country, group in dfjob20.groupby('country'): fig.add_trace(go.Bar( x = group['2020'], y = group['Count'], name = country ))fig.update_layout(barmode=&quot;group&quot;, plot_bgcolor = &quot;white&quot;, title='2020_나라별 직업 수', width=700, height=450)fig.show()#19년도 Bar graph 그리기fig = go.Figure()for country, group in dfjob19.groupby('country'): fig.add_trace(go.Bar( x = group['2019'], y = group['Count'], name = country ))fig.update_layout(barmode=&quot;group&quot;, plot_bgcolor = &quot;white&quot;, title='2019_나라별 직업 수', width=700, height=450 )fig.show()#18년도 Bar graph 그리기fig = go.Figure()for country, group in dfjob18.groupby('country'): fig.add_trace(go.Bar( x = group['2018'], y = group['Count'], name = country ))fig.update_layout(barmode=&quot;group&quot;, plot_bgcolor = &quot;white&quot;, title='2018_나라별 직업 수', width=700, height=450 )fig.show()#17년도 Bar graph 그리기fig = go.Figure()for country, group in dfjob17.groupby('country'): fig.add_trace(go.Bar( x = group['2017'], y = group['Count'], name = country ))fig.update_layout(barmode=&quot;group&quot;, plot_bgcolor = &quot;white&quot;, title='2017_나라별 직업 수', width=700, height=450 )fig.show() 0. Kaggle age &amp; Edu (W/Ea)0.1 data 전처리0.2 그래프 그리기123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990# 연도별 나이 df21Age_Ea = df21_Ea.loc[:,['Q3','Q1']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'2021'}).fillna('etc')df20Age_Ea = df20_Ea.loc[:,['Q3','Q1']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'2020'}).fillna('etc')df19Age_Ea = df19_Ea.loc[:,['Q3','Q1']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'2019'}).fillna('etc')df18Age_Ea = df18_Ea.loc[:,['Q3','Q2']].reset_index().rename(columns={'Q3':'East_Asia', 'Q2':'2018'}).fillna('etc')df17Age_Ea = df17_Ea.loc[:,['Country','Age']].reset_index().rename(columns={'Country':'East_Asia', 'Age':'2017'}).fillna('etc')#data frame 정리dfAge21 =df21Age_Ea.groupby(['East_Asia','2021']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfAge20 =df20Age_Ea.groupby(['East_Asia','2020']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfAge19 =df19Age_Ea.groupby(['East_Asia','2019']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfAge18 =df18Age_Ea.groupby(['East_Asia','2018']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfAge17 =(df17Age_Ea.groupby(['East_Asia','2017']) .size().reset_index().rename(columns = {0:&quot;Count&quot;}))#2017data# array([16.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0,# 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0,# 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 47.0, 50.0, 54.0, 100.0,# 'etc', 46.0, 48.0, 49.0, 51.0, 52.0, 53.0, 55.0, 57.0, 58.0, 59.0,# 62.0, 64.0, 65.0, 67.0, 68.0, 70.0, 17.0, 56.0, 60.0], dtype=object)#21년도 Bar graph 그리기fig = go.Figure()for country, group in dfAge21.groupby('East_Asia'): fig.add_trace(go.Bar( x = group['2021'], y = group['Count'], name = country ))fig.update_layout(barmode=&quot;group&quot;, plot_bgcolor = &quot;white&quot;, title='2021_나라별 연령')fig.show()#20년도 Bar graph 그리기fig = go.Figure()for country, group in dfAge20.groupby('East_Asia'): fig.add_trace(go.Bar( x = group['2020'], y = group['Count'], name = country ))fig.update_layout(barmode=&quot;group&quot;, plot_bgcolor = &quot;white&quot;, title='2020_나라별 연령')fig.show()#19년도 Bar graph 그리기fig = go.Figure()for country, group in dfAge19.groupby('East_Asia'): fig.add_trace(go.Bar( x = group['2019'], y = group['Count'], name = country ))fig.update_layout(barmode=&quot;group&quot;, plot_bgcolor = &quot;white&quot;, title='2019_나라별 연령')fig.show()#18년도 Bar graph 그리기fig = go.Figure()for country, group in dfAge18.groupby('East_Asia'): fig.add_trace(go.Bar( x = group['2018'], y = group['Count'], name = country ))fig.update_layout(barmode=&quot;group&quot;, plot_bgcolor = &quot;white&quot;, title='2018_나라별 연령')fig.show()#17년도 Bar graph 그리기_ Scatter 로 변경fig = go.Figure()for country, group in dfAge17.groupby('East_Asia'): fig.add_trace(go.Bar( x = group['2017'], y = group['Count'], name = country ))fig.update_layout(barmode=&quot;group&quot;, plot_bgcolor = &quot;white&quot;, title='2017_나라별 연령')fig.show() 0.2 Kaggle age (Ea)0.1.1 data 전처리0.2.1 그래프 그리기123456789101112131415161718192021222324252627282930313233#data frame 정리dfAge21_percent =df21Age_Ea.groupby(['East_Asia','2021']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfAge21_percent['percent'] =((dfAge21_percent['Count'] / len(df21Age_Ea))*100).round(2)dfAge21_percent['percent_str'] =((dfAge21_percent['Count'] / len(df21Age_Ea))*100).round(2).astype(str) + '%'# dfAge21['percent'] = ((media['Count'] / len(df))*100).round(2).astype(str) + '%'# dfAge_percent21=dfAge21.value_counts('East_Asia',normalize=True).mul(100).round(1).astype(str)dfAge21_percent# 나라별 연령대 비율 in East Asiafig = go.Figure()for country, group in dfAge21_percent.groupby('East_Asia'): fig.add_trace(go.Bar( x = group['2021'], y = group['percent'], name = country, text=group['percent_str'] ))fig.update_layout(barmode=&quot;stack&quot;, plot_bgcolor = &quot;white&quot;, title='2021_ 연령별 나라 비율 in East Asia')fig.show()# 연령별 나라 비율 in East Asia // 다중 pie 그래프로 바꾸기 fig = go.Figure()for country, group in dfAge21_percent.groupby('2021'): fig.add_trace(go.Bar( x = group['East_Asia'], y =group['percent'], text=dfAge21_percent['percent_str'] ))fig.update_layout(barmode=&quot;stack&quot;, plot_bgcolor = &quot;white&quot;, title='2021_ 나라별 연령 비율 in East Asia')fig.show() 연도별 연령1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 나라별로 연도가 나왓으면 좋겠어요 .z = df21Age_Ea.groupby(['East_Asia', '2021']).size().unstack().fillna(0).astype('int64')z_data = z.apply(lambda x:np.round(x/x.sum(), 2), axis = 1).to_numpy() # convert to correlation matrixx = z.columns.tolist()y = z.index.tolist()fig21 = ff.create_annotated_heatmap(z_data, x = x, y = y, colorscale = &quot;sunset&quot;)z = df20Age_Ea.groupby(['East_Asia', '2020']).size().unstack().fillna(0).astype('int64')z_data = z.apply(lambda x:np.round(x/x.sum(), 2), axis = 1).to_numpy() # convert to correlation matrixx = z.columns.tolist()y = z.index.tolist()fig20 = ff.create_annotated_heatmap(z_data, x = x, y = y, colorscale = &quot;sunset&quot;)z = df19Age_Ea.groupby(['East_Asia', '2019']).size().unstack().fillna(0).astype('int64')z_data = z.apply(lambda x:np.round(x/x.sum(), 2), axis = 1).to_numpy() # convert to correlation matrixx = z.columns.tolist()y = z.index.tolist()fig19 = ff.create_annotated_heatmap(z_data, x = x, y = y, colorscale = &quot;sunset&quot;)z = df18Age_Ea.groupby(['East_Asia', '2018']).size().unstack().fillna(0).astype('int64')z_data = z.apply(lambda x:np.round(x/x.sum(), 2), axis = 1).to_numpy() # convert to correlation matrixx = z.columns.tolist()y = z.index.tolist()fig18 = ff.create_annotated_heatmap(z_data, x = x, y = y, colorscale = &quot;sunset&quot;)z = df17Age_Ea.groupby(['East_Asia', '2017']).size().unstack().fillna(0).astype('int64')z_data = z.apply(lambda x:np.round(x/x.sum(), 2), axis = 1).to_numpy() # convert to correlation matrixx = z.columns.tolist()y = z.index.tolist()fig17 = ff.create_annotated_heatmap(z_data, x = x, y = y, colorscale = &quot;sunset&quot;)fig21.show()fig20.show()fig19.show()fig18.show() 연령별 지역123456789101112131415161718192021222324252627282930313233343536373839404142# 연령-지역 %dfKo_Age21= df21Age_Ea[df21Age_Ea['East_Asia']=='South Korea']dfKo_Age21_per=dfKo_Age21['2021'].value_counts().to_frame().reset_index()dfKo_Age21_per['South Korea']=((dfKo_Age21_per['2021'] / len(dfKo_Age21))*100).round(2)dfTw_Age21= df21Age_Ea[df21Age_Ea['East_Asia']=='Taiwan']dfTw_Age21_per=dfTw_Age21['2021'].value_counts().to_frame().reset_index()dfTw_Age21_per['Taiwan']=((dfTw_Age21_per['2021'] / len(dfTw_Age21))*100).round(2)dfTw_Age21_perdfCh_Age21= df21Age_Ea[df21Age_Ea['East_Asia']=='China']dfCh_Age21_per=dfCh_Age21['2021'].value_counts().to_frame().reset_index()dfCh_Age21_per['China']=((dfCh_Age21_per['2021'] / len(dfCh_Age21))*100).round(2)dfCh_Age21_perdf21Age_Ea.head()dfJp_Age21= df21Age_Ea[df21Age_Ea['East_Asia']=='Japan']dfJp_Age21_per=dfJp_Age21['2021'].value_counts().to_frame().reset_index()dfJp_Age21_per['Japan']=((dfJp_Age21_per['2021'] / len(dfJp_Age21))*100).round(2)dfJp_Age21_per#g 그리기(heatMap)merge1= pd.merge(dfKo_Age21_per,dfTw_Age21_per, on='index', how='outer')merge2= pd.merge(dfCh_Age21_per,dfJp_Age21_per, on='index', how='outer')merge= pd.merge(merge1,merge2, on='index', how='outer').fillna(0).sort_values(by=['index'],ascending=True)merge.iloc[:,[2,4,6,8]]merge.iloc[:,[2,4,6,8]].to_numpy()fig = go.Figure(data=go.Heatmap( z=merge.iloc[:,[2,4,6,8]].to_numpy(), x=['South Korea','Taiwan','China','Japan'], y=merge.sort_values(by=['index'],ascending=True)['index'].tolist(), hoverongaps = False, opacity=1.0, xgap=2.5, ygap=2.5, colorscale='orrd'), )fig.show() 0.2 Kaggle Edu (Ea)0.1.1 data 전처리0.2.1 그래프 그리기1234567891011121314151617181920212223242526272829303132333435363738394041424344# 연도별 학력df21Edu_Ea = df21_Ea.loc[:,['Q3','Q4']].reset_index().rename(columns={'Q3':'East_Asia', 'Q4':'2021'}).fillna('etc')df20Edu_Ea = df20_Ea.loc[:,['Q3','Q4']].reset_index().rename(columns={'Q3':'East_Asia', 'Q4':'2020'}).fillna('etc')df19Edu_Ea = df19_Ea.loc[:,['Q3','Q4']].reset_index().rename(columns={'Q3':'East_Asia', 'Q4':'2019'}).fillna('etc')df18Edu_Ea = df18_Ea.loc[:,['Q3','Q4']].reset_index().rename(columns={'Q3':'East_Asia', 'Q4':'2018'}).fillna('etc')df17Edu_Ea = df17_Ea.loc[:,['Country','FormalEducation']].reset_index().rename(columns={'Country':'East_Asia', 'FormalEducation':'2017'}).fillna('etc')df21Edu_Ea =df21Edu_Ea.replace({'I prefer not to answer':'etc'}).sort_values(by='2021', ascending=False)df20Edu_Ea =df20Edu_Ea.replace({'I prefer not to answer':'etc'}).sort_values(by='2020', ascending=False)df19Edu_Ea =df19Edu_Ea.replace({'I prefer not to answer':'etc'}).sort_values(by='2019', ascending=False)df18Edu_Ea =df18Edu_Ea.replace({'I prefer not to answer':'etc'}).sort_values(by='2018', ascending=False)df17Edu_Ea =df17Edu_Ea.replace({'I prefer not to answer':'etc'}).sort_values(by='2017', ascending=False)#data frame 정리dfEdu21 =df21Edu_Ea.groupby(['East_Asia','2021']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfEdu20 =df20Edu_Ea.groupby(['East_Asia','2020']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfEdu19 =df19Edu_Ea.groupby(['East_Asia','2019']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfEdu18 =df18Edu_Ea.groupby(['East_Asia','2018']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfEdu17 =(df17Edu_Ea.groupby(['East_Asia','2017']) .size().reset_index().rename(columns = {0:&quot;Count&quot;}))# 비율 data 추가##21dfEdu21_percent =df21Edu_Ea.groupby(['East_Asia','2021']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfEdu21_percent['percent'] =((dfEdu21_percent['Count'] / len(df21Edu_Ea))*100).round(2)dfEdu21_percent['percent_str'] =((dfEdu21_percent['Count'] / len(df21Edu_Ea))*100).round(2).astype(str) + '%'##20dfEdu20_percent =df20Edu_Ea.groupby(['East_Asia','2020']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfEdu20_percent['percent'] =((dfEdu20_percent['Count'] / len(df20Edu_Ea))*100).round(2)dfEdu20_percent['percent_str'] =((dfEdu20_percent['Count'] / len(df20Edu_Ea))*100).round(2).astype(str) + '%'##19dfEdu19_percent =df19Edu_Ea.groupby(['East_Asia','2019']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfEdu19_percent['percent'] =((dfEdu19_percent['Count'] / len(df19Edu_Ea))*100).round(2)dfEdu19_percent['percent_str'] =((dfEdu19_percent['Count'] / len(df19Edu_Ea))*100).round(2).astype(str) + '%'##18dfEdu18_percent =df18Edu_Ea.groupby(['East_Asia','2018']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfEdu18_percent['percent'] =((dfEdu18_percent['Count'] / len(df18Edu_Ea))*100).round(2)dfEdu18_percent['percent_str'] =((dfEdu18_percent['Count'] / len(df18Edu_Ea))*100).round(2).astype(str) + '%'##19dfEdu17_percent =df17Edu_Ea.groupby(['East_Asia','2017']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfEdu17_percent['percent'] =((dfEdu17_percent['Count'] / len(df17Edu_Ea))*100).round(2)dfEdu17_percent['percent_str'] =((dfEdu17_percent['Count'] / len(df17Edu_Ea))*100).round(2).astype(str) + '%' 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667#21년도 Bar graph 그리기fig = go.Figure()for country, group in dfEdu21_percent.groupby('East_Asia'): fig.add_trace(go.Bar( x = group['2021'], y = group['percent'], name = country ))fig.update_layout(barmode=&quot;group&quot;, plot_bgcolor = &quot;white&quot;, title='2021_나라별 학력')fig.show()#20년도 Bar graph 그리기fig = go.Figure()for country, group in dfEdu20_percent.groupby('East_Asia'): fig.add_trace(go.Bar( x = group['2020'], y = group['percent'], name = country ))fig.update_layout(barmode=&quot;group&quot;, plot_bgcolor = &quot;white&quot;, title='2020_나라별 학력')fig.show()#19년도 Bar graph 그리기fig = go.Figure()for country, group in dfEdu19_percent.groupby('East_Asia'): fig.add_trace(go.Bar( x = group['2019'], y = group['percent'], name = country ))fig.update_layout(barmode=&quot;group&quot;, plot_bgcolor = &quot;white&quot;, title='2019_나라별 학력')fig.show()#18년도 Bar graph 그리기fig = go.Figure()for country, group in dfEdu18_percent.groupby('East_Asia'): fig.add_trace(go.Bar( x = group['2018'], y = group['percent'], name = country ))fig.update_layout(barmode=&quot;group&quot;, plot_bgcolor = &quot;white&quot;, title='2018_나라별 학력')fig.show()#17년도 Bar graph 그리기fig = go.Figure()for country, group in dfEdu17_percent.groupby('East_Asia'): fig.add_trace(go.Bar( x = group['2017'], y = group['percent'], name = country ))fig.update_layout(barmode=&quot;group&quot;, plot_bgcolor = &quot;white&quot;, title='2017_나라별 학력')fig.show() 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667#21년도 Bar graph 그리기fig = go.Figure()for country, group in dfEdu21_percent.groupby('East_Asia'): fig.add_trace(go.Bar( x = group['2021'], y = group['Count'], name = country ))fig.update_layout(barmode=&quot;stack&quot;, plot_bgcolor = &quot;white&quot;, title='2021_나라별 학력 비율 ')fig.show()#20년도 Bar graph 그리기fig = go.Figure()for country, group in dfEdu20_percent.groupby('East_Asia'): fig.add_trace(go.Bar( x = group['2020'], y = group['Count'], name = country ))fig.update_layout(barmode=&quot;stack&quot;, plot_bgcolor = &quot;white&quot;, title='2020_나라별 학력 비율 ')fig.show()#19년도 Bar graph 그리기fig = go.Figure()for country, group in dfEdu19_percent.groupby('East_Asia'): fig.add_trace(go.Bar( x = group['2019'], y = group['Count'], name = country ))fig.update_layout(barmode=&quot;stack&quot;, plot_bgcolor = &quot;white&quot;, title='2019_나라별 학력 비율 ')fig.show()#18년도 Bar graph 그리기fig = go.Figure()for country, group in dfEdu18_percent.groupby('East_Asia'): fig.add_trace(go.Bar( x = group['2018'], y = group['Count'], name = country ))fig.update_layout(barmode=&quot;stack&quot;, plot_bgcolor = &quot;white&quot;, title='2018_나라별 학력 비율 ')fig.show()#17년도 Bar graph 그리기fig = go.Figure()for country, group in dfEdu17_percent.groupby('East_Asia'): fig.add_trace(go.Bar( x = group['2017'], y = group['Count'], name = country ))fig.update_layout(barmode=&quot;stack&quot;, plot_bgcolor = &quot;white&quot;, title='2017_나라별 학력 비율 ')fig.show() 12345678z = df21_Ea.groupby(['Q4', 'Q1']).size().unstack().fillna(0).astype('int64')z_data = z.apply(lambda x:np.round(x/x.sum(), 2), axis = 1).to_numpy() # convert to correlation matrixx = z.columns.tolist()y = z.index.tolist()fig = ff.create_annotated_heatmap(z_data, x = x, y = y, colorscale = &quot;sunset&quot;)fig.show() 경력123456789101112131415161718192021222324#전체 코드_3year = ['I have never written code', '&lt; 1 years', '1-3 years']_5year = ['3-5 years ','5-10 years']_10year = ['10-20 years','20+ years']df21_3year = df21['Q6'][df21['Q6'].isin(_3year)]df21_5year = df21['Q6'][df21['Q6'].isin(_5year)]df21_10year = df21['Q6'][df21['Q6'].isin(_10year)]df21_3year.count()df21_5year.count()df21_10year.count()years =['_3year','_5year', '_10year']values =[df21_3year.count(), df21_5year.count(), df21_10year.count()]fig = go.Figure(data=[ go.Bar(name='21년 World kaggler들의 경력', x=years, y=values ,orientation='v'),])fig.update_layout(title_text=&quot;&lt;b&gt;21년 World kaggler들의 경력&lt;/b&gt;&quot;,title_font_size=35)fig.show() 123456789101112131415161718192021222324252627282930#최종 합친 코드_3year = ['I have never written code', '&lt; 1 years', '1-3 years']_5year = ['3-5 years ','5-10 years']_10year = ['10-20 years','20+ years']df21_Ea_3year = df21_Ea['Q3'][df21_Ea['Q6'].isin(_3year)].value_counts().to_frame().rename(columns = {'Q3':'3year'})df21_Ea_5year = df21_Ea['Q3'][df21_Ea['Q6'].isin(_5year)].value_counts().to_frame().rename(columns = {'Q3':'5year'})df21_Ea_10year = df21_Ea['Q3'][df21_Ea['Q6'].isin(_10year)].value_counts().to_frame().rename(columns = {'Q3':'10year'})career=(df21_Ea_3year.join(df21_Ea_5year).join(df21_Ea_10year))careercareer.iloc[0,0:3] #Chinacareer.iloc[1,0:3] #Japancareer.iloc[2,0:3] #South Koreacareer.iloc[3,0:3] #Taiwanfig = go.Figure(data=[ go.Bar(name='China', x = years, y=career.iloc[0,0:3]), go.Bar(name='Japan', x = years, y=career.iloc[1,0:3]), go.Bar(name='South Korea', x= years, y=career.iloc[2,0:3]), go.Bar(name='Taiwan', x= years, y=career.iloc[3,0:3]) ])fig.update_layout(title_text=&quot;&lt;b&gt;21년 EastAisa kaggler들의 경력&lt;/b&gt;&quot;,title_font_size=35)fig.show() 연봉1234567891011121314#전체 코드#마지막 행 삭제해줌df21_=(df21['Q25'].value_counts().to_frame())#df21_=df21_.drop(df21_.index[26])#df21_compensation = df21_['Q25'].indexfig = go.Figure(data=[ go.Bar(name='21년 World kaggler들의 연봉', x=compensation, y=df21_['Q25'].to_numpy() ,orientation='v')])fig.update_layout(title_text=&quot;&lt;b&gt;21년 World kaggler들의 연봉&lt;/b&gt;&quot;,title_font_size=35)fig.show() 123456789101112131415161718compensation = df21_['Q25'].indexfig = go.Figure(data=[ go.Bar(name='China', x = compensation, y = df21_Ea['Q25'][df21_Ea['Q3'] =='Japan'].value_counts()), go.Bar(name='Japan', x = compensation, y=df21_Ea['Q25'][df21_Ea['Q3'] =='Taiwan'].value_counts()), go.Bar(name='South Korea', x = compensation, y=df21_Ea['Q25'][df21_Ea['Q3'] =='South Korea'].value_counts()), go.Bar(name='Taiwan', x = compensation, y=df21_Ea['Q25'][df21_Ea['Q3'] =='China'].value_counts()) ])fig.update_layout(title_text=&quot;&lt;b&gt;21년 EastAisa kaggler들의 연봉&lt;/b&gt;&quot;,title_font_size=35)fig.show() 언어1df21['Q7_Part_1'].value_counts() Python 21860 Name: Q7_Part_1, dtype: int64 12345678910111213141516171819202122232425262728293031323334353637#코드 전체df21_p = df21['Q7_Part_1'].value_counts().to_frame() #pythondf21_r = df21['Q7_Part_2'].value_counts().to_frame() #rdf21_s = df21['Q7_Part_3'].value_counts().to_frame() #sqldf21_c = df21['Q7_Part_4'].value_counts().to_frame() #cdf21_cc = df21['Q7_Part_5'].value_counts().to_frame() #c++df21_j = df21['Q7_Part_6'].value_counts().to_frame() #javadf21_js = df21['Q7_Part_7'].value_counts().to_frame() #javascriptdf21_ju = df21['Q7_Part_8'].value_counts().to_frame() #juliadf21_sw = df21['Q7_Part_9'].value_counts().to_frame() #swiftdf21_b = df21['Q7_Part_10'].value_counts().to_frame() #bashdf21_ma = df21['Q7_Part_11'].value_counts().to_frame() #matlabdf21_n = df21['Q7_Part_12'].value_counts().to_frame() #nonelanguages = ['Python','R','SQL','C','C++','Java','Javascript','Julia','Swift','Bash','MATLAB','None']fig = go.Figure(data=[ go.Bar(name='21년 World kaggler들이 사용하는 언어', x = languages, y = [df21_p.iloc[0,0], df21_r.iloc[0,0], df21_s.iloc[0,0], df21_c.iloc[0,0], df21_cc.iloc[0,0], df21_j.iloc[0,0], df21_js.iloc[0,0], df21_ju.iloc[0,0], df21_sw.iloc[0,0], df21_b.iloc[0,0], df21_ma.iloc[0,0], df21_n.iloc[0,0]],orientation='v') ])fig.update_layout(title_text=&quot;&lt;b&gt;21년 World kaggler들이 사용하는 언어&lt;/b&gt;&quot;,title_font_size=35)fig.show() 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980df21_lan_ch_p=df21_Ea['Q7_Part_1'][df21_Ea['Q3']=='China'].value_counts().to_frame().rename(columns = {'Q7_Part_1':'cnt'})df21_lan_ch_r=df21_Ea['Q7_Part_2'][df21_Ea['Q3']=='China'].value_counts().to_frame().rename(columns = {'Q7_Part_2':'cnt'})df21_lan_ch_s=df21_Ea['Q7_Part_3'][df21_Ea['Q3']=='China'].value_counts().to_frame().rename(columns = {'Q7_Part_3':'cnt'})df21_lan_ch_c=df21_Ea['Q7_Part_4'][df21_Ea['Q3']=='China'].value_counts().to_frame().rename(columns = {'Q7_Part_4':'cnt'})df21_lan_ch_cc=df21_Ea['Q7_Part_5'][df21_Ea['Q3']=='China'].value_counts().to_frame().rename(columns = {'Q7_Part_5':'cnt'})df21_lan_ch_j=df21_Ea['Q7_Part_6'][df21_Ea['Q3']=='China'].value_counts().to_frame().rename(columns = {'Q7_Part_6':'cnt'})df21_lan_ch_js=df21_Ea['Q7_Part_7'][df21_Ea['Q3']=='China'].value_counts().to_frame().rename(columns = {'Q7_Part_7':'cnt'})df21_lan_ch_ju=df21_Ea['Q7_Part_8'][df21_Ea['Q3']=='China'].value_counts().to_frame().rename(columns = {'Q7_Part_8':'cnt'})df21_lan_ch_sw=df21_Ea['Q7_Part_9'][df21_Ea['Q3']=='China'].value_counts().to_frame().rename(columns = {'Q7_Part_9':'cnt'})df21_lan_ch_b=df21_Ea['Q7_Part_10'][df21_Ea['Q3']=='China'].value_counts().to_frame().rename(columns = {'Q7_Part_10':'cnt'})df21_lan_ch_ma=df21_Ea['Q7_Part_11'][df21_Ea['Q3']=='China'].value_counts().to_frame().rename(columns = {'Q7_Part_11':'cnt'})df21_lan_ch_n=df21_Ea['Q7_Part_12'][df21_Ea['Q3']=='China'].value_counts().to_frame().rename(columns = {'Q7_Part_12':'cnt'})ch_lan = pd.concat([df21_lan_ch_p,df21_lan_ch_r,df21_lan_ch_s,df21_lan_ch_c,df21_lan_ch_cc,df21_lan_ch_j,df21_lan_ch_js,df21_lan_ch_ju,df21_lan_ch_sw,df21_lan_ch_b,df21_lan_ch_ma,df21_lan_ch_n])df21_lan_jp_p=df21_Ea['Q7_Part_1'][df21_Ea['Q3']=='Japan'].value_counts().to_frame().rename(columns = {'Q7_Part_1':'cnt'})df21_lan_jp_r=df21_Ea['Q7_Part_2'][df21_Ea['Q3']=='Japan'].value_counts().to_frame().rename(columns = {'Q7_Part_2':'cnt'})df21_lan_jp_s=df21_Ea['Q7_Part_3'][df21_Ea['Q3']=='Japan'].value_counts().to_frame().rename(columns = {'Q7_Part_3':'cnt'})df21_lan_jp_c=df21_Ea['Q7_Part_4'][df21_Ea['Q3']=='Japan'].value_counts().to_frame().rename(columns = {'Q7_Part_4':'cnt'})df21_lan_jp_cc=df21_Ea['Q7_Part_5'][df21_Ea['Q3']=='Japan'].value_counts().to_frame().rename(columns = {'Q7_Part_5':'cnt'})df21_lan_jp_j=df21_Ea['Q7_Part_6'][df21_Ea['Q3']=='Japan'].value_counts().to_frame().rename(columns = {'Q7_Part_6':'cnt'})df21_lan_jp_js=df21_Ea['Q7_Part_7'][df21_Ea['Q3']=='Japan'].value_counts().to_frame().rename(columns = {'Q7_Part_7':'cnt'})df21_lan_jp_ju=df21_Ea['Q7_Part_8'][df21_Ea['Q3']=='Japan'].value_counts().to_frame().rename(columns = {'Q7_Part_8':'cnt'})df21_lan_jp_sw=df21_Ea['Q7_Part_9'][df21_Ea['Q3']=='Japan'].value_counts().to_frame().rename(columns = {'Q7_Part_9':'cnt'})df21_lan_jp_b=df21_Ea['Q7_Part_10'][df21_Ea['Q3']=='Japan'].value_counts().to_frame().rename(columns = {'Q7_Part_10':'cnt'})df21_lan_jp_ma=df21_Ea['Q7_Part_11'][df21_Ea['Q3']=='Japan'].value_counts().to_frame().rename(columns = {'Q7_Part_11':'cnt'})df21_lan_jp_n=df21_Ea['Q7_Part_12'][df21_Ea['Q3']=='Japan'].value_counts().to_frame().rename(columns = {'Q7_Part_12':'cnt'})jp_lan = pd.concat([df21_lan_jp_p,df21_lan_jp_r,df21_lan_jp_s,df21_lan_jp_c,df21_lan_jp_cc,df21_lan_jp_j,df21_lan_jp_js,df21_lan_jp_ju,df21_lan_jp_sw,df21_lan_jp_b,df21_lan_jp_ma,df21_lan_jp_n])df21_lan_tw_p=df21_Ea['Q7_Part_1'][df21_Ea['Q3']=='Taiwan'].value_counts().to_frame().rename(columns = {'Q7_Part_1':'cnt'})df21_lan_tw_r=df21_Ea['Q7_Part_2'][df21_Ea['Q3']=='Taiwan'].value_counts().to_frame().rename(columns = {'Q7_Part_2':'cnt'})df21_lan_tw_s=df21_Ea['Q7_Part_3'][df21_Ea['Q3']=='Taiwan'].value_counts().to_frame().rename(columns = {'Q7_Part_3':'cnt'})df21_lan_tw_c=df21_Ea['Q7_Part_4'][df21_Ea['Q3']=='Taiwan'].value_counts().to_frame().rename(columns = {'Q7_Part_4':'cnt'})df21_lan_tw_cc=df21_Ea['Q7_Part_5'][df21_Ea['Q3']=='Taiwan'].value_counts().to_frame().rename(columns = {'Q7_Part_5':'cnt'})df21_lan_tw_j=df21_Ea['Q7_Part_6'][df21_Ea['Q3']=='Taiwan'].value_counts().to_frame().rename(columns = {'Q7_Part_6':'cnt'})df21_lan_tw_js=df21_Ea['Q7_Part_7'][df21_Ea['Q3']=='Taiwan'].value_counts().to_frame().rename(columns = {'Q7_Part_7':'cnt'})df21_lan_tw_ju=df21_Ea['Q7_Part_8'][df21_Ea['Q3']=='Taiwan'].value_counts().to_frame().rename(columns = {'Q7_Part_8':'cnt'})df21_lan_tw_sw=df21_Ea['Q7_Part_9'][df21_Ea['Q3']=='Taiwan'].value_counts().to_frame().rename(columns = {'Q7_Part_9':'cnt'})df21_lan_tw_b=df21_Ea['Q7_Part_10'][df21_Ea['Q3']=='Taiwan'].value_counts().to_frame().rename(columns = {'Q7_Part_10':'cnt'})df21_lan_tw_ma=df21_Ea['Q7_Part_11'][df21_Ea['Q3']=='Taiwan'].value_counts().to_frame().rename(columns = {'Q7_Part_11':'cnt'})df21_lan_tw_n=df21_Ea['Q7_Part_12'][df21_Ea['Q3']=='Taiwan'].value_counts().to_frame().rename(columns = {'Q7_Part_12':'cnt'})tw_lan = pd.concat([df21_lan_tw_p,df21_lan_tw_r,df21_lan_tw_s,df21_lan_tw_c,df21_lan_tw_cc,df21_lan_tw_j,df21_lan_tw_js,df21_lan_tw_ju,df21_lan_tw_sw,df21_lan_tw_b,df21_lan_tw_ma,df21_lan_tw_n])df21_lan_ko_p=df21_Ea['Q7_Part_1'][df21_Ea['Q3']=='South Korea'].value_counts().to_frame().rename(columns = {'Q7_Part_1':'cnt'})df21_lan_ko_r=df21_Ea['Q7_Part_2'][df21_Ea['Q3']=='South Korea'].value_counts().to_frame().rename(columns = {'Q7_Part_2':'cnt'})df21_lan_ko_s=df21_Ea['Q7_Part_3'][df21_Ea['Q3']=='South Korea'].value_counts().to_frame().rename(columns = {'Q7_Part_3':'cnt'})df21_lan_ko_c=df21_Ea['Q7_Part_4'][df21_Ea['Q3']=='South Korea'].value_counts().to_frame().rename(columns = {'Q7_Part_4':'cnt'})df21_lan_ko_cc=df21_Ea['Q7_Part_5'][df21_Ea['Q3']=='South Korea'].value_counts().to_frame().rename(columns = {'Q7_Part_5':'cnt'})df21_lan_ko_j=df21_Ea['Q7_Part_6'][df21_Ea['Q3']=='South Korea'].value_counts().to_frame().rename(columns = {'Q7_Part_6':'cnt'})df21_lan_ko_js=df21_Ea['Q7_Part_7'][df21_Ea['Q3']=='South Korea'].value_counts().to_frame().rename(columns = {'Q7_Part_7':'cnt'})df21_lan_ko_ju=df21_Ea['Q7_Part_8'][df21_Ea['Q3']=='South Korea'].value_counts().to_frame().rename(columns = {'Q7_Part_8':'cnt'})df21_lan_ko_sw=df21_Ea['Q7_Part_9'][df21_Ea['Q3']=='South Korea'].value_counts().to_frame().rename(columns = {'Q7_Part_9':'cnt'})df21_lan_ko_b=df21_Ea['Q7_Part_10'][df21_Ea['Q3']=='South Korea'].value_counts().to_frame().rename(columns = {'Q7_Part_10':'cnt'})df21_lan_ko_ma=df21_Ea['Q7_Part_11'][df21_Ea['Q3']=='South Korea'].value_counts().to_frame().rename(columns = {'Q7_Part_11':'cnt'})df21_lan_ko_n=df21_Ea['Q7_Part_12'][df21_Ea['Q3']=='South Korea'].value_counts().to_frame().rename(columns = {'Q7_Part_12':'cnt'})ko_lan = pd.concat([df21_lan_ko_p,df21_lan_ko_r,df21_lan_ko_s,df21_lan_ko_c,df21_lan_ko_cc,df21_lan_ko_j,df21_lan_ko_js,df21_lan_ko_ju,df21_lan_ko_sw,df21_lan_ko_b,df21_lan_ko_ma,df21_lan_ko_n])ch_lan['cnt'].to_list()languages = ['Python','R','SQL','C','C++','Java','Javascript','Julia','Swift','Bash','MATLAB','None']fig = go.Figure(data=[ go.Bar(name='China', x = languages, y = ch_lan['cnt'].tolist()), go.Bar(name='Japan', x = languages, y=jp_lan['cnt'].tolist()), go.Bar(name='South Korea', x = languages, y=ko_lan['cnt'].tolist()), go.Bar(name='Taiwan', x = languages, y=tw_lan['cnt'].tolist()) ])fig.update_layout(title_text=&quot;&lt;b&gt;21년 EastAisa kaggler들이 사용하는 언어&lt;/b&gt;&quot;,title_font_size=35)fig.show() Thank you for reading!","link":"/2021/11/18/kgg/Kgg_eastasia_Ver.1118/"},{"title":"NLP_text_classification","text":"##Kaggle _ API !pip Install Kaggle : Kaggle 설치 google.colab에 kaggle.json files upload Saving kaggle.json to kaggle.json User uploaded file “kaggle.json” with length 66 bytes kaggle.json file에는 뭐가 들어있을까 너무 궁금하다. !kaggle competitions download -c nlp-getting-started 케글 대회 자료를 다운받기. (-c nlp-getting-started 이게 뭘까) data path 설정하기 ##data 둘러보기 data frame을 만들기 위해 Pandas와 numpy를 import후 각 file을 data set을 Load해 준다. data set 확인 .head()로 대략적인 data set 확인 .shape로 각 data set의 크기 확인 .info()로 각 data frame의 정보 확인 ##EDAEDA : 수집한 data를 다양한 각도에서 관찰하고 이해하는 과정 : 통계적 방법으로 자료를 직관적으로 바라보는 과정 data의 분포 및 값을 검토함으로써 데이터가 표현하는 현상을 더 잘 이해하고, 잠재적 문제를 발견 할 수 있다. 문제를 발견하여 기존의 가설을 수정하거나 새로운 가설을 세울 수 있다. data visualiztion을 위해 matplotlib.pyplot과 seaborn 설치 missing_colunms = [“keyword”, “location”] 각 data set에서 null인 columns를 가져온다. matplotlib.pyplot으로 bar plot 그리기 ##Feature Engineering##Mideling##algorithm logistic regression","link":"/2021/11/11/kgg/Kgg_nlp_Tex/"},{"title":"kaggle :HorizontalBar (Q7)","text":"kaggle dictation (06) plotly.graph_objects as go: 를 이용한 bar graphHorizontalBar plot /가로 막대 차트0. data sethttps://www.kaggle.com/miguelfzzz/the-typical-kaggle-data-scientist-in-2021 Subject : 가장 많이쓰는 programming 언어_Horizontal bar1. data 읽어오기 Q7에는 sub가 많기 때문에 python 구문을 이용하여‘Q7’ 이 붙어있는 컬럼 불러오기. languages_cols = [col for col in df if col.startswith(‘Q7’)] col 1부터 df 끝까지 Q7로 시작하는지 확인하여 true 일 때만 데이터 가져오기 1languages_cols = [col for col in df if col.startswith('Q7')] 2. data Frame 만들어 주기algorithms 에 data frame을 씌워서 표를 만들고, 이름을 다음과같이 바꿔줌. 12345languages = df[languages_cols]languages.columns = ['Python', 'R', 'SQL', 'C', 'C++', 'Java', 'Javascript', 'Julia', 'Swift', 'Bash', 'MATLAB', 'None', 'Other'] 3.표 설정.12345678languages = ( languages .count() .to_frame() .reset_index() .rename(columns={'index':'Languages', 0:'Count'}) .sort_values(by=['Count'], ascending=False) ) .count() :coulumn 수 세기 .to_frame() : frame 생성 .reset_index() : 원본과 상관없는 Index 생성 .rename() columns의 이름을 지정 : ‘index’:’Languages’, 0:’Count’ .sort_values() by=[‘Count’], ascending=False Count 기준으로 내림차순으로 정렬 3. percent 추가1languages['percent'] = ((languages['Count'] / len(df))*100).round(2).astype(str) + '%' 표에 ‘percent’를 추가algorithms의 count에 df의 length로 나누고 *100을 하는 전형적인 % 나타내기 값자체에 %를 입력하여 나중에 %를 추가 입력하지 않아도 됨 소숫점 자리 2까지 반영(반올림).type 자체를 String으로 하여 추가 계산은 불가능. 4. 색 지정1234567891011colors = ['#033351',] * 13colors[0] = '#5abbf9'colors[1] = '#5abbf9'colors[2] = '#0779c3'colors[3] = '#0779c3'colors[4] = '#0779c3'colors[5] = '#0779c3'colors[6] = '#0779c3'colors[7] = '#05568a'colors[8] = '#05568a'colors[9] = '#05568a' 5. bar Graph 만들기1234567fig = go.Figure(go.Bar( x=algorithms['Count'], y=algorithms['Algorithms'], text=algorithms['percent'], orientation='h', marker_color=colors )) horizontal과 vertical Graph의 차이는 x, y axis를 바꾸어 주는 것과 orientation=’h’ 을 넣어 주는 것의 차이. 6. update_traces()traces() 수정 : Trace에 대한 설정 123456fig.update_traces(texttemplate='%{text}', textposition='outside', cliponaxis = False, hovertemplate='&lt;b&gt;Lenguage&lt;/b&gt;: %{y}&lt;br&gt;&lt;extra&gt;&lt;/extra&gt;'+ '&lt;b&gt;Count&lt;/b&gt;: %{x}', textfont_size=12) texttemplate : text type textposition : ‘outside’ _ 설정 해 주지 않은 경우 칸에 따라 적당히 들어감. cliponaxis = False : text가 칸이 작아서 짤리는 경우를 막아주는 기능 (off) hovertemplate : 마우스 On하면 (커서를 위에 대면) 나오는 Hovert에대한 설정. textfont_size : 폰트 size 7. Design1234567891011121314151617fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.update_layout(showlegend=False, plot_bgcolor='#F7F7F7', margin=dict(pad=20), paper_bgcolor='#F7F7F7', height=700, xaxis={'showticklabels': False}, yaxis_title=None, xaxis_title=None, yaxis={'categoryorder':'total ascending'}, title_text=&quot;Most Commonly Used &lt;b&gt;Programming Languages&lt;/b&gt;&quot;, title_x=0.5, font=dict(family=&quot;Hiragino Kaku Gothic Pro, sans-serif&quot;, size=17, color='#000000'), title_font_size=35) Grid Delete update_layout showlegend=False, plot_bgcolor=’#F7F7F7’ margin=dict(pad=20), paper_bgcolor=’#F7F7F7’, xaxis={‘showticklabels’: False}, x 축 labels을 삭제. yaxis_title=None, xaxis_title=None, yaxis={‘categoryorder’:’total ascending’}, y 축 title을 categoryorder : 정렬 title_text=”Most Commonly Used Algorithms“, title_x=0.5, font=dict(family=”Hiragino Kaku Gothic Pro, sans-serif”, size=15, color=’#000000’), title_font_size=35) 8. Annotation12345678910111213141516171819fig.add_annotation(dict(font=dict(size=14), x=0.98, y=-0.13, showarrow=False, text=&quot;@miguelfzzz&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.add_annotation(dict(font=dict(size=12), x=0, y=-0.13, showarrow=False, text=&quot;Source: 2021 Kaggle Machine Learning &amp; Data Science Survey&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show()","link":"/2021/11/09/kgg/Kgg_plotly_Bar_H(3)/"},{"title":"kaggle :HorizontalBar (Q9)","text":"kaggle dictation (04) plotly.graph_objects as go: 를 이용한 bar graphHorizontalBar plot /가로 막대 차트0. data sethttps://www.kaggle.com/miguelfzzz/the-typical-kaggle-data-scientist-in-2021 Subject : 어떤알고리즘을 선호 하는 지 알아보는 Horizontal bar1. data 읽어오기123456# Features that start with Q7ide_cols = [col for col in df if col.startswith('Q9')]ide = df[ide_cols]print(ide) list comprehensionfor문으로 돌린 data값을 전부 col로 받아와서그 값을 또 ide_cols에 싣는 한줄로 이루어진 코드. 2. data Frame 만들어 주기ide를 dataframe화 완료. 123ide.columns = ['JupyterLab', 'RStudio', 'Visual Studio', 'VSCode', 'PyCharm', 'Spyder', 'Notepad++', 'Sublime Text', 'Vim, Emacs, or similar', 'MATLAB', 'Jupyter Notebook', 'None', 'Other'] Q9의 column이름 재 설정. 3.표 설정.12345678ide = ( ide .count() .to_frame() .reset_index() .rename(columns={'index':'IDE', 0:'Count'}) .sort_values(by=['Count'], ascending=False) ) 3. percent 추가1ide['percent'] = ((ide['Count'] / len(df))*100).round(2).astype(str) + '%' 4. 색 지정123456789colors = ['#033351',] * 13colors[0] = '#5abbf9'colors[1] = '#5abbf9'colors[2] = '#0779c3'colors[3] = '#0779c3'colors[4] = '#0779c3'colors[5] = '#0779c3'colors[6] = '#0779c3'colors[7] = '#0779c3' 5. bar Graph 만들기1234567fig = go.Figure(go.Bar( x=ide['Count'], y=ide['IDE'], text=ide['percent'], orientation='h', marker_color=colors )) 6. update_traces()123456fig.update_traces(texttemplate='%{text}', textposition='outside', cliponaxis = False, hovertemplate='&lt;b&gt;IDE&lt;/b&gt;: %{y}&lt;br&gt;&lt;extra&gt;&lt;/extra&gt;'+ '&lt;b&gt;Count&lt;/b&gt;: %{x}', textfont_size=12) 7. Design12345678910111213141516fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False) fig.update_layout(showlegend=False, plot_bgcolor='#F7F7F7', margin=dict(pad=20), paper_bgcolor='#F7F7F7', xaxis={'showticklabels': False}, yaxis_title=None, height = 600, xaxis_title=None, yaxis={'categoryorder':'total ascending'}, title_text=&quot;Most Commonly Used &lt;b&gt;IDE's&lt;/b&gt;&quot;, title_x=0.5, font=dict(family=&quot;Hiragino Kaku Gothic Pro, sans-serif&quot;, size=15, color='#000000'), title_font_size=35) 8. Annotation1234567891011121314151617181920fig.add_annotation(dict(font=dict(size=14), x=0.98, y=-0.17, showarrow=False, text=&quot;@miguelfzzz&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.add_annotation(dict(font=dict(size=12), x=0, y=-0.17, showarrow=False, text=&quot;Source: 2021 Kaggle Machine Learning &amp; Data Science Survey&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show()","link":"/2021/11/09/kgg/Kgg_plotly_HZB(2)/"},{"title":"kaggle :HorizontalBar (Q17)","text":"kaggle dictation (03) plotly.graph_objects as go: 를 이용한 bar graphHorizontalBar plot /가로 막대 차트0. data sethttps://www.kaggle.com/miguelfzzz/the-typical-kaggle-data-scientist-in-2021 Subject : 어떤알고리즘을 선호 하는 지 알아보는 Horizontal bar1. data 읽어오기algorithms_cols에 data frame을 먼저 만들고 시작.잘 모르겠지만 아마도 Q17이 붙은 data를 선택 하기 위한 code algorithms_cols = [col for col in df if col.startswith(‘Q17’)] col 1부터 df 끝까지 Q17로 시작하는지 확인하여 true일때만 데이터 가져오기 ref. How to select dataframe columns that start with *** using pandas in python ? 12algorithms_cols = [col for col in df if col.startswith('Q17')] 2. data Frame 만들어 주기algorithms 에 data frame을 씌워서 표를 만들고, 이름을 다음과같이 바꿔줌. 12345678algorithms = df[algorithms_cols]algorithms.columns = ['Linear or Logistic Regression', 'Decision Trees or Random Forests', 'Gradient Boosting Machines', 'Bayesian Approaches', 'Evolutionary Approaches', 'Dense Neural Networks', 'Convolutional Neural Networks', 'Generative Adversarial Networks', 'Recurrent Neural Networks', 'Transformer Networks', 'None', 'Other'] 3.표 설정.12345678algorithms = ( algorithms .count() .to_frame() .reset_index() .rename(columns={'index':'Algorithms', 0:'Count'}) .sort_values(by=['Count'], ascending=False) ) .count() :coulumn 수 세기 .to_frame() : frame 생성 .reset_index() : 원본과 상관없는 Index 생성 .rename() columns의 이름을 지정 : ‘index’:’Algorithms’, 0:’Count’ .sort_values() by=[‘Count’], ascending=False Count 기준으로 내림차순으로 정렬 3. percent 추가12algorithms['percent'] = ((algorithms['Count'] / len(df))*100).round(2).astype(str) + '%' 표에 ‘percent’를 추가algorithms의 count에 df의 length로 나누고 *100을 하는 전형적인 % 나타내기 값자체에 %를 입력하여 나중에 %를 추가 입력하지 않아도 됨 소숫점 자리 2까지 반영(반올림).type 자체를 String으로 하여 추가 계산은 불가능. 4. 색 지정12345678colors = ['#033351',] * 12colors[0] = '#5abbf9'colors[1] = '#5abbf9'colors[2] = '#066eb0'colors[3] = '#066eb0'colors[4] = '#044a77'colors[5] = '#044a77'colors[6] = '#044a77' 색 깊이는 12단계, 0~6까지는 정해주고 나머지 5개는 #033351을 default로 한 것을 알 수 있다. 5. bar Graph 만들기1234567fig = go.Figure(go.Bar( x=algorithms['Count'], y=algorithms['Algorithms'], text=algorithms['percent'], orientation='h', marker_color=colors )) horizontal과 vertical Graph의 차이는 x, y axis를 바꾸어 주는 것과 orientation=’h’ 을 넣어 주는 것의 차이. &lt;orientation 없음&gt; &lt;orientation H 있음&gt; 6. update_traces()traces() 수정 : Trace에 대한 설정 123456fig.update_traces(texttemplate='%{text}', textposition='outside', cliponaxis = False, hovertemplate='&lt;b&gt;Algorithm&lt;/b&gt;: %{y}&lt;br&gt;&lt;extra&gt;&lt;/extra&gt;'+ '&lt;b&gt;Count&lt;/b&gt;: %{x}', textfont_size=12) texttemplate : text type textposition : ‘outside’ _ 설정 해 주지 않은 경우 칸에 따라 적당히 들어감. cliponaxis = False : text가 칸이 작아서 짤리는 경우를 막아주는 기능 (off) hovertemplate : 마우스 On하면 (커서를 위에 대면) 나오는 Hovert에대한 설정. textfont_size : 폰트 size 7. Design1234567891011121314151617fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.update_layout(showlegend=False, plot_bgcolor='#F7F7F7', margin=dict(pad=20), paper_bgcolor='#F7F7F7', xaxis={'showticklabels': False}, yaxis_title=None, height = 600, xaxis_title=None, yaxis={'categoryorder':'total ascending'}, title_text=&quot;Most Commonly Used &lt;b&gt;Algorithms&lt;/b&gt;&quot;, title_x=0.5, font=dict(family=&quot;Hiragino Kaku Gothic Pro, sans-serif&quot;, size=15, color='#000000'), title_font_size=35) Grid Delete update_layout showlegend=False, plot_bgcolor=’#F7F7F7’ margin=dict(pad=20), paper_bgcolor=’#F7F7F7’, xaxis={‘showticklabels’: False}, x 축 labels을 삭제. yaxis_title=None, xaxis_title=None, yaxis={‘categoryorder’:’total ascending’}, y 축 title을 categoryorder Automatically Sorting Categories by Name or Total Value layout-xaxis-categoryorder title_text=”Most Commonly Used Algorithms“, title_x=0.5, font=dict(family=”Hiragino Kaku Gothic Pro, sans-serif”, size=15, color=’#000000’), title_font_size=35) 이미 앞서서 충분히 설명 했기 때문에 본 posting에서 설명되지 않은 부분은 basic bar Graph 에서 확인 8. Annotation1234567891011121314151617181920fig.add_annotation(dict(font=dict(size=14), x=0.98, y=-0.17, showarrow=False, text=&quot;@miguelfzzz&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.add_annotation(dict(font=dict(size=12), x=0, y=-0.17, showarrow=False, text=&quot;Source: 2021 Kaggle Machine Learning &amp; Data Science Survey&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show()","link":"/2021/11/09/kgg/Kgg_plotly_Horizontal/"},{"title":"kaggle :ScatterLine (Q42)","text":"kaggle dictation (08) plotly.graph_objects as go_ Scatter + line plot산점도 bivariate”이변수” 값을 시각화 하는 기본적인 그래프. correlation: Positive, Negative, non 두 개의 변수 각각의 분포과 변수간의 관계를 확인 할 수 있다. ref. line-and-scatter/En. Q11_Scatter 0. data sethttps://www.kaggle.com/miguelfzzz/the-typical-kaggle-data-scientist-in-2021 Subject : 가장 많이 이용하는 Media source1. data 읽어오기Q42로 시작하는 col을 읽어오기.python의 for문을 이용. 1media_cols = [col for col in df if col.startswith('Q42')] 2. data Frame 만들어 주기123456media = df[media_cols]media.columns = ['Twitter', 'Email newsletters', 'Reddit', 'Kaggle', 'Course Forums', 'YouTube', 'Podcasts', 'Blogs', 'Journal Publications', 'Slack Communities', 'None', 'Other'] 3.표 설정.12345678910media = ( media .count() .to_frame() .reset_index() .rename(columns={'index':'Medias', 0:'Count'}) .sort_values(by=['Count'], ascending=False) )media 4. 색 지정12345678910colors = ['#033351',] * 12colors[11] = '#5abbf9'colors[10] = '#5abbf9'colors[9] = '#5abbf9'colors[8] = '#0779c3'colors[7] = '#0779c3'colors[6] = '#0779c3'colors[5] = '#0779c3'colors[4] = '#0779c3' 5. percent로 계산한 column 추가i. add percent column 123media['percent'] = ((media['Count'] / len(df))*100).round(2).astype(str) + '%'media ii. Count값 (column값으로 ) 정렬 123456media = (media .sort_values(by = ['Count']) .iloc[0:15] .reset_index())media 1. Default는 내림차순 2. iloc으로 0번부터 15까지 List로 긁어오기 3. reset index() 6.plotly.graph_objects.Scatter()Scatter G 그리기 i. 산점도 점 찍기 12345678fig = go.Figure(go.Scatter(x = media['Count'], y = media[&quot;Medias&quot;], text = media['percent'], mode = 'markers', marker_color =colors, marker_size = 12))fig ii. 산점도에 for문을 이용하여 line 연결하기 1234567for i in range(0, len(media)): fig.add_shape(type='line', x0 = 0, y0 = i, x1 = media[&quot;Count&quot;][i], y1 = i, line=dict(color=colors[i], width = 4)) fig for i in range(0~platform의 길이만큼) fig. add_shape() type = ‘line’ line모양의 grape shape add x0 = 0, y0 = i, 초기값 (0, i)에서 시작 (0, 0) = other Line Start x1 = platform[“Count”][i], x축 Index : count의 값만큼 x축방향으로 Line이 그어진다. y1 = i, y축 Index, 마지막 값 line=dict(color=colors[i], width = 4) line의 세부 설정, 색과 두께 7. update_traces(hovertemplate)123fig.update_traces(hovertemplate='&lt;b&gt;Media Source&lt;/b&gt;: %{y}&lt;br&gt;&lt;extra&gt;&lt;/extra&gt;'+ '&lt;b&gt;Count&lt;/b&gt;: %{x}&lt;br&gt;'+ '&lt;b&gt;Proportion&lt;/b&gt;: %{text}') 8. Designi. 축 grid 12fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='#9f9f9f', ticklabelmode='period')fig.update_yaxes(showgrid=False) x 축의 grid만 보여줌. tick labe lmode : period plotly의 axes/En ii. update_layout() 123456789101112fig.update_layout(showlegend=False, plot_bgcolor='#F7F7F7', margin=dict(pad=20), paper_bgcolor='#F7F7F7', yaxis_title=None, xaxis_title=None, title_text=&quot;Most Commonly Used &lt;b&gt;Media Sources&lt;/b&gt;&quot;, title_x=0.5, height=700, font=dict(family=&quot;Hiragino Kaku Gothic Pro, sans-serif&quot;, size=17, color='#000000'), title_font_size=35) 9. Annotation12345678910111213141516171819fig.add_annotation(dict(font=dict(size=14), x=0.98, y=-0.22, showarrow=False, text=&quot;@miguelfzzz&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.add_annotation(dict(font=dict(size=12), x=0.04, y=-0.22, showarrow=False, text=&quot;Source: 2021 Kaggle Machine Learning &amp; Data Science Survey&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show()","link":"/2021/11/09/kgg/Kgg_plotly_ScatterL(2)/"},{"title":"kaggle :ScatterLine (Q11)","text":"kaggle dictation (05) plotly.graph_objects as go: 를 이용한 Scatter + line G산점도 bivariate”이변수” 값을 시각화 하는 기본적인 그래프. correlation: Positive, Negative, non 두 개의 변수 각각의 분포과 변수간의 관계를 확인 할 수 있다. ref. box and scatter plot/Ko.통계 그리는 방법 0. data sethttps://www.kaggle.com/miguelfzzz/the-typical-kaggle-data-scientist-in-2021 Subject : 가장 많이 이용하는 computer platform(hardware)1. data 읽어오기 + data Frame 만들어 주기1234567891011platform = ( df['Q11'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'Platform', 'Q11':'Count'}) .sort_values(by=['Count'], ascending=False) .replace(['A deep learning workstation (NVIDIA GTX, LambdaLabs, etc)', 'A cloud computing platform (AWS, Azure, GCP, hosted notebooks, etc)'], ['A deep learning workstation', 'A cloud computing platform']) ) ide를 dataframe화 완료. Q11의 column이름 까지 재설정 완료. 2.표 설정.1234567891011ide = ( ide .count() .to_frame() .reset_index() .rename(columns={'index':'IDE', 0:'Count'}) .sort_values(by=['Count'], ascending=False) )ide['percent'] = ((ide['Count'] / len(df))*100).round(2).astype(str) + '%' 3. percent 추가1platform['percent'] = ((platform['Count'] / platform['Count'].sum())*100).round(2).astype(str) + '%' 3. 색 지정1234colors = ['#033351',] * 6colors[5] = '#5abbf9'colors[4] = '#0779c3'colors[3] = '#0779c3' 4. 표 재 설정12345platform = (platform .sort_values(by = ['Count']) .iloc[0:15] .reset_index())platform .sort_values(by = [‘Count’]) : [Count]로 정렬, .iloc[0:15] platform의 column 선택: 0~15까지 data 가져오기 .reset_index() : data와 상관 없는 새 index 가져오기 5.plotly.graph_objects.Scatter() 본격적으로 Scatter G 만들기. ## 산점도 점 찍기 12345678fig = go.Figure(go.Scatter(x = platform['Count'], y = platform[&quot;Platform&quot;], text = platform['percent'], mode = 'markers', marker_color =colors, marker_size = 12))fig x = platform[‘Count’], y = platform[“Platform”], x축, y축 설정 text = platform[‘percent’], text를 넣는다고 하는데 안보이네 mode = ‘markers’, Text, lines+markers, makers, line 이 가능 한거 같다. Scatter.mod marker_color =colors, marker_size = 12) 산점도 안에 있는 점의 색과 크기 ## 산점도에 for문을 이용하여 line 연결하기 123456for i in range(0, len(platform)): fig.add_shape(type='line', x0 = 0, y0 = i, x1 = platform[&quot;Count&quot;][i], y1 = i, line=dict(color=colors[i], width = 4)) for i in range(0~platform의 길이만큼) fig. add_shape() type = ‘line’ - line모양의 grape shape add x0 = 0, y0 = i, - 초기값 x1 = platform[“Count”][i], x축 Indexy1 = i, y축 Index, 마지막 값 line=dict(color=colors[i], width = 4) line의 세부 설정, 색과 두께 flatform은 .iloc[0:15] 로 뽑아진 list 형식따라서 platform[“Count”][i]값을 뽑아 낼 수 있다. 6. update_traces()123fig.update_traces(hovertemplate='&lt;b&gt;Platform&lt;/b&gt;: %{y}&lt;br&gt;&lt;extra&gt;&lt;/extra&gt;'+ '&lt;b&gt;Count&lt;/b&gt;: %{x}&lt;br&gt;'+ '&lt;b&gt;Proportion&lt;/b&gt;: %{text}') 7. Design12345678910111213fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='#9f9f9f', ticklabelmode='period')fig.update_yaxes(showgrid=False) fig.update_layout(showlegend=False, plot_bgcolor='#F7F7F7', margin=dict(pad=20), paper_bgcolor='#F7F7F7', yaxis_title=None, xaxis_title=None, title_text=&quot;Most Commonly Used &lt;b&gt;Computing Platforms&lt;/b&gt;&quot;, title_x=0.5, font=dict(family=&quot;Hiragino Kaku Gothic Pro, sans-serif&quot;, size=17, color='#000000'), title_font_size=35) 8. Annotation12345678910111213141516171819fig.add_annotation(dict(font=dict(size=14), x=0.98, y=-0.22, showarrow=False, text=&quot;@miguelfzzz&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.add_annotation(dict(font=dict(size=12), x=0.04, y=-0.22, showarrow=False, text=&quot;Source: 2021 Kaggle Machine Learning &amp; Data Science Survey&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show()","link":"/2021/11/09/kgg/Kgg_plotly_ScatterLine/"},{"title":"kaggle :Bar Graph (Q6)","text":"kaggle dictation (02) plotly.graph_objects as go: 를 이용한 bar graph###bar plot /막대 차트 막대그래프는 가장 많이 쓰이는 플롯들중 하나로 숫자 변수와 범주 형 변수 간의 관계를 보여 줍니다. 막대 차트는 종종 히스토그램과 혼동 되기도 하는데(숫자의 분포를 보여줌) 이는 통계학의 부분 그룹당 여러 값이 있는 경우에는 박스플롯이나 바이올린 플롯을 추천 최소한 그룹당 관측치 수와 각 그룹의 신뢰구간은 표시되야함. 사용한 Library12345678import pandas as pdimport numpy as npimport seaborn as snsimport plotly.express as pximport plotly.graph_objects as goimport warningswarnings.filterwarnings('ignore') 사실 이 부분에서 seaborn을 사용 했는지 잘 모르겠음. github에서 plotly가 동적 Livrary라 자꾸 오류가남. data importdata 원문data import 방법 data: Kaggle의 the-typical-kaggle-data-scientist-in-2021 이 부분은 data import 방법 을 참고 하거나kaggle dictation (01) 을 참조하세요. data encoding (Feature Engineering)사실 이 부분이 feature Engineering에 해당하는 부분인지 잘 모르겠다. 이 부분은 data를 computer로 자동화하여 계산, 동적 그래프를 만들기 위한 부분. ###Feature Engineering Experience라는 Question 6에 해당하는 값을 전처리 해 준다. 123456789101112experience = ( df['Q6'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'Experience', 'Q6':'Count'}) .replace(['I have never written code','&lt; 1 years', '1-3 years', '3-5 years', '5-10 years', '10-20 years', '20+ years'], ['No experience', '&lt;1 years', '1-3 years', '3-5 years', '5-10 years', '10-20 years', '20+ years']) ) .value_counts() : 데이터의 분포를 확인하는데 매우 유용한 함수 column 값의 개수를 확인 하는것. 중복되는 값을 묶어줌. .to_frame() : frame을 설정 (표 생성) reset_index() 원본 data를 회손하지 않고 Index를 새로 만듦 .rename(columns={‘index’:’Experience’, ‘Q6’:’Count’}) column에 새로운 이름을 붙여줌 index는 Experience로 Q6은 count로 지정 replace() [왼쪽]에 있는 값대신 [오른쪽]에 있는 값을 넣으려고 함. 이 경우 ‘I have never written code’를 ‘No experience’로 바꾸려고 한듯. Ref. 판다스 함수 df.value_counts() 함수만 사용하면 아래와 같이 나온다. 1df['Q6'].value_counts() 1-3 years 7874&lt; 1 years 58813-5 years 40615-10 years 309910-20 years 216620+ years 1860I have never written code 1032Name: Q6, dtype: int64 이는 문자형 data를 분석하여 display하기 위한 방법 data categoircal로 List로 만들고, 함수정의, 정렬(#1) : Pandas lib의 categories function문자열 객체의 배열을 series로 변환하여 범주형으로 변환 1234567#1experience['Experience'] = pd.Categorical( experience['Experience'], ['No experience', '&lt;1 years', '1-3 years', '3-5 years', '5-10 years', '10-20 years', '20+ years'] ) pd.categories (#2) : experience[‘percent’]Experience에 하나의 tap을 추가 해준다.SQL 의 insert 에 percent tep을 만들어 줄 때 column에대해 계산하여 값을 보여주는 것과 같은 느낌. value에 들어갈 수식 지정_ Experience의 percent 계산 123 #2experience['percent'] = ((experience['Count'] / experience['Count'].sum())*100).round(2).astype(str) + '%' (#3) : experience.sort_values()데이터 정렬하기 : 컬럼의 data를 기준으로 정렬 - short_Index의 경우에는 Index를 기준으로 data를 정렬한다. - 이 경우에는 (‘Experience’)를 기준으로 default값인 오름차순으로 정렬 보통 percent나 count를 기준으로 정렬 되는데 이 경우 sort_values(‘Experience’) 를 하였기때문에기준인 Experience를 기준으로 오름차순으로 정렬 되었다. Short_value 123#3experience = experience.sort_values('Experience') (#4) : colors바chart의 color을 설정 *7 은 7개의 수준이 있다는 것. colors[N] = 뭘까 chart color 123456789#4colors = ['#033351',] * 7colors[1] = '#5abbf9'colors[2] = '#5abbf9'colors[3] = '#0779c3'colors[4] = '#0779c3' (#5) : fig = go.Figure(go.Bar())import plotly.graph_objects as go 이기때문에 fig는 plotly library 함수 사용 12345678#5fig = go.Figure(go.Bar( y=experience['Count'], x=experience['Experience'], cliponaxis = False, text=experience['percent'], marker_color=colors )) x축, y축 정해주기 y=experience[‘Count’], x=experience[‘Experience’], cliponaxis cliponaxis = False, cliponaxis – Text node를 아래 축에 고정 할지 아닐지 결정text node를 축 라인과 체크라벨 위에 보여주기위해서는 x축Layer와 y축 layer 설정을 해 주어야 한다. text=experience[‘percent’] marker_color=colors 지정 해 준 colors를 사용. (#6) : fig.update_traces()그래프 위에 캡션 다는 기능 Perform a property update operation on all traces that satisfy the specified selection criteria 지정된 선택 기준을 충족하는 모든 추적에 대해 속성 업데이트 작업 수행 (?? 전혀 모르겠군 !) 아래 본문과는 상관없는 data를 좀 보세요. 12345678910111213141516df1 = df['Q1'].value_counts()df2 = df['Q1'].value_counts(normalize = True)val_pnt_df = pd.DataFrame({&quot;count&quot;:df1,&quot;%&quot;:df2*100})fig = go.Figure()# [str(x) + ' %' for x in np.round(val_pnt_df[&quot;%&quot;].values, 1).tolist()]# Add Tracesfig.add_bar(x = val_pnt_df.index, y = val_pnt_df['count'].values, text = [str(x) + ' %' for x in np.round(val_pnt_df[&quot;%&quot;].values, 1).tolist()], textposition=&quot;auto&quot;)# layoutfig.update_layout(title_text = &quot;Q1. &quot; + questions[1])fig.show() python의 comprehension 오늘은 못보고 나중에 다시 보자 ! 123456#6fig.update_traces(texttemplate='%{text}', textposition='outside', hovertemplate='&lt;b&gt;Experience&lt;/b&gt;: %{x}&lt;br&gt;&lt;extra&gt;&lt;/extra&gt;'+ '&lt;b&gt;Count&lt;/b&gt;: %{y}', textfont_size=12) texttemplate=’%{text}’, : text type textposition=’outside’, : inside하면 그래프 안쪽, ouside 하면 그래프 위쪽에 생성 hovertemplate= 커서를 가까이 대면 나오는 창 x값과, y 값이 어떤 상태인지 알려 준다. hevertemplate Returns the Figure object that the method was called on 메서드가 호출된 그림객체를 반환. plotly.graph_objects.Figure.update_traces 123#7fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False) (#7) : fig.update_?axes()만들어진 fig를 수정.SQL의 update와 비슷한 기능인듯. fig.update_xaxes(showgrid=False) : x축의 grid 수정fig.update_yaxes(showgrid=False) : y축의 grid 수정 축을 보이지 않는 형태로 바꾸어 예쁘게 보이게 해줌. (#8) : update_layout()1234567891011121314#8fig.update_layout(showlegend=False, plot_bgcolor='#F7F7F7', margin=dict(pad=20), paper_bgcolor='#F7F7F7', height=500, yaxis={'showticklabels': False}, yaxis_title=None, xaxis_title=None, title_text=&quot;&lt;b&gt;Experience&lt;/b&gt; Distribution&quot;, title_x=0.5, font=dict(family=&quot;Hiragino Kaku Gothic Pro, sans-serif&quot;, size=14, color='#000000'), title_font_size=35) default로 되어있는 그래프의 Layout을 수정. showlegend = False 래전드를 보여줄지 : 안보여줌 plot_bgcolor=’#F7F7F7’ margin=dict(pad=20)-dic에는 여러가지가 올 수 있는데 여기서는 dict(pad)를 사용 padding을 설정, 축과 그래프 사이의 패딩을 px 단위로 설정 Sets the amount of padding (in px) between the plotting area and the axis lines layout-margin paper_bgcolor=’#F7F7F7’ 배경색 설정 height=500 plot size 설정 yaxis={‘showticklabels’: False} y축의 showticklabels 설정 : 안함 yaxis_title=None, xaxis_title=None y축 제목, x축 제목 설정 : 없음 title_text=”Most Recommended Programming Language“ 제목 달기 &lt;b&gt; code는 bolde tag인듯. title_x=0.5, title_y=0.95, 제목의 위치 (상단 고정) font=dict(family=”Hiragino Kaku Gothic Pro, sans-serif”, size=17, color=’#000000’), title_font_size=35) title의 font 설정 (Default: “”Open Sans”, verdana, arial, sans-serif”) font 설정 family, color, size 설정 가능, title_fond도 함께 설정 가능 해 보임. plotly.graph_objects.Figure.update_layout (#9) : add_annotation()annotation의 경우 plot 안에 글을 집어 넣는 것. 설명을 추가 해 준다고 생각하면 쉽다. 어렵지도 않고, 같은 내용이므로 이전 posting을 첨부 annotation 12345678910111213141516171819fig.add_annotation(dict(font=dict(size=14), x=0.98, y=-0.24, showarrow=False, text=&quot;@miguelfzzz&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.add_annotation(dict(font=dict(size=12), x=-0.03, y=-0.24, showarrow=False, text=&quot;Source: 2021 Kaggle Machine Learning &amp; Data Science Survey&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() ref. plotly.graph_object Parameter var chart in plotly/en","link":"/2021/11/08/kgg/Kgg_plotly_bar/"},{"title":"kaggle :Donut Chart Graph (Q2)","text":"","link":"/2021/11/09/kgg/Kgg_plotly_pie/"},{"title":"kaggle :Plotly_Treemap (Q8)","text":"kaggle dictation (01) plotly.graph_objects as go : 를 이용한 Treemap 많은 계층 구조 데이트를 표현할때 적합. Ben Shneiderman에 의해 1990년도부터 출발 Treemap은 크기(count) 순서로 %에따라 공간을 차지하는 사각형으로 표현됨. 계층에서 동일한 수준에 속하는 각 사각형 집합은 데이터 테이블의 표현식 또는 컬럼을 표현. 계층에서 동일한 수준의 개별 사각형은 컬럼의 Index 언제 사용하면 좋을까- 많은 범주간의 부분과 전체를 시각화 하고 싶을때 - 범주 간의 정확한 비교 대신 큰 특징을 나타내고 싶을때 - 데이터가 계층을 이루고 있을때 사용한 Library12345678import pandas as pdimport numpy as npimport seaborn as snsimport plotly.express as pximport plotly.graph_objects as goimport warningswarnings.filterwarnings('ignore') 사실 이 부분에서 seaborn을 사용 했는지 잘 모르겠음. github에서 plotly가 동적 Livrary라 자꾸 오류가남. data importdata 원문data import 방법data는 Kaggle의 the-typical-kaggle-data-scientist-in-2021 data를 이용하였음. data 불러오기/합치기data를 표현 해 주기 위해 컴퓨터가 읽을 수 있는 형태로 가공. 12df = pd.read_csv('../input/kaggle-survey-2021/kaggle_survey_2021_responses.csv')df = df.iloc[1:, :] df : data frame에 file 연동.df.iloc[1:, :] : 1행부터 끝까지 건너뛰기 없이 선택 행번호(row number)로 선택하는 방법 (.iloc) label이나 조건표현으로 선택하는 방법 (.loc) Ref. loc, iloc을 이용한 행 선택 data encoding (Feature Engineering)사실 이 부분이 feature Engineering에 해당하는 부분인지 잘 모르겠다. 이 부분은 data를 computer로 자동화하여 계산, 동적 그래프를 만들기 위한 부분. ###Feature Engineering####data frame 설정 123456789101112recommend_leng = ( df['Q8'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'Lenguage', 'Q8':'Count'}) .sort_values(by=['Count'], ascending=False) )df['Q8'].head()recommend_leng dataframe[Q8] : data를 가공하여 분석 할 예정 pd.value_counts() Q8의 data counting, 중복된 data 를 counting 하여 수를 나타냄. pd.frame() dataframe으로 표의 형태를 잡아준다. pd.reset_Index() Q8의 Index reset, 원본 data에는 영향을 주지 않으면서 새로운 Index 생성 Reset_parameter/Ko pd.rename() columns에 Index를 붙여 호출 하기 위해 이름을 바꿔줌 index는 Lenguage로, count는 Q8로 pd.sort_values() count 값으로 정렬 by=[‘Count’], ascending = false by에 option 기준, (오름차순 = F )= 내림차순으로 정렬 앞으로 사용 할 색상을 미리 지정 한 눈에 보기 편함. 123456colors = ['#033351',] * 13colors[0] = '#5abbf9'colors[1] = '#066eb0'colors[2] = '#044a77'colors[3] = '#043e64'colors[4] = '#043e64' 아직 잘 모르겠는 부분은 왜 colors = [‘#NNNNNN’, ] 이 부분과 *13 이부분 Treemap 생성123456fig = go.Figure(go.Treemap( labels = recommend_leng['Lenguage'], values = recommend_leng['Count'], parents = ['']*recommend_leng.shape[0], textinfo = &quot;percent root+label+value+text&quot;, )) fig 생성 import plotly.graph_objects as goplotly의 graph_objects 를 이용하여 객체 생성. Treemap의 parameter labels : values 이름 values : 값 parents : ??? textinfo = 표시형식 fig.update_traces(hovertemplate)12fig.update_traces(hovertemplate='&lt;b&gt;Lenguage&lt;/b&gt;: %{label}&lt;br&gt;&lt;extra&gt;&lt;/extra&gt;'+ '&lt;b&gt;Count&lt;/b&gt;: %{value}') fig.update_traces() 그래프 위에 캡션 다는 기능 Perform a property update operation on all traces that satisfy the specified selection criteria 지정된 선택 기준을 충족하는 모든 추적에 대해 속성 업데이트 작업 수행 (?? 전혀 모르겠군 !) hoverinfo : 마우스 오버시 나타나는 추적정보 hovertemplate : 커서를 가까이 대면 나오는 창을 렌더링하는데 사용되는 Temp 변수 : %{variable} (변수의 형식을 지정) 숫자 : %{d3-format} price : %{yL$.2f} hovertemplate/en hoverTemp.para/ko fig.update_layout()fig의 layout을 설정. hoverTemp까지 설정된 treemap. fig.update_layout()을 사용하여 layout을 변경 해 보자. 1234567891011121314fig.update_layout(showlegend=False, treemapcolorway = colors, margin=dict(pad=20), paper_bgcolor='#F7F7F7', plot_bgcolor='#F7F7F7', height=600, yaxis={'showticklabels': False}, yaxis_title=None, xaxis_title=None, title_text=&quot;Most Recommended &lt;b&gt;Programming Language&lt;/b&gt;&quot;, title_x=0.5, title_y=0.95, font=dict(family=&quot;Hiragino Kaku Gothic Pro, sans-serif&quot;, size=17, color='#000000'), title_font_size=35) showlegend = False 래전드를 보여줄지 : 안보여줌 treemapcolorway = colors 위에서 정의 해 준 colors가 13개였는데 여기 계층도 13개임 아마도 light 부터 deep으로 색이 정해지는듯. margin=dict(pad=20)-dic에는 여러가지가 올 수 있는데 여기서는 dict(pad)를 사용 padding을 설정, 축과 그래프 사이의 패딩을 px 단위로 설정 Sets the amount of padding (in px) between the plotting area and the axis lines layout-margin paper_bgcolor=’#F7F7F7’ 배경색 설정 plot_bgcolor=’#F7F7F7’ 설정 바꿔 보았으나 안보임 height=600 plot size 설정 yaxis={‘showticklabels’: False} y축의 showticklabels 설정 : 안함 yaxis_title=None, xaxis_title=None y축 제목, x축 제목 설정 : 없음 title_text=”Most Recommended Programming Language“ 제목 달기 &lt;b&gt; code는 bolde tag인듯. title_x=0.5, title_y=0.95, 제목의 위치 (상단 고정) font=dict(family=”Hiragino Kaku Gothic Pro, sans-serif”, size=17, color=’#000000’), title_font_size=35) title의 font 설정 (Default: “”Open Sans”, verdana, arial, sans-serif”) font 설정 family, color, size 설정 가능, title_fond도 함께 설정 가능 해 보임. fig Information 추가fig.add_annotation() _1플롯에 메모를 남길 수 있는것. 코멘트나 copy-Right 같은걸 남기는듯 -[plotly-annotation/ko.] (https://soohee410.github.io/plotly_annotation) 12345678fig.add_annotation(dict(font=dict(size=14), x=0.96, y=-0.14, showarrow=False, text=&quot;@miguelfzzz&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;)) dict = dictionary A list or tuple of dicts of string/value properties where:The ‘type’ property specifies the trace type 여기서 dict는 fig객체 즉 plotly.graph_objects.Figure 에서 상응하는 정보(font, size등) 변수를 가져와 대응 시켜 주는 역할을 한다. plot 아래에 보면 “@miguelfzzz”이라는 글자가 보이는데 이것을 설정 한 것. showarrow=False, 화살표등을 남길 수 있는데 이 Graph에선 false xref=”paper”, yref=”paper” 어느 부분 (plot or paper)에 표시 할 것인지 나머지는 말 안해도 이제는 알 수 있기 때문에 생략. fig.add_annotation() _212345678fig.add_annotation(dict(font=dict(size=12), x=0.01, y=-0.14, showarrow=False, text=&quot;Source: 2021 Kaggle Machine Learning &amp; Data Science Survey&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;)) fig.show() 내보내기1fig.show() fig.show()로 마무리 해 주면 된다. 이건 java의 return과 같은 느낌인듯. Plotly Treemap/en. layout/en.","link":"/2021/11/08/kgg/Kgg_plotly_treemap/"},{"title":"[Kgg]Tabular Playground Series (Dec. 2021)","text":"origin of dictation 이어지는 포스팅 :Kgg_TPS 02 Description of competitiondata overview Kaggle에서 매달 1일에 data scientists의 Featured competitions을 위해 beginner- friendly로 제공하는 대회 대회의 목적 : a fun, and approachable 를 위해 anyone에게 tabular dataset 제공. dataset : CTGAN에서 만들어진 숲 토양 타입 예측 대회 data The study area includes four wilderness areas located inthe Roosevelt National Forest of northern Colorado.Each observation is a 30m x 30m patch.You are asked to predict an integer classification for the forest cover type(FCT). The seven types are: 1 - Spruce/Fir 2 - Lodgepole Pine 3 - Ponderosa Pine 4 - Cottonwood/Willow 5 - Aspen 6 - Douglas-fir 7 - Krummholz The training set (15120 observations) contains both features and the Cover_Type.The test set contains only the features.You must predict the Cover_Type for every row in the test set (565892 observations). Data Fields Elevation - 미터 단위 고도 Aspect - 방위각의 종횡비 (위치) Slope - 경사 기울기 Horizontal_Distance_To_Hydrology - 해수면까지의 수평거리 Vertical_Distance_To_Hydrology - 해수면까지의 수직거리 Horizontal_Distance_To_Roadways - 도로와의 수평 거리 Hillshade_9am (0 to 255 index) - 여름, 오전 9시 Hillshade Hillshade_Noon (0 to 255 index) - 여름, 정오 Hillshade Hillshade_3pm (0 to 255 index) - 여름, 오후 3시 Hillshade Horizontal_Distance_To_Fire_Points - 산불 발화점까지 수평거리 Wilderness_Area : 야생지역 - 4 개의 columns (토양 유형 지정) + 0 = 없음 + 1 = 있음 Soil_Type : 토양 유형 지정 - 40 개의 columns + 0 = 없음 + 1 = 있음 Cover_Type FCT 지정br&gt; - 7 개 columns + 0 = 없음 + 1 = 있음 The wilderness areas are: 1 - Rawah Wilderness Area 2 - Neota Wilderness Area 3 - Comanche Peak Wilderness Area 4 - Cache la Poudre Wilderness Area The soil types are: 1 Cathedral family - Rock outcrop complex, extremely stony. 2 Vanet - Ratake families complex, very stony. 3 Haploborolis - Rock outcrop complex, rubbly. 4 Ratake family - Rock outcrop complex, rubbly. 5 Vanet family - Rock outcrop complex complex, rubbly. 6 Vanet - Wetmore families - Rock outcrop complex, stony. 7 Gothic family. Na 8 Supervisor - Limber families complex. 9 Troutville family, very stony. 10 Bullwark - Catamount families - Rock outcrop complex, rubbly. 11 Bullwark - Catamount families - Rock land complex, rubbly. 12 Legault family - Rock land complex, stony. 13 Catamount family - Rock land - Bullwark family complex, rubbly. 14 Pachic Argiborolis - Aquolis complex. 15 unspecified in the USFS Soil and ELU Survey. (Na) 16 Cryaquolis - Cryoborolis complex. 17 Gateview family - Cryaquolis complex. 18 Rogert family, very stony. 19 Typic Cryaquolis - Borohemists complex. 20 Typic Cryaquepts - Typic Cryaquolls complex. 21 Typic Cryaquolls - Leighcan family, till substratum complex. 22 Leighcan family, till substratum, extremely bouldery. 23 Leighcan family, till substratum - Typic Cryaquolls complex. 24 Leighcan family, extremely stony. 25 Leighcan family, warm, extremely stony. 26 Granile - Catamount families complex, very stony. 27 Leighcan family, warm - Rock outcrop complex, extremely stony. 28 Leighcan family - Rock outcrop complex, extremely stony. 29 Como - Legault families complex, extremely stony. 30 Como family - Rock land - Legault family complex, extremely stony. 31 Leighcan - Catamount families complex, extremely stony. 32 Catamount family - Rock outcrop - Leighcan family complex, extremely stony. 33 Leighcan - Catamount families - Rock outcrop complex, extremely stony. 34 Cryorthents - Rock land complex, extremely stony. 35 Cryumbrepts - Rock outcrop - Cryaquepts complex. 36 Bross family - Rock land - Cryumbrepts complex, extremely stony. 37 Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony. 38 Leighcan - Moran families - Cryaquolls complex, extremely stony. 39 Moran family - Cryorthents - Leighcan family complex, extremely stony. 40 Moran family - Cryorthents - Rock land complex, extremely stony. 경사(Slope) : 어떤 지점의 지반이 수평을 기준으로 몇도 기울어져 있는가 θ(theta) 로 표현 각이 클 수록 지반의 경사가 급하고 각이 0이면 평편한 지반 향(Aspect): 지반의 경사면이 어디를 향하는가 북: 0도, 동: 90도, 남: 180도, 서: 270도. 완전히 평편할 경우 GIS 시스템마다 다른 값, Null 가능, (-1과 같은 값이 적당) Ref. Evaluation 각각의 ID 를 cover type 과 Matching하여 file format 형태를 만들어 제출 하면 됩니다.","link":"/2021/12/20/kgg/Tabular_Playground_Series_Dec(2021)/"},{"title":"kaggle study","text":"Sumarry Demographics &amp; Geographics (Q1) Age-bar (Q2) Gender-pie (Q3)countries-scatter+line Education &amp; Occupation (Q4) Age-bar_h (Q5) Role-bar_h (Q20) Industry-bar_h knowledge &amp; skills (Q6) Experience : 52% 넘는 응답이 3년이상 코딩과 프로그래밍을 했다. -bar vertical (위) (Q17) Algorithms : Linear or Logistic Regression 55% 과 Decision tree or Random Forests, respectively 66% 사용. - bar horizon (옆) (Q7) Languages : python 84%, SQL 41% 사용 -bar horizon (Q8) Recommend_Leng: programming에 추천하는 언어는 81%가 python -Treemap (Q9) F_EG /w Q7 가장 많이 쓰는 IDE : jupyter Notebook26.2%, VSCODE 13.92% - bar horizon Platforms &amp; Media4. Platforms &amp; Media (Q11) Platform: 많이 쓰는 컴퓨터 플랫폼은 랩탑이 66% -Scatter + line (Q27_A)cloud_platform: 아마존 14%, 구글클라우드 12%, 마쏘 아줠 9% - bar horizon (Q40) courses: DS들이 많이 쓰는 course 플랫폼 Coursera 20, Kaggle 18%-Treemap (Q42) media : DS topic report를 위해 많이 쓰는 media는 kaggle 44%, youTube 40%, blog 31% -scatter + line Takeaways : typical Kaggle DS in 2021 :HTML picture n line n text 123456789101112131415161718# This Python 3 environment, PKG Loadimport numpy as np # linear algebraimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)# Input data files : &quot;../input/&quot; # Running : Shift+Enterimport osfor dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename))# ~ 20GB ,/kaggle/temp/ /kaggle/input/kaggle-survey-2021/kaggle_survey_2021_responses.csv /kaggle/input/kaggle-survey-2021/supplementary_data/kaggle_survey_2021_methodology.pdf /kaggle/input/kaggle-survey-2021/supplementary_data/kaggle_survey_2021_answer_choices.pdf The Typical Kaggle Data Scientist in 2021 Remade By @YoonHwa-P HTML code는 Markdown 형식으로 넣을 수 있게 해 준다. scr : 구글 팟케스트에서 바로 연결하여 사용. 1234567891011import pandas as pdimport numpy as npimport seaborn as snsimport plotly.express as pximport plotly.graph_objects as goimport warningswarnings.filterwarnings('ignore') 시각화, 계산을 위해 Pandas, Numpy, seaborn을 이용 할 것이고, 동적보드를 만들기 위해 plotly를 이용 하였다. plotly 중에서 Express와 Graph_objects를 가져와서 사용 할 예정인듯. plotly-express The Plotly Express API in general offers the following features: Every PX function returns a** plotly.graph_objects.Figure object**, so you can edit it using all the same methods like update_layout and add_trace. input으로 Express를 사용 한다면 Graph_objects가 동적 plotly 를 만드는 것 같다. : update 하거나 trace를 가능 하게 하는듯. 실제 필사할 data에서는 어떤 data가 있는지 확인 해 보지 않았지만,나는 배우는 입장이니 어떤 data가 있는지, 어떤 head가 있는지 확인 해 보도록 한다. 맨 위에 가보면 Note가 생성 될때 /kaggle/input/kaggle-survey-2021/kaggle_survey_2021_responses.csv /kaggle/input/kaggle-survey-2021/supplementary_data/kaggle_survey_2021_methodology.pdf /kaggle/input/kaggle-survey-2021/supplementary_data/kaggle_survey_2021_answer_choices.pdf 위와같은 file dir을 알려준다. 이제, pandas로 이 files를 로딩 시켜 주면된다. Ref. Kaggle활용.국문 12345678df = pd.read_csv(&quot;../input/kaggle-survey-2021/kaggle_survey_2021_responses.csv&quot;)df = df.iloc[1:, :] #이건 왜 선택 해 놓은 것일까요?df.head()df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 25973 entries, 1 to 25973 Columns: 369 entries, Time from Start to Finish (seconds) to Q38_B_OTHER dtypes: object(369) memory usage: 73.1+ MB df에 pd.read_csv로 csv file을 읽어 옵니다. 역시 pd인 df객체에 iloc을 이용하여 [ 1행부터 : , : ] iloc를 선택 해 놓았다. 행번호(row number)로 선택하는 방법 (.iloc) label이나 조건표현으로 선택하는 방법 (.loc) Ref. loc를 이용한 행 선택 0. Introduction This notebook will explore the fascinating results obtained from the survey conducted by Kaggle in September 2021. Over 25,000 data scientists and ML engineers participated, providing information on their backgrounds and experience in their occupations. To increase readability, this report is divided into four sections: Demographics & Geographics Education & Occupation Knowledge & Skills Platforms & Media Introduction이 노트북은 25000 data scientist들과 ML Engineer들의 kaggle에서 경험 한것을 조사한 data를 매력적인 결과로 탐험 하게 될 것이다.(대충) ; Introduction 에서 이 notebook의 성격, data의 간간한 정보, 목차 등을 설명. 모든 글은 Markdown을 이용한 css 로 작성 된 것 같다. 혹시,css에 대하여 더 알아보고 싶으면, 이 문서를 참조 하자.","link":"/2021/11/07/kgg/kaggle_CC/"},{"title":"Scatter with bar","text":"#Kgg 대회 준비 Scatter와 bar가 함께 있는 Graph를 그려 보았다.1234567891011121314151617181920212223242526272829fig = go.Figure()y=[len(df17_Ea),len(df18_Ea), len(df19_Ea),len(df20_Ea),len(df21_Ea)]fig.add_trace(go.Bar(x=years, y=y, base=0, marker_color='#D9946C', yaxis = &quot;y1&quot;, name='East Asia', text= percent, texttemplate='%{text} %', textposition='outside', hovertemplate='&lt;b&gt;KaggleUser&lt;/b&gt;: %{x}&lt;br&gt;'+ '&lt;b&gt;Count&lt;/b&gt;: %{y}', textfont_size=14 ))fig.add_trace( go.Scatter(name = &quot;World&quot;, x=years, y=[len(df17), len(df18), len(df19), len(df20), len(df21)], marker_color='#88BFBA', mode = 'lines+markers', # please check option here yaxis = &quot;y2&quot;))fig.update_layout(yaxis = dict(title = &quot;KaggleUser in World&quot;, showgrid = False, range=[0, len(df21_Ea)*1.2]), yaxis2 = dict(title = &quot;KaggleUser in East Asia&quot;, overlaying = &quot;y1&quot;, side = &quot;right&quot;, showgrid = False, zeroline = False, range=[0, len(df21)*1.2]), # This code solves the different zero set but with same zero values. template = &quot;plotly_white&quot;, height=500, width=700,)fig.show() age data , 연도별로12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394#age data 전처리# age 뽑아오기Age21_W = df21.loc[:,['Q3','Q1', 'year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'age'}).fillna('etc')Age20_W = df20.loc[:,['Q3','Q1','year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'age'}).fillna('etc')Age19_W = df19.loc[:,['Q3','Q1','year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'age'}).fillna('etc')Age18_W = df18.loc[:,['Q3','Q2','year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q2':'age'}).fillna('etc')#data 정제(한꺼번에 이름바꾸기)Age5y_W= pd.concat([Age21_W, Age20_W, Age19_W, Age18_W])Age5y_W= (Age5y_W.replace(['60-69', '70+', '70-79', '80+'], '60+') .replace(['22-24', '25-29'], '22-29') .replace(['30-34', '35-39'], '30-39') .replace(['40-44', '45-49'], '40-49') .replace(['50-54', '55-59'], '50-59') .groupby(['year', 'age']) .size() .reset_index() .rename(columns = {0:&quot;Count&quot;}))#percent data 넣기Age21_percent_W = Age5y_W[Age5y_W['year'] == &quot;2021&quot;].reset_index(drop = True)Age21_percent_W['percentage'] = Age21_percent_W[&quot;Count&quot;] / Age21_percent_W[&quot;Count&quot;].sum()Age21_percent_W['%'] = np.round(Age21_percent_W['percentage'] * 100, 1)Age20_percent_W = Age5y_W[Age5y_W['year'] == &quot;2020&quot;].reset_index(drop = True)Age20_percent_W['percentage'] = Age20_percent_W[&quot;Count&quot;] / Age20_percent_W[&quot;Count&quot;].sum()Age20_percent_W['%'] = np.round(Age20_percent_W['percentage'] * 100, 1)Age19_percent_W = Age5y_W[Age5y_W['year'] == &quot;2019&quot;].reset_index(drop = True)Age19_percent_W['percentage'] = Age19_percent_W[&quot;Count&quot;] / Age19_percent_W[&quot;Count&quot;].sum()Age19_percent_W['%'] = np.round(Age19_percent_W['percentage'] * 100, 1)Age18_percent_W = Age5y_W[Age5y_W['year'] == &quot;2018&quot;].reset_index(drop = True)Age18_percent_W['percentage'] = Age18_percent_W[&quot;Count&quot;] / Age18_percent_W[&quot;Count&quot;].sum()Age18_percent_W['%'] = np.round(Age18_percent_W['percentage'] * 100, 1)#data 완성Age5y_percent_W = pd.concat([Age18_percent_W, Age19_percent_W, Age20_percent_W, Age21_percent_W], ignore_index = True)Age5y_percent_W= pd.pivot(Age5y_percent_W, index = &quot;year&quot;, columns = 'age', values = &quot;%&quot;).reset_index()Age5y_percent_W#age data 전처리# age 뽑아오기Age21 = df21_Ea.loc[:,['Q3','Q1', 'year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'age'}).fillna('etc')Age20 = df20_Ea.loc[:,['Q3','Q1','year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'age'}).fillna('etc')Age19 = df19_Ea.loc[:,['Q3','Q1','year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'age'}).fillna('etc')Age18 = df18_Ea.loc[:,['Q3','Q2','year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q2':'age'}).fillna('etc')#data 정제(한꺼번에 이름바꾸기)Age5y= pd.concat([Age21, Age20, Age19, Age18])Age5y= (Age5y.replace(['60-69', '70+', '70-79', '80+'], '60+') .replace(['22-24', '25-29'], '22-29') .replace(['30-34', '35-39'], '30-39') .replace(['40-44', '45-49'], '40-49') .replace(['50-54', '55-59'], '50-59') .groupby(['year', 'age']) .size() .reset_index() .rename(columns = {0:&quot;Count&quot;}))#percent data 넣기Age21_percent = Age5y[Age5y['year'] == &quot;2021&quot;].reset_index(drop = True)Age21_percent['percentage'] = Age21_percent[&quot;Count&quot;] / Age21_percent[&quot;Count&quot;].sum()Age21_percent['%'] = np.round(Age21_percent['percentage'] * 100, 1)Age21_percentAge20_percent = Age5y[Age5y['year'] == &quot;2020&quot;].reset_index(drop = True)Age20_percent['percentage'] = Age20_percent[&quot;Count&quot;] / Age20_percent[&quot;Count&quot;].sum()Age20_percent['%'] = np.round(Age20_percent['percentage'] * 100, 1)Age20_percentAge19_percent = Age5y[Age5y['year'] == &quot;2019&quot;].reset_index(drop = True)Age19_percent['percentage'] = Age19_percent[&quot;Count&quot;] / Age19_percent[&quot;Count&quot;].sum()Age19_percent['%'] = np.round(Age19_percent['percentage'] * 100, 1)Age19_percentAge18_percent = Age5y[Age5y['year'] == &quot;2018&quot;].reset_index(drop = True)Age18_percent['percentage'] = Age18_percent[&quot;Count&quot;] / Age18_percent[&quot;Count&quot;].sum()Age18_percent['%'] = np.round(Age18_percent['percentage'] * 100, 1)Age18_percent#data 완성Age5y_percent = pd.concat([Age18_percent, Age19_percent, Age20_percent, Age21_percent], ignore_index = True)Age5y_percent= pd.pivot(Age5y_percent, index = &quot;year&quot;, columns = 'age', values = &quot;%&quot;).reset_index()Age5y_percent Graph 그리기 _1_World12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364Age5y_percent_order = Age5y_percent_W['year'].tolist()Age5y_order = Age5y_W['age'].unique().tolist()fig = go.Figure()fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent_W['18-21'].tolist(), mode = &quot;lines&quot;, name = '18-21', line = dict(width = 1), stackgroup = &quot;one&quot;))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent_W['22-29'].tolist(), mode = &quot;lines&quot;, name = &quot;20s&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent_W['30-39'].tolist(), mode = &quot;lines&quot;, name = &quot;30s&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent_W['40-49'].tolist(), mode = &quot;lines&quot;, name = &quot;40s&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent_W['50-59'].tolist(), mode = &quot;lines&quot;, name = &quot;50s&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent_W['60+'].tolist(), mode = &quot;lines&quot;, name = &quot;60s&lt;&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;))fig.update_layout(yaxis_range = (0, 100), height=500, width=600, title_text=&quot;&lt;b&gt;in world 나이의 변화&lt;/b&gt;&quot;, title_x=0.5)fig.show() Graph 그리기 _1_in East Asia12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364Age5y_percent_order = Age5y_percent['year'].tolist()Age5y_order = Age5y['age'].unique().tolist()fig = go.Figure()fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent['18-21'].tolist(), mode = &quot;lines&quot;, name = '18-21', line = dict(width = 1), stackgroup = &quot;one&quot;))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent['22-29'].tolist(), mode = &quot;lines&quot;, name = &quot;20s&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent['30-39'].tolist(), mode = &quot;lines&quot;, name = &quot;30s&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent['40-49'].tolist(), mode = &quot;lines&quot;, name = &quot;40s&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent['50-59'].tolist(), mode = &quot;lines&quot;, name = &quot;50s&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent['60+'].tolist(), mode = &quot;lines&quot;, name = &quot;60s&lt;&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;))fig.update_layout(yaxis_range = (0, 100), height=500, width=600, title_text=&quot;&lt;b&gt;East Asia 나이의 변화&lt;/b&gt;&quot;, title_x=0.5)fig.show() 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# 연도별 나이 df21Age_Ea = df21_Ea.loc[:,['Q3','Q1']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'2021'}).fillna('etc')# 연령-지역 %dfKo_Age21= df21Age_Ea[df21Age_Ea['East_Asia']=='South Korea']dfKo_Age21_per=dfKo_Age21['2021'].value_counts().to_frame().reset_index()dfKo_Age21_per['South Korea']=((dfKo_Age21_per['2021'] / len(dfKo_Age21))*100).round(2)dfTw_Age21= df21Age_Ea[df21Age_Ea['East_Asia']=='Taiwan']dfTw_Age21_per=dfTw_Age21['2021'].value_counts().to_frame().reset_index()dfTw_Age21_per['Taiwan']=((dfTw_Age21_per['2021'] / len(dfTw_Age21))*100).round(2)dfTw_Age21_perdfCh_Age21= df21Age_Ea[df21Age_Ea['East_Asia']=='China']dfCh_Age21_per=dfCh_Age21['2021'].value_counts().to_frame().reset_index()dfCh_Age21_per['China']=((dfCh_Age21_per['2021'] / len(dfCh_Age21))*100).round(2)dfCh_Age21_perdf21Age_Ea.head()dfJp_Age21= df21Age_Ea[df21Age_Ea['East_Asia']=='Japan']dfJp_Age21_per=dfJp_Age21['2021'].value_counts().to_frame().reset_index()dfJp_Age21_per['Japan']=((dfJp_Age21_per['2021'] / len(dfJp_Age21))*100).round(2)dfJp_Age21_per#g 그리기(heatMap)merge1= pd.merge(dfKo_Age21_per,dfTw_Age21_per, on='index', how='outer')merge2= pd.merge(dfCh_Age21_per,dfJp_Age21_per, on='index', how='outer')merge= pd.merge(merge1,merge2, on='index', how='outer').fillna(0).sort_values(by=['index'],ascending=True)merge.iloc[:,[2,4,6,8]]merge.iloc[:,[2,4,6,8]].to_numpy()fig = go.Figure(data=go.Heatmap( z=merge.iloc[:,[2,4,6,8]].to_numpy(), x=['South Korea','Taiwan','China','Japan'], y=merge.sort_values(by=['index'],ascending=True)['index'].tolist(), hoverongaps = False, opacity=1.0, xgap=2.5, ygap=2.5, colorscale='orrd'), )fig.update_layout( height=500, width=600, title_text=&quot;&lt;b&gt;East Asia 나이 2021&lt;/b&gt;&quot;, title_x=0.5)fig.show() 123456789101112131415161718192021222324252627282930313233343536#데이터 전처리df21_Ea_degree=(df21_Ea['Q4'].replace(['No formal education past high school', 'Some college/university study without earning a bachelor’s degree'],'~college') .replace(['Doctoral degree', 'Professional doctorate'],'Doctoral degree~').value_counts().to_frame().rename(columns={'Q4':'2021'}))df20_Ea_degree=(df20_Ea['Q4'].replace(['No formal education past high school', 'Some college/university study without earning a bachelor’s degree'],'~college') .replace(['Doctoral degree', 'Professional doctorate'],'Doctoral degree~').value_counts().to_frame().rename(columns={'Q4':'2020'}))df19_Ea_degree=(df19_Ea['Q4'].replace(['No formal education past high school', 'Some college/university study without earning a bachelor’s degree'],'~college') .replace(['Doctoral degree', 'Professional doctorate'],'Doctoral degree~').value_counts().to_frame().rename(columns={'Q4':'2019'}))df18_Ea_degree=(df18_Ea['Q4'].replace(['No formal education past high school', 'Some college/university study without earning a bachelor’s degree'],'~college') .replace(['Doctoral degree', 'Professional doctorate'],'Doctoral degree~').value_counts().to_frame().rename(columns={'Q4':'2018'}))df17_Ea_degree=(df17_Ea['FormalEducation'].replace(['No formal education past high school', 'Some college/university study without earning a bachelor’s degree'],'~college') .replace(['Doctoral degree', 'Professional doctorate'],'Doctoral degree~') .value_counts().to_frame() .rename(columns={'FormalEducation':'2017'}) .rename(index = {'I did not complete any formal education past high school':'No formal education past high school'}))concat1 = pd.concat([df21_Ea_degree,df20_Ea_degree],axis=1, join='outer') concat2 = pd.concat([df19_Ea_degree,df18_Ea_degree],axis=1, join='outer') concat3 = pd.concat([concat1,concat2],axis=1, join='outer') df21_Ea_degree_yearly_=concat3.join(df17_Ea_degree).fillna(0).transpose() #.transpose() 행 열 바꾸기df21_Ea_degree_yearly=df21_Ea_degree_yearly_.stack().to_frame().reset_index().rename(columns={'level_0':'year','level_1':'degree',0:'value'})df21_Ea_degree_yearly#그래프 그리기fig = px.sunburst(df21_Ea_degree_yearly, path=['year','degree'], values=df21_Ea_degree_yearly['value'].tolist())fig.show()","link":"/2021/11/22/kgg/kgg_EastAsia_varH/"},{"title":"Auto Machine Learning by pycaret(01)","text":"AutoMachineLearning by pycaretpycaret pycaret으로 autoML 하기 low-code machine learning library PyCaret 2.0 ver. 분석가가 가야 하는 최종 도착지 머신러닝 + operation (운영) : 배포 -&gt; MLflow, Airflow, Kubeflow… gitHub and pycaret pycaret install123!pip install pycaret# !pip install pycaret==2.0 Collecting pycaret Downloading pycaret-2.3.5-py3-none-any.whl (288 kB) |████████████████████████████████| 288 kB 5.4 MB/sCollecting lightgbm&gt;=2.3.1 Downloading lightgbm-3.3.1-py3-none-manylinux1_x86_64.whl (2.0 MB) |████████████████████████████████| 2.0 MB 54.5 MB/sCollecting pyod Downloading pyod-0.9.5.tar.gz (113 kB) |████████████████████████████████| 113 kB 67.4 MB/sRequirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (from pycaret) (0.15.3)Requirement already satisfied: yellowbrick&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from pycaret) (1.3.post1)Collecting Boruta Downloading Boruta-0.3-py3-none-any.whl (56 kB) |████████████████████████████████| 56 kB 4.6 MB/sCollecting pyLDAvis Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB) |████████████████████████████████| 1.7 MB 63.6 MB/s Installing build dependencies … done Getting requirements to build wheel … done Installing backend dependencies … done Preparing wheel metadata … doneRequirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from pycaret) (0.11.2)Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pycaret) (3.2.5)Collecting imbalanced-learn==0.7.0 Downloading imbalanced_learn-0.7.0-py3-none-any.whl (167 kB) |████████████████████████████████| 167 kB 65.5 MB/sRequirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (from pycaret) (1.19.5)Collecting kmodes&gt;=0.10.1 Downloading kmodes-0.11.1-py2.py3-none-any.whl (19 kB)Requirement already satisfied: spacy&lt;2.4.0 in /usr/local/lib/python3.7/dist-packages (from pycaret) (2.2.4)Collecting umap-learn Downloading umap-learn-0.5.2.tar.gz (86 kB) |████████████████████████████████| 86 kB 4.8 MB/sRequirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from pycaret) (5.5.0)Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from pycaret) (3.2.2)Requirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (from pycaret) (1.5.0)Requirement already satisfied: cufflinks&gt;=0.17.0 in /usr/local/lib/python3.7/dist-packages (from pycaret) (0.17.3)Collecting mlflow Downloading mlflow-1.22.0-py3-none-any.whl (15.5 MB) |████████████████████████████████| 15.5 MB 68.5 MB/sCollecting mlxtend&gt;=0.17.0 Downloading mlxtend-0.19.0-py2.py3-none-any.whl (1.3 MB) |████████████████████████████████| 1.3 MB 66.9 MB/sRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pycaret) (1.1.5)Collecting scikit-plot Downloading scikit_plot-0.3.7-py3-none-any.whl (33 kB)Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pycaret) (1.1.0)Requirement already satisfied: plotly&gt;=4.4.1 in /usr/local/lib/python3.7/dist-packages (from pycaret) (4.4.1)Collecting scikit-learn==0.23.2 Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB) |████████████████████████████████| 6.8 MB 37.1 MB/sRequirement already satisfied: gensim&lt;4.0.0 in /usr/local/lib/python3.7/dist-packages (from pycaret) (3.6.0)Collecting pandas-profiling&gt;=2.8.0 Downloading pandas_profiling-3.1.0-py2.py3-none-any.whl (261 kB) |████████████████████████████████| 261 kB 60.5 MB/sRequirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from pycaret) (7.6.5)Requirement already satisfied: scipy&lt;=1.5.4 in /usr/local/lib/python3.7/dist-packages (from pycaret) (1.4.1)Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2-&gt;pycaret) (3.0.0)Requirement already satisfied: setuptools&gt;=34.4.1 in /usr/local/lib/python3.7/dist-packages (from cufflinks&gt;=0.17.0-&gt;pycaret) (57.4.0)Requirement already satisfied: colorlover&gt;=0.2.1 in /usr/local/lib/python3.7/dist-packages (from cufflinks&gt;=0.17.0-&gt;pycaret) (0.3.0)Requirement already satisfied: six&gt;=1.9.0 in /usr/local/lib/python3.7/dist-packages (from cufflinks&gt;=0.17.0-&gt;pycaret) (1.15.0)Requirement already satisfied: smart-open&gt;=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim&lt;4.0.0-&gt;pycaret) (5.2.1)Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (0.7.5)Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (4.8.0)Requirement already satisfied: traitlets&gt;=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (5.1.1)Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (2.6.1)Requirement already satisfied: simplegeneric&gt;0.8 in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (0.8.1)Requirement already satisfied: prompt-toolkit&lt;2.0.0,&gt;=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (1.0.18)Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (4.4.2)Requirement already satisfied: nbformat&gt;=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;pycaret) (5.1.3)Requirement already satisfied: widgetsnbextension=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;pycaret) (3.5.2)Requirement already satisfied: ipython-genutils=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;pycaret) (0.2.0)Requirement already satisfied: ipykernel&gt;=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;pycaret) (4.10.1)Requirement already satisfied: jupyterlab-widgets&gt;=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;pycaret) (1.0.2)Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets-&gt;pycaret) (5.3.5)Requirement already satisfied: tornado&gt;=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets-&gt;pycaret) (5.1.1)Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm&gt;=2.3.1-&gt;pycaret) (0.37.0)Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;pycaret) (3.0.6)Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;pycaret) (2.8.2)Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;pycaret) (1.3.2)Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;pycaret) (0.11.0)Requirement already satisfied: jsonschema!=2.5.0,&gt;=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets-&gt;pycaret) (2.6.0)Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets-&gt;pycaret) (4.9.1)Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas-&gt;pycaret) (2018.9)Collecting visions[type_image_path]==0.7.4 Downloading visions-0.7.4-py3-none-any.whl (102 kB) |████████████████████████████████| 102 kB 12.4 MB/sCollecting pydantic&gt;=1.8.1 Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB) |████████████████████████████████| 10.1 MB 24.8 MB/sCollecting tangled-up-in-unicode==0.1.0 Downloading tangled_up_in_unicode-0.1.0-py3-none-any.whl (3.1 MB) |████████████████████████████████| 3.1 MB 22.1 MB/sCollecting joblib Downloading joblib-1.0.1-py3-none-any.whl (303 kB) |████████████████████████████████| 303 kB 60.4 MB/sCollecting requests&gt;=2.24.0 Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB) |████████████████████████████████| 62 kB 805 kB/sRequirement already satisfied: tqdm&gt;=4.48.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling&gt;=2.8.0-&gt;pycaret) (4.62.3)Collecting PyYAML&gt;=5.0.0 Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB) |████████████████████████████████| 596 kB 42.5 MB/sRequirement already satisfied: missingno&gt;=0.4.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling&gt;=2.8.0-&gt;pycaret) (0.5.0)Requirement already satisfied: markupsafe=2.0.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling&gt;=2.8.0-&gt;pycaret) (2.0.1)Collecting htmlmin&gt;=0.1.12 Downloading htmlmin-0.1.12.tar.gz (19 kB)Collecting multimethod&gt;=1.4 Downloading multimethod-1.6-py3-none-any.whl (9.4 kB)Collecting phik&gt;=0.11.1 Downloading phik-0.12.0-cp37-cp37m-manylinux2010_x86_64.whl (675 kB) |████████████████████████████████| 675 kB 41.5 MB/sRequirement already satisfied: jinja2&gt;=2.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling&gt;=2.8.0-&gt;pycaret) (2.11.3)Requirement already satisfied: networkx&gt;=2.4 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (2.6.3)Requirement already satisfied: attrs&gt;=19.3.0 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (21.2.0)Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (7.1.2)Collecting imagehash Downloading ImageHash-4.2.1.tar.gz (812 kB) |████████████████████████████████| 812 kB 37.7 MB/sCollecting scipy&lt;=1.5.4 Downloading scipy-1.5.4-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB) |████████████████████████████████| 25.9 MB 74.1 MB/sRequirement already satisfied: retrying&gt;=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly&gt;=4.4.1-&gt;pycaret) (1.3.3)Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit&lt;2.0.0,&gt;=1.0.4-&gt;IPython-&gt;pycaret) (0.2.5)Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic&gt;=1.8.1-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (3.10.0.2)Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (1.24.3)Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (2021.10.8)Requirement already satisfied: charset-normalizer=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (2.0.8)Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (2.10)Requirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (3.0.6)Requirement already satisfied: srsly&lt;1.1.0,&gt;=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (1.0.5)Requirement already satisfied: wasabi&lt;1.1.0,&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (0.8.2)Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (7.4.0)Requirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (1.0.6)Requirement already satisfied: plac&lt;1.2.0,&gt;=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (1.1.3)Requirement already satisfied: catalogue&lt;1.1.0,&gt;=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (1.0.0)Requirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (2.0.6)Requirement already satisfied: blis&lt;0.5.0,&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (0.4.1)Requirement already satisfied: importlib-metadata&gt;=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&lt;2.4.0-&gt;pycaret) (4.8.2)Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata&gt;=0.20-&gt;catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&lt;2.4.0-&gt;pycaret) (3.6.0)Requirement already satisfied: notebook&gt;=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension=3.5.0-&gt;ipywidgets-&gt;pycaret) (5.3.1)Requirement already satisfied: terminado&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension=3.5.0-&gt;ipywidgets-&gt;pycaret) (0.12.1)Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension=3.5.0-&gt;ipywidgets-&gt;pycaret) (5.6.1)Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension=3.5.0-&gt;ipywidgets-&gt;pycaret) (1.8.0)Requirement already satisfied: pyzmq&gt;=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client-&gt;ipykernel&gt;=4.5.1-&gt;ipywidgets-&gt;pycaret) (22.3.0)Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado&gt;=0.8.1-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension=3.5.0-&gt;ipywidgets-&gt;pycaret) (0.7.0)Requirement already satisfied: PyWavelets in /usr/local/lib/python3.7/dist-packages (from imagehash-&gt;visions[type_image_path]==0.7.4-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (1.2.0)Collecting querystring-parser Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (0.3)Collecting alembic&lt;=1.4.1 Downloading alembic-1.4.1.tar.gz (1.1 MB) |████████████████████████████████| 1.1 MB 66.5 MB/sCollecting gitpython&gt;=2.1.0 Downloading GitPython-3.1.24-py3-none-any.whl (180 kB) |████████████████████████████████| 180 kB 40.6 MB/sRequirement already satisfied: protobuf&gt;=3.7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (3.17.3)Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (1.4.27)Requirement already satisfied: Flask in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (1.1.4)Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (1.3.0)Collecting databricks-cli&gt;=0.8.7 Downloading databricks-cli-0.16.2.tar.gz (58 kB) |████████████████████████████████| 58 kB 5.6 MB/sCollecting prometheus-flask-exporter Downloading prometheus_flask_exporter-0.18.6-py3-none-any.whl (17 kB)Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (21.3)Requirement already satisfied: click&gt;=7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (7.1.2)Requirement already satisfied: sqlparse&gt;=0.3.1 in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (0.4.2)Collecting gunicorn Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB) |████████████████████████████████| 79 kB 7.6 MB/sCollecting docker&gt;=4.0.0 Downloading docker-5.0.3-py2.py3-none-any.whl (146 kB) |████████████████████████████████| 146 kB 58.9 MB/sCollecting Mako Downloading Mako-1.1.6-py2.py3-none-any.whl (75 kB) |████████████████████████████████| 75 kB 4.2 MB/sCollecting python-editor&gt;=0.3 Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)Requirement already satisfied: tabulate&gt;=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli&gt;=0.8.7-&gt;mlflow-&gt;pycaret) (0.8.9)Collecting websocket-client&gt;=0.32.0 Downloading websocket_client-1.2.3-py3-none-any.whl (53 kB) |████████████████████████████████| 53 kB 1.2 MB/sCollecting gitdb&lt;5,&gt;=4.0.1 Downloading gitdb-4.0.9-py3-none-any.whl (63 kB) |████████████████████████████████| 63 kB 1.6 MB/sCollecting smmap&lt;6,&gt;=3.0.1 Downloading smmap-5.0.0-py3-none-any.whl (24 kB)Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy-&gt;mlflow-&gt;pycaret) (1.1.2)Requirement already satisfied: itsdangerous&lt;2.0,&gt;=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask-&gt;mlflow-&gt;pycaret) (1.1.0)Requirement already satisfied: Werkzeug&lt;2.0,&gt;=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask-&gt;mlflow-&gt;pycaret) (1.0.1)Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension=3.5.0-&gt;ipywidgets-&gt;pycaret) (4.1.0)Requirement already satisfied: pandocfilters&gt;=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension=3.5.0-&gt;ipywidgets-&gt;pycaret) (1.5.0)Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension=3.5.0-&gt;ipywidgets-&gt;pycaret) (0.5.0)Requirement already satisfied: mistune&lt;2,&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension=3.5.0-&gt;ipywidgets-&gt;pycaret) (0.8.4)Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension=3.5.0-&gt;ipywidgets-&gt;pycaret) (0.7.1)Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach-&gt;nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension=3.5.0-&gt;ipywidgets-&gt;pycaret) (0.5.1)Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from prometheus-flask-exporter-&gt;mlflow-&gt;pycaret) (0.12.0)Collecting pyLDAvis Downloading pyLDAvis-3.3.0.tar.gz (1.7 MB) |████████████████████████████████| 1.7 MB 44.1 MB/s Installing build dependencies … done Getting requirements to build wheel … done Installing backend dependencies … done Preparing wheel metadata … done Downloading pyLDAvis-3.2.2.tar.gz (1.7 MB) |████████████████████████████████| 1.7 MB 30.5 MB/sRequirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis-&gt;pycaret) (2.7.3)Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis-&gt;pycaret) (0.16.0)Collecting funcy Downloading funcy-1.16-py2.py3-none-any.whl (32 kB)Requirement already satisfied: numba&gt;=0.35 in /usr/local/lib/python3.7/dist-packages (from pyod-&gt;pycaret) (0.51.2)Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from pyod-&gt;pycaret) (0.10.2)Requirement already satisfied: llvmlite&lt;0.35,&gt;=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba&gt;=0.35-&gt;pyod-&gt;pycaret) (0.34.0)Requirement already satisfied: patsy&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels-&gt;pyod-&gt;pycaret) (0.5.2)Collecting pynndescent&gt;=0.5 Downloading pynndescent-0.5.5.tar.gz (1.1 MB) |████████████████████████████████| 1.1 MB 55.1 MB/sBuilding wheels for collected packages: htmlmin, imagehash, alembic, databricks-cli, pyLDAvis, pyod, umap-learn, pynndescent Building wheel for htmlmin (setup.py) … done Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27098 sha256=6dff1694390dae41ea8bd3ca00f5564142023ea037fa606be0a8ffba9c16d1da Stored in directory: /root/.cache/pip/wheels/70/e1/52/5b14d250ba868768823940c3229e9950d201a26d0bd3ee8655 Building wheel for imagehash (setup.py) … done Created wheel for imagehash: filename=ImageHash-4.2.1-py2.py3-none-any.whl size=295207 sha256=9e38b104e77871b6f6a6a9267c3debd3ac85d39441acb3cda64d4dc07a11dd27 Stored in directory: /root/.cache/pip/wheels/4c/d5/59/5e3e297533ddb09407769762985d134135064c6831e29a914e Building wheel for alembic (setup.py) … done Created wheel for alembic: filename=alembic-1.4.1-py2.py3-none-any.whl size=158172 sha256=652d8b88b2468cf1d1c9f1c3242dda689e0f670bc6e5b88dc4dbf087fecbaccc Stored in directory: /root/.cache/pip/wheels/be/5d/0a/9e13f53f4f5dfb67cd8d245bb7cdffe12f135846f491a283e3 Building wheel for databricks-cli (setup.py) … done Created wheel for databricks-cli: filename=databricks_cli-0.16.2-py3-none-any.whl size=106811 sha256=9dbaaca3ece5f6a1522d676d8b1ec35c065a0bd0c564bd862ae3012984b70c9a Stored in directory: /root/.cache/pip/wheels/f4/5c/ed/e1ce20a53095f63b27b4964abbad03e59cf3472822addf7d29 Building wheel for pyLDAvis (setup.py) … done Created wheel for pyLDAvis: filename=pyLDAvis-3.2.2-py2.py3-none-any.whl size=135618 sha256=471d50a9a2e725465ffc7d32b21edb15c9b84dc0573891185a43e97f567aa0a7 Stored in directory: /root/.cache/pip/wheels/f8/b1/9b/560ac1931796b7303f7b517b949d2d31a4fbc512aad3b9f284 Building wheel for pyod (setup.py) … done Created wheel for pyod: filename=pyod-0.9.5-py3-none-any.whl size=132699 sha256=d116f5b46155bf0fa31aa88cc21da0e3be461b448e9c9b2d599c763a5ef0a6a1 Stored in directory: /root/.cache/pip/wheels/3d/bb/b7/62b60fb451b33b0df1ab8006697fba7a6a49709a629055cf77 Building wheel for umap-learn (setup.py) … done Created wheel for umap-learn: filename=umap_learn-0.5.2-py3-none-any.whl size=82709 sha256=7c48e34d2c19d333a623ed12491d3c7d07bafd52f2d35e474df56908f5cc7525 Stored in directory: /root/.cache/pip/wheels/84/1b/c6/aaf68a748122632967cef4dffef68224eb16798b6793257d82 Building wheel for pynndescent (setup.py) … done Created wheel for pynndescent: filename=pynndescent-0.5.5-py3-none-any.whl size=52603 sha256=7abff97eebc36deea7220f1b5e9907020826a07404003a9c7d794fef4d396e87 Stored in directory: /root/.cache/pip/wheels/af/e9/33/04db1436df0757c42fda8ea6796d7a8586e23c85fac355f476Successfully built htmlmin imagehash alembic databricks-cli pyLDAvis pyod umap-learn pynndescentInstalling collected packages: tangled-up-in-unicode, smmap, scipy, multimethod, joblib, websocket-client, visions, scikit-learn, requests, python-editor, Mako, imagehash, gitdb, querystring-parser, PyYAML, pynndescent, pydantic, prometheus-flask-exporter, phik, htmlmin, gunicorn, gitpython, funcy, docker, databricks-cli, alembic, umap-learn, scikit-plot, pyod, pyLDAvis, pandas-profiling, mlxtend, mlflow, lightgbm, kmodes, imbalanced-learn, Boruta, pycaret Attempting uninstall: scipy Found existing installation: scipy 1.4.1 Uninstalling scipy-1.4.1: Successfully uninstalled scipy-1.4.1 Attempting uninstall: joblib Found existing installation: joblib 1.1.0 Uninstalling joblib-1.1.0: Successfully uninstalled joblib-1.1.0 Attempting uninstall: scikit-learn Found existing installation: scikit-learn 1.0.1 Uninstalling scikit-learn-1.0.1: Successfully uninstalled scikit-learn-1.0.1 Attempting uninstall: requests Found existing installation: requests 2.23.0 Uninstalling requests-2.23.0: Successfully uninstalled requests-2.23.0 Attempting uninstall: PyYAML Found existing installation: PyYAML 3.13 Uninstalling PyYAML-3.13: Successfully uninstalled PyYAML-3.13 Attempting uninstall: pandas-profiling Found existing installation: pandas-profiling 1.4.1 Uninstalling pandas-profiling-1.4.1: Successfully uninstalled pandas-profiling-1.4.1 Attempting uninstall: mlxtend Found existing installation: mlxtend 0.14.0 Uninstalling mlxtend-0.14.0: Successfully uninstalled mlxtend-0.14.0 Attempting uninstall: lightgbm Found existing installation: lightgbm 2.2.3 Uninstalling lightgbm-2.2.3: Successfully uninstalled lightgbm-2.2.3 Attempting uninstall: imbalanced-learn Found existing installation: imbalanced-learn 0.8.1 Uninstalling imbalanced-learn-0.8.1: Successfully uninstalled imbalanced-learn-0.8.1ERROR: pip’s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.google-colab 1.0.0 requires requests=2.23.0, but you have requests 2.26.0 which is incompatible.datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.albumentations 0.1.12 requires imgaug&lt;0.2.7,&gt;=0.2.5, but you have imgaug 0.2.9 which is incompatible.Successfully installed Boruta-0.3 Mako-1.1.6 PyYAML-6.0 alembic-1.4.1 databricks-cli-0.16.2 docker-5.0.3 funcy-1.16 gitdb-4.0.9 gitpython-3.1.24 gunicorn-20.1.0 htmlmin-0.1.12 imagehash-4.2.1 imbalanced-learn-0.7.0 joblib-1.0.1 kmodes-0.11.1 lightgbm-3.3.1 mlflow-1.22.0 mlxtend-0.19.0 multimethod-1.6 pandas-profiling-3.1.0 phik-0.12.0 prometheus-flask-exporter-0.18.6 pyLDAvis-3.2.2 pycaret-2.3.5 pydantic-1.8.2 pynndescent-0.5.5 pyod-0.9.5 python-editor-1.0.4 querystring-parser-1.2.4 requests-2.26.0 scikit-learn-0.23.2 scikit-plot-0.3.7 scipy-1.5.4 smmap-5.0.0 tangled-up-in-unicode-0.1.0 umap-learn-0.5.2 visions-0.7.4 websocket-client-1.2.3 pycaret을 그냥 설치 할 수도 있고,version을 정해서 설치 할 수도 있다. 먼저 개괄적으로 확인만 하고, github 에서 autoML을 pycaret 2.0 ver로 진행 해야 한다. google colab에 설치 한 경우 Install 후 런타임>런타임다시시작(CTRL+M) 을 꼭 한번 해 준 후 아래 설명에 따라 가는 것이 좋음. ** 만약 오류가 난다면, 런타임 초기화 후 import, 런타임다시시작 후 진행 하는 것을 추천 왜 그런지 모름 _ 그냥 이렇게 하면 된다는 것만 알려주겠음 Data Load12from pycaret.datasets import get_datadata = get_data(&quot;diamond&quot;) pycaret.regression123from pycaret.regression import *reg_set = setup(data, target = 'Price', transform_target = True, log_experiment = True, experiment_name = 'diamond') pycaret.regression : 종류 \\ Description Value 0 session_id 2882 1 Target Price 2 Original Data (6000, 8) 3 Missing Values False 4 Numeric Features 1 5 Categorical Features 6 6 Ordinal Features False 7 High Cardinality Features False 8 High Cardinality Method None 9 Transformed Train Set (4199, 28) 10 Transformed Test Set (1801, 28) 11 Shuffle Train-Test True 12 Stratify Train-Test False 13 Fold Generator KFold 14 Fold Number 10 15 CPU Jobs -1 16 Use GPU False 17 Log Experiment True 18 Experiment Name diamond 19 USI 116c 20 Imputation Type simple 21 Iterative Imputation Iteration None 22 Numeric Imputer mean 23 Iterative Imputation Numeric Model None 24 Categorical Imputer constant 25 Iterative Imputation Categorical Model None 26 Unknown Categoricals Handling least_frequent 27 Normalize False 28 Normalize Method None 29 Transformation False 30 Transformation Method None 31 PCA False 32 PCA Method None 33 PCA Components None 34 Ignore Low Variance False 35 Combine Rare Levels False 36 Rare Level Threshold None 37 Numeric Binning False 38 Remove Outliers False 39 Outliers Threshold None 40 Remove Multicollinearity False 41 Multicollinearity Threshold None 42 Remove Perfect Collinearity True 43 Clustering False 44 Clustering Iteration None 45 Polynomial Features False 46 Polynomial Degree None 47 Trignometry Features False 48 Polynomial Threshold None 49 Group Features False 50 Feature Selection False 51 Feature Selection Method classic 52 Features Selection Threshold None 53 Feature Interaction False 54 Feature Ratio False 55 Interaction Threshold None 56 Transform Target True 57 Transform Target Method box-cox 더 많이 알고 싶으면 저거 다 공부 해 ^0^ 모델 만들기 최적의 모델을 만들기 위해 한줄의 코드면 된다 ㅠㅠ 1best = compare_models() Model MAE MSE RMSE R2 RMSLE MAPE TT (Sec) lightgbm Light Gradient Boosting Machine 637.8811 1.928277e+06 1367.4159 0.9813 0.0677 0.0491 0.120 et Extra Trees Regressor 748.9529 2.253684e+06 1478.3926 0.9782 0.0802 0.0594 1.199 rf Random Forest Regressor 742.9041 2.417200e+06 1528.6437 0.9765 0.0785 0.0579 1.090 gbr Gradient Boosting Regressor 764.6458 2.449865e+06 1544.3382 0.9762 0.0783 0.0583 0.288 dt Decision Tree Regressor 946.3401 3.350058e+06 1811.0705 0.9672 0.1034 0.0756 0.040 ada AdaBoost Regressor 1997.1826 1.710448e+07 4091.7565 0.8350 0.1895 0.1511 0.251 knn K Neighbors Regressor 3072.0318 3.642699e+07 6017.2046 0.6421 0.3636 0.2323 0.086 omp Orthogonal Matching Pursuit 3317.3424 8.643676e+07 9045.7885 0.1344 0.2823 0.2209 0.026 llar Lasso Least Angle Regression 6540.9142 1.144871e+08 10682.7674 -0.1241 0.7130 0.5636 0.281 lasso Lasso Regression 6540.9147 1.144871e+08 10682.7665 -0.1241 0.7130 0.5636 0.025 en Elastic Net 6540.9147 1.144871e+08 10682.7665 -0.1241 0.7130 0.5636 0.025 dummy Dummy Regressor 6540.9142 1.144871e+08 10682.7674 -0.1241 0.7130 0.5636 0.021 ridge Ridge Regression 3376.7759 4.409370e+08 17429.1601 -3.0382 0.2235 0.1734 0.026 br Bayesian Ridge 3464.5342 6.180348e+08 19047.2745 -4.5803 0.2244 0.1745 0.028 huber Huber Regressor 3490.0167 7.900161e+08 19860.5244 -6.0721 0.2254 0.1729 0.118 lr Linear Regression 3566.8112 8.908481e+08 21034.8582 -6.9766 0.2253 0.1755 0.309 par Passive Aggressive Regressor 8585.4060 5.154119e+10 94736.3961 -439.8984 0.2947 0.2745 0.031 모형 평가 최적의 모델 확인 후 평가 역시 코드 한줄 ㅠㅠ 감동 1plot_model(best) 1plot_model(best, plot = &quot;feature&quot;) 모형 저장, 모형 배포 MLOps 개념, RestAPI, Flask 1234finalize_best = finalize_model(best)#save modelsave_model(finalize_best, &quot;diamond_pipeline&quot;) Transformation Pipeline and Model Successfully Saved(Pipeline(memory=None, steps=[(‘dtypes’, DataTypes_Auto_infer(categorical_features=[], display_types=True, features_todrop=[], id_columns=[], ml_usecase=’regression’, numerical_features=[], target=’Price’, time_features=[])), (‘imputer’, Simple_Imputer(categorical_strategy=’not_available’, fill_value_categorical=None, fill_value_numerical=None, numeric_strategy=’… learning_rate=0.1, max_depth=-1, min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_leaves=31, objective=None, random_state=2882, reg_alpha=0.0, reg_lambda=0.0, silent=’warn’, subsample=1.0, subsample_for_bin=200000, subsample_freq=0), silent=’warn’, subsample=1.0, subsample_for_bin=200000, subsample_freq=0)]], verbose=False), ‘diamond_pipeline.pkl’) MLOps devOPs (개발과 운영 팀이 별도로 있었음.) 자동화 되면서 같이 됨.MLOps dash board 1234567891011121314151617181920212223242526272829!pip install mlflow --quiet!pip install pyngrok --quietimport mlflowwith mlflow.start_run(run_name=&quot;MLflow on Colab&quot;): mlflow.log_metric(&quot;m1&quot;, 2.0) mlflow.log_param(&quot;p1&quot;, &quot;mlflow-colab&quot;)# run tracking UI in the backgroundget_ipython().system_raw(&quot;mlflow ui --port 5000 &amp;&quot;) # run tracking UI in the background# create remote tunnel using ngrok.com to allow local port access# borrowed from https://colab.research.google.com/github/alfozan/MLflow-GBRT-demo/blob/master/MLflow-GBRT-demo.ipynb#scrollTo=4h3bKHMYUIG6from pyngrok import ngrok# Terminate open tunnels if existngrok.kill()# Setting the authtoken (optional)# Get your authtoken from https://dashboard.ngrok.com/authNGROK_AUTH_TOKEN = &quot;&quot;ngrok.set_auth_token(NGROK_AUTH_TOKEN)# Open an HTTPs tunnel on port 5000 for http://localhost:5000ngrok_tunnel = ngrok.connect(addr=&quot;5000&quot;, proto=&quot;http&quot;, bind_tls=True)print(&quot;MLflow Tracking UI:&quot;, ngrok_tunnel.public_url) |████████████████████████████████| 745 kB 5.4 MB/s Building wheel for pyngrok (setup.py) ... done --------------------------------------- Exception Traceback (most recent call last) in () 4 import mlflow 5 ----> 6 with mlflow.start_run(run_name=\"MLflow on Colab\"): 7 mlflow.log_metric(\"m1\", 2.0) 8 mlflow.log_param(\"p1\", \"mlflow-colab\") /usr/local/lib/python3.7/dist-packages/mlflow/tracking/fluent.py in start_run(run_id, experiment_id, run_name, nested, tags) 229 + “current run with mlflow.end_run(). To start a nested “ 230 + “run, call start_run with nested=True” –&gt; 231 ).format(_active_run_stack[0].info.run_id) 232 ) 233 client = MlflowClient() Exception: Run with UUID 3cbca838cdd44eac8620700ac1929a64 is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True 배포 하는 것이 마지막 단계인데, 구글 코랩에서 안먹는 다는 것이 함정이라고 한다. 언젠간 내가 스스로 할 수 있는 날이 오지 않을까 한다.","link":"/2021/12/10/python/AoutoML_Pycaret(01)/"},{"title":"Crawling_basic(01)","text":"크롤링즐거운 마음으로 크롤링 해 봅시다 ! 01. file 준비file 은repository에 올려놓았다. pycham을 열어서 python 가상환경(VENV)설정 후 그 file에서 진행. 02. 크롤링 : BeautifulSoup 설치beautifulsoup 설치는 여기 에서 시작 terminal에 위의 명령어 입력 BeautifulSoup 설치 되었는지 한번 더 확인 12from bs4 import BeautifulSoupprint(&quot;library imported&quot;) terminal에 library imported가 print 된다. 03. 크롤링 : code03.1 객체 초기화12345678def main(): #객체 초기화, 뒤에있는 parser가 핵심: python에서 접근 가능하게 만들어줌 soup = BeautifulSoup(open(&quot;data/index.html&quot;), &quot;html.parser&quot;) print(soup)if __name__ == &quot;__main__&quot;: main() 함수를 만들어 html file을 열고 그 file을 parser로 python이 읽을 수 있는 형태로 가져온다. out: 12345678910111213141516171819202122library imported&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;/&gt;&lt;title&gt;Crawl This Page&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&quot;cheshire&quot;&gt;&lt;p&gt;Don't crawl this.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;elice&quot;&gt;&lt;p&gt;Hello, Python Crawling!&lt;/p&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;Process finished with exit code 0 읽어 오는 데 까지가 끝 03.2 원하는 객체 뽑아내기 : HTML tag로123456789101112131415161718# /c/../Crawring/venv/Scripts/python# -*- Encoding : UTF-8 -*-from bs4 import BeautifulSoupprint(&quot;library imported&quot;)def main(): # 객체 초기화, 뒤에있는 parser가 핵심: python에서 접근 가능하게 만들어줌 soup = BeautifulSoup(open(&quot;data/index.html&quot;), &quot;html.parser&quot;) #원하는 data 출력 print(soup.find(&quot;div&quot;).get_text()) #find_all 쓰면 get_text가 안된다. #저장(Excel:pandas, DB)if __name__ == &quot;__main__&quot;: main() out: 12345library importedDon't crawl this.Process finished with exit code 0 .find(&quot;div&quot;) : div tag를 뽑아오는 구문 .get_text() : div tag를 삭제하고 text만 뽑아옴 .find_all(&quot;div&quot;) : get text()가 안먹는다. 03.3 원하는 객체 뽑아내기 : class name으로1234567891011def main(): # 객체 초기화 soup = BeautifulSoup(open(&quot;data/index2.html&quot;), &quot;html.parser&quot;) # 원하는 data 출력 print(soup.find(&quot;div&quot;, class_ = &quot;elice&quot;).find(&quot;p&quot;).get_text()) # 저장(Excel:pandas, DB)if __name__ == &quot;__main__&quot;: main() 03.4 원하는 객체 뽑아내기 : id 로1234567891011def main(): # 객체 초기화 soup = BeautifulSoup(open(&quot;data/index3.html&quot;), &quot;html.parser&quot;) # 원하는 data 출력 print(soup.find(&quot;div&quot;, id = &quot;main&quot;).find(&quot;p&quot;).get_text()) # 저장(Excel:pandas, DB)if __name__ == &quot;__main__&quot;: main() 04. data 뽑아내기 신비롭게 data 뽑아내는 거는 google colab에서 진행 04.1 data file 받아오기 Data file은 여기에서 무료로 혹은 유료로 받을 수 있다. 인증키를 받아서 사용 해야 하는데, 인증키는 여기 서 개인정보를 입력하고 받아 오면 된다. 인증키를 받았다면 google colab 열고 진행 인증키는 입력한 메일로 오기 때문에 메일을 잘 쓰기 바란다. 12345import requestskey = &quot;*본인의 인증키 숫자를 넣는다*&quot;url = &quot;http://data.ex.co.kr/openapi/trtm/realUnitTrtm?key=`인증키여기`&amp;type=json&amp;iStartUnitCode=101&amp;iEndUnitCode=103&quot;responses = requests.get(url) &lt;Response [200]&gt; 인증키를 서공적으로 넣으면 위와 같은 out이 나온다. 04.2 responses로 json file 만들기12json = responses.json() out: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226{'code': 'SUCCESS', 'count': 602, 'message': '인증키가 유효합니다.', 'numOfRows': 10, 'pageNo': 1, 'pageSize': 61, 'realUnitTrtmVO': [{'efcvTrfl': '18', 'endUnitCode': '103 ', 'endUnitNm': '수원신갈', 'iEndUnitCode': None, 'iStartEndStdTypeCode': None, 'iStartUnitCode': None, 'numOfRows': None, 'pageNo': None, 'startEndStdTypeCode': '2', 'startEndStdTypeNm': '도착기준통행시간', 'startUnitCode': '101 ', 'startUnitNm': '서울', 'stdDate': '20220103', 'stdTime': '00:25', 'sumTmUnitTypeCode': None, 'tcsCarTypeCode': '1', 'tcsCarTypeDivCode': '1', 'tcsCarTypeDivName': '소형차', 'tcsCarTypeName': '1종', 'timeAvg': '7.466666666666667', 'timeMax': '9.216', 'timeMin': '5.800'}, {'efcvTrfl': '53', 'endUnitCode': '103 ', 'endUnitNm': '수원신갈', 'iEndUnitCode': None, 'iStartEndStdTypeCode': None, 'iStartUnitCode': None, 'numOfRows': None, 'pageNo': None, 'startEndStdTypeCode': '2', 'startEndStdTypeNm': '도착기준통행시간', 'startUnitCode': '101 ', 'startUnitNm': '서울', 'stdDate': '20220103', 'stdTime': '00:30', 'sumTmUnitTypeCode': None, 'tcsCarTypeCode': '1', 'tcsCarTypeDivCode': '1', 'tcsCarTypeDivName': '소형차', 'tcsCarTypeName': '1종', 'timeAvg': '7.466666666666667', 'timeMax': '9.283', 'timeMin': '5.783'}, {'efcvTrfl': '14', 'endUnitCode': '103 ', 'endUnitNm': '수원신갈', 'iEndUnitCode': None, 'iStartEndStdTypeCode': None, 'iStartUnitCode': None, 'numOfRows': None, 'pageNo': None, 'startEndStdTypeCode': '2', 'startEndStdTypeNm': '도착기준통행시간', 'startUnitCode': '101 ', 'startUnitNm': '서울', 'stdDate': '20220103', 'stdTime': '00:35', 'sumTmUnitTypeCode': None, 'tcsCarTypeCode': '1', 'tcsCarTypeDivCode': '1', 'tcsCarTypeDivName': '소형차', 'tcsCarTypeName': '1종', 'timeAvg': '7.416666666666667', 'timeMax': '8.400', 'timeMin': '6.050'}, {'efcvTrfl': '15', 'endUnitCode': '103 ', 'endUnitNm': '수원신갈', 'iEndUnitCode': None, 'iStartEndStdTypeCode': None, 'iStartUnitCode': None, 'numOfRows': None, 'pageNo': None, 'startEndStdTypeCode': '2', 'startEndStdTypeNm': '도착기준통행시간', 'startUnitCode': '101 ', 'startUnitNm': '서울', 'stdDate': '20220103', 'stdTime': '00:40', 'sumTmUnitTypeCode': None, 'tcsCarTypeCode': '1', 'tcsCarTypeDivCode': '1', 'tcsCarTypeDivName': '소형차', 'tcsCarTypeName': '1종', 'timeAvg': '7.516666666666667', 'timeMax': '9.283', 'timeMin': '5.966'}, {'efcvTrfl': '41', 'endUnitCode': '103 ', 'endUnitNm': '수원신갈', 'iEndUnitCode': None, 'iStartEndStdTypeCode': None, 'iStartUnitCode': None, 'numOfRows': None, 'pageNo': None, 'startEndStdTypeCode': '2', 'startEndStdTypeNm': '도착기준통행시간', 'startUnitCode': '101 ', 'startUnitNm': '서울', 'stdDate': '20220103', 'stdTime': '00:45', 'sumTmUnitTypeCode': None, 'tcsCarTypeCode': '1', 'tcsCarTypeDivCode': '1', 'tcsCarTypeDivName': '소형차', 'tcsCarTypeName': '1종', 'timeAvg': '7.5', 'timeMax': '8.800', 'timeMin': '5.750'}, {'efcvTrfl': '11', 'endUnitCode': '103 ', 'endUnitNm': '수원신갈', 'iEndUnitCode': None, 'iStartEndStdTypeCode': None, 'iStartUnitCode': None, 'numOfRows': None, 'pageNo': None, 'startEndStdTypeCode': '2', 'startEndStdTypeNm': '도착기준통행시간', 'startUnitCode': '101 ', 'startUnitNm': '서울', 'stdDate': '20220103', 'stdTime': '00:50', 'sumTmUnitTypeCode': None, 'tcsCarTypeCode': '1', 'tcsCarTypeDivCode': '1', 'tcsCarTypeDivName': '소형차', 'tcsCarTypeName': '1종', 'timeAvg': '7.416666666666667', 'timeMax': '8.800', 'timeMin': '5.766'}, {'efcvTrfl': '8', 'endUnitCode': '103 ', 'endUnitNm': '수원신갈', 'iEndUnitCode': None, 'iStartEndStdTypeCode': None, 'iStartUnitCode': None, 'numOfRows': None, 'pageNo': None, 'startEndStdTypeCode': '2', 'startEndStdTypeNm': '도착기준통행시간', 'startUnitCode': '101 ', 'startUnitNm': '서울', 'stdDate': '20220103', 'stdTime': '00:55', 'sumTmUnitTypeCode': None, 'tcsCarTypeCode': '1', 'tcsCarTypeDivCode': '1', 'tcsCarTypeDivName': '소형차', 'tcsCarTypeName': '1종', 'timeAvg': '7.433333333333334', 'timeMax': '8.350', 'timeMin': '5.750'}, {'efcvTrfl': '87', 'endUnitCode': '103 ', 'endUnitNm': '수원신갈', 'iEndUnitCode': None, 'iStartEndStdTypeCode': None, 'iStartUnitCode': None, 'numOfRows': None, 'pageNo': None, 'startEndStdTypeCode': '2', 'startEndStdTypeNm': '도착기준통행시간', 'startUnitCode': '101 ', 'startUnitNm': '서울', 'stdDate': '20220103', 'stdTime': '01 ', 'sumTmUnitTypeCode': None, 'tcsCarTypeCode': '1', 'tcsCarTypeDivCode': '1', 'tcsCarTypeDivName': '소형차', 'tcsCarTypeName': '1종', 'timeAvg': '7.55', 'timeMax': '10.183', 'timeMin': '5.750'}, {'efcvTrfl': '30', 'endUnitCode': '103 ', 'endUnitNm': '수원신갈', 'iEndUnitCode': None, 'iStartEndStdTypeCode': None, 'iStartUnitCode': None, 'numOfRows': None, 'pageNo': None, 'startEndStdTypeCode': '2', 'startEndStdTypeNm': '도착기준통행시간', 'startUnitCode': '101 ', 'startUnitNm': '서울', 'stdDate': '20220103', 'stdTime': '01:00', 'sumTmUnitTypeCode': None, 'tcsCarTypeCode': '1', 'tcsCarTypeDivCode': '1', 'tcsCarTypeDivName': '소형차', 'tcsCarTypeName': '1종', 'timeAvg': '7.625', 'timeMax': '10.183', 'timeMin': '6.600'}, {'efcvTrfl': '3', 'endUnitCode': '103 ', 'endUnitNm': '수원신갈', 'iEndUnitCode': None, 'iStartEndStdTypeCode': None, 'iStartUnitCode': None, 'numOfRows': None, 'pageNo': None, 'startEndStdTypeCode': '2', 'startEndStdTypeNm': '도착기준통행시간', 'startUnitCode': '101 ', 'startUnitNm': '서울', 'stdDate': '20220103', 'stdTime': '01:05', 'sumTmUnitTypeCode': None, 'tcsCarTypeCode': '1', 'tcsCarTypeDivCode': '1', 'tcsCarTypeDivName': '소형차', 'tcsCarTypeName': '1종', 'timeAvg': '7.683333333333334', 'timeMax': '8.250', 'timeMin': '7.500'}]} json 객체에 responses 함수를 이용하여 json형태의 file을 담고 cars : 내가 찾고 싶은 부분을 file의 형태에서 찾아서 넣는다. file의 구조와 내가 찾고 싶은 부분을 알고 있어야 원하는 정보를 넣을 수 있다. 04.3 json file 에서 원하는 정보 빼오기 알 수 없지만, realUnitTrtmVO 라는 tag? dictionarly에 접근하여정보를 빼보자. 1cars = json[&quot;realUnitTrtmVO&quot;] out: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220[{'efcvTrfl': '18', 'endUnitCode': '103 ', 'endUnitNm': '수원신갈', 'iEndUnitCode': None, 'iStartEndStdTypeCode': None, 'iStartUnitCode': None, 'numOfRows': None, 'pageNo': None, 'startEndStdTypeCode': '2', 'startEndStdTypeNm': '도착기준통행시간', 'startUnitCode': '101 ', 'startUnitNm': '서울', 'stdDate': '20220103', 'stdTime': '00:25', 'sumTmUnitTypeCode': None, 'tcsCarTypeCode': '1', 'tcsCarTypeDivCode': '1', 'tcsCarTypeDivName': '소형차', 'tcsCarTypeName': '1종', 'timeAvg': '7.466666666666667', 'timeMax': '9.216', 'timeMin': '5.800'}, {'efcvTrfl': '53', 'endUnitCode': '103 ', 'endUnitNm': '수원신갈', 'iEndUnitCode': None, 'iStartEndStdTypeCode': None, 'iStartUnitCode': None, 'numOfRows': None, 'pageNo': None, 'startEndStdTypeCode': '2', 'startEndStdTypeNm': '도착기준통행시간', 'startUnitCode': '101 ', 'startUnitNm': '서울', 'stdDate': '20220103', 'stdTime': '00:30', 'sumTmUnitTypeCode': None, 'tcsCarTypeCode': '1', 'tcsCarTypeDivCode': '1', 'tcsCarTypeDivName': '소형차', 'tcsCarTypeName': '1종', 'timeAvg': '7.466666666666667', 'timeMax': '9.283', 'timeMin': '5.783'}, {'efcvTrfl': '14', 'endUnitCode': '103 ', 'endUnitNm': '수원신갈', 'iEndUnitCode': None, 'iStartEndStdTypeCode': None, 'iStartUnitCode': None, 'numOfRows': None, 'pageNo': None, 'startEndStdTypeCode': '2', 'startEndStdTypeNm': '도착기준통행시간', 'startUnitCode': '101 ', 'startUnitNm': '서울', 'stdDate': '20220103', 'stdTime': '00:35', 'sumTmUnitTypeCode': None, 'tcsCarTypeCode': '1', 'tcsCarTypeDivCode': '1', 'tcsCarTypeDivName': '소형차', 'tcsCarTypeName': '1종', 'timeAvg': '7.416666666666667', 'timeMax': '8.400', 'timeMin': '6.050'}, {'efcvTrfl': '15', 'endUnitCode': '103 ', 'endUnitNm': '수원신갈', 'iEndUnitCode': None, 'iStartEndStdTypeCode': None, 'iStartUnitCode': None, 'numOfRows': None, 'pageNo': None, 'startEndStdTypeCode': '2', 'startEndStdTypeNm': '도착기준통행시간', 'startUnitCode': '101 ', 'startUnitNm': '서울', 'stdDate': '20220103', 'stdTime': '00:40', 'sumTmUnitTypeCode': None, 'tcsCarTypeCode': '1', 'tcsCarTypeDivCode': '1', 'tcsCarTypeDivName': '소형차', 'tcsCarTypeName': '1종', 'timeAvg': '7.516666666666667', 'timeMax': '9.283', 'timeMin': '5.966'}, {'efcvTrfl': '41', 'endUnitCode': '103 ', 'endUnitNm': '수원신갈', 'iEndUnitCode': None, 'iStartEndStdTypeCode': None, 'iStartUnitCode': None, 'numOfRows': None, 'pageNo': None, 'startEndStdTypeCode': '2', 'startEndStdTypeNm': '도착기준통행시간', 'startUnitCode': '101 ', 'startUnitNm': '서울', 'stdDate': '20220103', 'stdTime': '00:45', 'sumTmUnitTypeCode': None, 'tcsCarTypeCode': '1', 'tcsCarTypeDivCode': '1', 'tcsCarTypeDivName': '소형차', 'tcsCarTypeName': '1종', 'timeAvg': '7.5', 'timeMax': '8.800', 'timeMin': '5.750'}, {'efcvTrfl': '11', 'endUnitCode': '103 ', 'endUnitNm': '수원신갈', 'iEndUnitCode': None, 'iStartEndStdTypeCode': None, 'iStartUnitCode': None, 'numOfRows': None, 'pageNo': None, 'startEndStdTypeCode': '2', 'startEndStdTypeNm': '도착기준통행시간', 'startUnitCode': '101 ', 'startUnitNm': '서울', 'stdDate': '20220103', 'stdTime': '00:50', 'sumTmUnitTypeCode': None, 'tcsCarTypeCode': '1', 'tcsCarTypeDivCode': '1', 'tcsCarTypeDivName': '소형차', 'tcsCarTypeName': '1종', 'timeAvg': '7.416666666666667', 'timeMax': '8.800', 'timeMin': '5.766'}, {'efcvTrfl': '8', 'endUnitCode': '103 ', 'endUnitNm': '수원신갈', 'iEndUnitCode': None, 'iStartEndStdTypeCode': None, 'iStartUnitCode': None, 'numOfRows': None, 'pageNo': None, 'startEndStdTypeCode': '2', 'startEndStdTypeNm': '도착기준통행시간', 'startUnitCode': '101 ', 'startUnitNm': '서울', 'stdDate': '20220103', 'stdTime': '00:55', 'sumTmUnitTypeCode': None, 'tcsCarTypeCode': '1', 'tcsCarTypeDivCode': '1', 'tcsCarTypeDivName': '소형차', 'tcsCarTypeName': '1종', 'timeAvg': '7.433333333333334', 'timeMax': '8.350', 'timeMin': '5.750'}, {'efcvTrfl': '87', 'endUnitCode': '103 ', 'endUnitNm': '수원신갈', 'iEndUnitCode': None, 'iStartEndStdTypeCode': None, 'iStartUnitCode': None, 'numOfRows': None, 'pageNo': None, 'startEndStdTypeCode': '2', 'startEndStdTypeNm': '도착기준통행시간', 'startUnitCode': '101 ', 'startUnitNm': '서울', 'stdDate': '20220103', 'stdTime': '01 ', 'sumTmUnitTypeCode': None, 'tcsCarTypeCode': '1', 'tcsCarTypeDivCode': '1', 'tcsCarTypeDivName': '소형차', 'tcsCarTypeName': '1종', 'timeAvg': '7.55', 'timeMax': '10.183', 'timeMin': '5.750'}, {'efcvTrfl': '30', 'endUnitCode': '103 ', 'endUnitNm': '수원신갈', 'iEndUnitCode': None, 'iStartEndStdTypeCode': None, 'iStartUnitCode': None, 'numOfRows': None, 'pageNo': None, 'startEndStdTypeCode': '2', 'startEndStdTypeNm': '도착기준통행시간', 'startUnitCode': '101 ', 'startUnitNm': '서울', 'stdDate': '20220103', 'stdTime': '01:00', 'sumTmUnitTypeCode': None, 'tcsCarTypeCode': '1', 'tcsCarTypeDivCode': '1', 'tcsCarTypeDivName': '소형차', 'tcsCarTypeName': '1종', 'timeAvg': '7.625', 'timeMax': '10.183', 'timeMin': '6.600'}, {'efcvTrfl': '3', 'endUnitCode': '103 ', 'endUnitNm': '수원신갈', 'iEndUnitCode': None, 'iStartEndStdTypeCode': None, 'iStartUnitCode': None, 'numOfRows': None, 'pageNo': None, 'startEndStdTypeCode': '2', 'startEndStdTypeNm': '도착기준통행시간', 'startUnitCode': '101 ', 'startUnitNm': '서울', 'stdDate': '20220103', 'stdTime': '01:05', 'sumTmUnitTypeCode': None, 'tcsCarTypeCode': '1', 'tcsCarTypeDivCode': '1', 'tcsCarTypeDivName': '소형차', 'tcsCarTypeName': '1종', 'timeAvg': '7.683333333333334', 'timeMax': '8.250', 'timeMin': '7.500'}] 04.4 csv file로 출력 pandas를 이용하여 dictionarly, json file을 dataFrame, csv file로 변환 1234567891011import pandas as pddt = []for car in cars: dic_df = {} dic_df[&quot;data&quot;] = car[&quot;stdDate&quot;] dic_df[&quot;time&quot;] = car[&quot;stdTime&quot;] dic_df[&quot;destination&quot;] = car[&quot;endUnitNm&quot;] dt.append(dic_df)pd.DataFrame(dt).to_csv(&quot;temp.csv&quot;,index = False, encoding=&quot;euc-kr&quot;) Encoding의 경우 해당 json file의 documents를 봐야함. google과 같은 기업에서 API를 받아와서 사용 하는 것도 가능 하므로앞으로 이 기술은 무궁무진한 발전 가능성이 있을 것으로 보임 ^^","link":"/2022/01/03/python/Crowling_basic01/"},{"title":"DTS: Missing Value detection(01)","text":"§ 결측치 찾기 이론 § 결측치 찾기 실습 § 변수 1개를 이용하여 이상값 찾기 § 변수 2개를 이용하여 이상값 찾기 ♠ Ref.01 Missing Value : 결측치 정의 : Missing Feature(누락 data) 를 처리 해주어야 ML이 잘 돌아 간다. Na, Nan 과 같은 값 종류 : Random : 패턴이 없는 무작위 값 No Random : 패턴을 가진 결측치 Deletion deletion해서 특성이 바뀌지 않는다면, 가장 좋은 방법 dropna() axis = (0 : 행 제거, default),(1: 열제거) subset = (특정 feature을 지정하여 해당 누락 data 제거) Listwist(목록삭제) 결측치가 있는 행 전부 삭제 pairwise(단일 값 삭제) 12345df = df.dropna() # 결측치 있는 행 전부 삭제df = df.dropna(axis = 1) # 결측치 있는 열 전부 삭제df = df.dropna(how = 'all') # 전체가 결측인 행 삭제df = df.dropna(thresh = 2) # threshold 2, 결측치 2초과 삭제 123456789101112131415161718192021222324252627282930313233df = df.dropna(subset=['col1', 'col2', 'col3'])# 특정열 모두가 결측치일 경우 해당 행 삭제df = df.dropna(subset=['col1', 'col2', 'col3'], how = 'all')# 특정열에 1개 초과의 결측치가 있을 경우 해당 행 삭제df = df.dropna(subset=['col1', 'col2', 'col3'], thresh = 1 )#바로 적용df.dropna(inplace = True)``` &lt;br&gt;&lt;br&gt;---### Imputation1. 결측치를 특정 값으로 대치 - mode : 최빈값 + 번주형, 빈도가 제일 높은값으로 대치 - median : 중앙값 + 연속형, 결측값을 제외한 중앙값으로 대치 - mean : 평균 + 연속형, 결측값을 제외한 평균으로 대치 - similar case imputation : 조건부 대치 - Generalized imputation : 회귀분석을 이용한 대치 2. 사용함수 - fillna(), replace(), interpolate()##### fillna() : 0 처리```pythondf.fillna(0) df[].fillna() : 특정 column만 대치12345# 0으로 대체하기df['col'] = df['col'].fillna(0)# 컬럼의 평균으로 대체하기df['col'] = df['col'].fillna(df['col'].mean()) 12345# 바로 위의 값으로 채우기df.fillna(method = 'pad')#바로 아래 값으로 채우기 df.fillna(method='bfill') replace()12# 대체, 결측치가 있으면, -50으로 채운다.df.replace(to_replace = np.nan, value = -50) interpolate() 만약, 값들이 선형적이라추정 후 간격으로 처리 1df.interpolate(method = 'linear' , limit_direction = 'forward') prediction Model (예측모델) 결측치가 pattern을 가진다고 가정하고 진행. 결측값이 없는 컬럼들로 구성된 dataset으로 예측 회기분석기술 혹은 SVM과같은 ML 통계기법이 있다. guid Line (Missiong Value : MV) MV &lt; 10% : 삭제 or 대치 10% &lt; MV &lt; 50% : regression or model based imputation 50%&lt; MV : 해당 column 제거","link":"/2021/12/21/python/DTS_MissingValue/"},{"title":"DTS: Missing Value detection(02)","text":"§ 결측치 찾기 이론 § 결측치 찾기 실습 § 변수 1개를 이용하여 이상값 찾기 § 변수 2개를 이용하여 이상값 찾기 ♠ Ref.01 data in Kaggle note를 public으로 올려는 놨는데 검색이 될까 모르겠네요. Missing Value : 결측치 확인data Loading123import pandas as pdcovidtotals = pd.read_csv(&quot;../input/covid-data/covidtotals.csv&quot;)covidtotals.head() data info1covidtotals.info() data division 인구통계 관련 column Covid 관련 column 12case_vars = [&quot;location&quot;, &quot;total_cases&quot;, &quot;total_deaths&quot;, &quot;total_cases_pm&quot;, &quot;total_deaths_pm&quot;]demo_vars = [&quot;population&quot;, &quot;pop_density&quot;, &quot;median_age&quot;, &quot;gdp_per_capita&quot;, &quot;hosp_beds&quot;] demo_vars column별로 결측치를 측정1covidtotals[demo_vars].isnull().sum(axis = 0) # column별로 결측치를 측정 case_vars column별로 결측치를 측정1covidtotals[case_vars].isnull().sum(axis = 0) # column별로 결측치를 측정 case_vars 에는 결측치가 없지만, demo_vars에는 결측치가 있는 것을 확인 할 수 있다. pop_density 12 median_age 24 gdp_per_capita 28 hosp_beds 46 위의 column들에 각각 수만큼의 결측치를 확인 할 수 있다. 행 방향으로 발생한 결측치 확인12demovars_misscnt = covidtotals[demo_vars].isnull().sum(axis = 1)demovars_misscnt.value_counts() 0 156 1 242 123 104 8dtype: int64 1covidtotals[case_vars].isnull().sum(axis = 1).value_counts() 0 210dtype: int64 인구통계 데이터가 3가지 이상 누락된 국가를 나열하기123[&quot;location&quot;] + demo_varscovidtotals.loc[demovars_misscnt &gt;= 3, [&quot;location&quot;] + demo_vars].T case에는 누락국가가 없지만, 그냥 한번 확인12casevars_misscnt = covidtotals[case_vars].isnull().sum(axis = 1)casevars_misscnt.value_counts() 0 210dtype: int64 1covidtotals[covidtotals['location'] == &quot;Hong Kong&quot;] 123456temp = covidtotals.copy()temp[case_vars].isnull().sum(axis = 0)temp.total_cases_pm.fillna(0, inplace = True)temp.total_deaths_pm.fillna(0, inplace = True)temp[case_vars].isnull().sum(axis = 0) 이건 잘 모르겠다. 그냥 삭제 할 수 있다.","link":"/2021/12/22/python/DTS_MissingValue2/"},{"title":"DTS: Outlier detection02","text":"§ 결측치 찾기 이론 § 결측치 찾기 실습 § 변수 1개를 이용하여 이상값 찾기 § 변수 2개를 이용하여 이상값 찾기 § data 출처이상값 찾기 서로 겹치는 값이 있거나, 한 변수의 범주거나 연속일 경우 수치형 데이터에 대한 상관행렬 12# 상관관계 확인covidtotals.corr(method = &quot;pearson&quot;) corr &lt;|0.2| : 약한 상관관계 corr &lt; |0.3~0.6| : 중간정도의 상관관계 상관관계를 확인 할 수 있다. crosstab 총 사망자 분위수별 총 확진자 분위수의 크로스 탭 표시 case: 확진자수 deaths: 사망자 수 12pd.crosstab(covidtotalsonly[&quot;total_cases_q&quot;], covidtotalsonly[&quot;total_deaths_q&quot;]) 매우 낮은 수로 사망 했지만, 확진이 중간 = 이상치 12covidtotals.loc[(covidtotalsonly[&quot;total_cases_q&quot;]== &quot;very high&quot;) &amp; (covidtotalsonly[&quot;total_deaths_q&quot;]== &quot;medium&quot;)].T 1234567fig, ax = plt.subplots()sns.regplot(x = &quot;total_cases_pm&quot;, y = &quot;total_deaths_pm&quot;, data = covidtotals, ax = ax)ax.set(xlabel = &quot;Cases Per Million&quot;, ylabel = &quot;Deaths Per Million&quot;, title = &quot;Total Covid Cases and Deaths per Million by Country&quot;)ax.ticklabel_format(axis = &quot;x&quot;, useOffset=False, style = &quot;plain&quot;)plt.xticks(rotation=90)plt.show()","link":"/2021/12/22/python/DTS_Outlier_Detection2/"},{"title":"DTS: Outlier detection01","text":"§ 결측치 찾기 이론 § 결측치 찾기 실습 § 변수 1개를 이용하여 이상값 찾기 § 변수 2개를 이용하여 이상값 찾기 이상값 찾기 주관적이며 연구자 마다 다르고, 산업에 따라 차이가 있다. 통계에서의 이상값 정규 분포를 이루고 있지 않음 : 이상값이 존재 왜도, 첨도가 발생. 균등분포(Uniform distribution) 1. 변수 1개를 이용하여 이상값 찾기 12345678910111213import numpy as npimport matplotlib.pyplot as pltimport pandas as pdimport statsmodels.api as sm # 검정 확인을 위한 그래프 import scipy.stats as scistat #샤피로 검정을 위한 Librarycovidtotals = pd.read_csv(&quot;../input/covid-data/covidtotals.csv&quot;)covidtotals.set_index(&quot;iso_code&quot;, inplace = True)case_vars = [&quot;location&quot;, &quot;total_cases&quot;, &quot;total_deaths&quot;, &quot;total_cases_pm&quot;, &quot;total_deaths_pm&quot;]demo_vars = [&quot;population&quot;, &quot;pop_density&quot;, &quot;median_age&quot;, &quot;gdp_per_capita&quot;, &quot;hosp_beds&quot;]covidtotals.head() 결측치와 마찬가지로 covidtotals data를 kaggle note에 불러와서 실행 백분위수(quantile)로 데이터 표시 판다스 내부의 함수를 이용하여 확인한다. 12345covid_case_df = covidtotals.loc[:, case_vars]covid_case_df.describecovid_case_df.quantile(np.arange(0.0, 1.1, 0.1))#Index이기 때문에 1.1로 표시 왜도(대칭 정도), 첨도(뾰족한 정도) 구하기 역시 pandas 함수를 이용. 들어가기 전에 pandas.DataFrame.skew 위와 같은 Warring Error가 발생 하면, 구글링을 통해 처리 할 수 있어야 한다. 왜도 구하기1covid_case_df.skew(axis=0, numeric_only = True) total_cases 10.804275 total_deaths 8.929816 total_cases_pm 4.396091 total_deaths_pm 4.674417 dtype: float64 -1~1사이에 있어야 대칭이다. skewness &lt; |3| : 기본적 허용 대칭이 아닌 것을 알 수 있다.( = 정규분포가 아니다. ) 첨도 구하기 정규 분포의 첨도는 0이다. 0보다 크면 더 뾰족하고 0보다 작으면 뭉툭하다. 12#첨도 구하기 covid_case_df.kurtosis(axis=0, numeric_only = True) total_cases 134.979577 total_deaths 95.737841 total_cases_pm 25.242790 total_deaths_pm 27.238232 dtype: float64 5~10 정도 사이에 첨도가 있어야 하는데 정규분포를 이루고 있지 않다. kurtosis &lt; |7| : 기본적 허용 ( = 정규분포가 아니다. ) 이산값이 있을 확률이 높다는 것을 알 수 있다. 정규성 검정 테스트 정규성 가정을 검토하는 방법 Q-Q plot 그래프로 정규성 확인 눈으로 보는 것이기 때문에 해석이 주관적. Shapiro-Wilk Test (샤피로-윌크 검정) 귀무가설 : 표본의 모집단이 정규 분포를 이루고 있다. (H0: 정규분포를 따른다 p-value &gt; 0.05) 대립가설 : 표본의 모집단이 정규 분포를 이루고 있지 않다. p value &lt; 0.05 : 귀무가설을 충족하지 않아 대립가설로 Kolnogorov-Smirnov test (콜모고로프-스미노프 검정) EDF(Empirical distribution fuction)에 기반한 적합도 검정방법 자료의 평균/표준편차, Histogram을 통해 표준 정규분포와 비교하여 적합도 검정. p value &gt; 0.05 : 정규성 가정 Shapiro-Wilk Test12# 샤피로 검정scistat.shapiro(covid_case_df['total_cases']) ShapiroResult(statistic=0.19379639625549316, pvalue=3.753789128593843e-29) 우리는 p value 를 가지고 유의성을 확인한다. p value : 3.75e-29 이므로 정규분포를 이루지 않음. covid_case_df[‘total_cases’] 안에 아래 column들을 하나씩 다 넣어 봐야 한다. 12case_vars = [&quot;location&quot;, &quot;total_cases&quot;, &quot;total_deaths&quot;, &quot;total_cases_pm&quot;, &quot;total_deaths_pm&quot;]demo_vars = [&quot;population&quot;, &quot;pop_density&quot;, &quot;median_age&quot;, &quot;gdp_per_capita&quot;, &quot;hosp_beds&quot;] 함수를 짜면 너의 code가 될 것이라고 한다. qqplot 통계적 이상값 범위 : 1사분위 (25%), 3사분위(75%) 사이의 거리 그 거리가 상하좌우 1.5배를 넘으면 이상값으로 여김 123sm.qqplot(covid_case_df[[&quot;total_cases&quot;]].sort_values( [&quot;total_cases&quot;]), line = 's')plt.title(&quot;Total Class&quot;) 12345678thirdq = covid_case_df[&quot;total_cases&quot;].quantile(0.75)firstq = covid_case_df[&quot;total_cases&quot;].quantile(0.25)interquantile_range = 1.5 * (thirdq- firstq)outlier_high = interquantile_range + thirdqoutliner_low = firstq - interquantile_rangeprint(outliner_low, outlier_high, sep = &quot; &lt;-------&gt; &quot;) -14736.125 &lt;——-&gt; 25028.875 이상치를 제거한 data 가져오기 조건: outlier_high 보다 높은 이상치 or outlier_low 보다 낮은 이상치 12remove_outlier_df = covid_case_df.loc[~(covid_case_df[&quot;total_cases&quot;]&gt;outlier_high)|(covid_case_df[&quot;total_cases&quot;]&lt;outliner_low)]remove_outlier_df.info() 이상치 data 12remove_outlier_df = covid_case_df.loc[(covid_case_df[&quot;total_cases&quot;]&gt;outlier_high)|(covid_case_df[&quot;total_cases&quot;]&lt;outliner_low)]remove_outlier_df.info() 12345678910fig, ax = plt.subplots(figsize = (16, 6), ncols = 2)ax[0].hist(covid_case_df[&quot;total_cases&quot;]/1000, bins = 7)ax[0].set_title(&quot;Total Covid Cases (thousands) for all&quot;)ax[0].set_xlabel(&quot;Cases&quot;)ax[0].set_ylabel(&quot;Number of Countries&quot;)ax[1].hist(remove_outlier_df[&quot;total_cases&quot;]/1000, bins = 7)ax[1].set_title(&quot;Total Covid Cases (thousands) for removed outlier&quot;)ax[1].set_xlabel(&quot;Cases&quot;)ax[1].set_ylabel(&quot;Number of Countries&quot;)plt.show() 완벽하진 않지만, 먼 잔차들을 제거한 정규 분포를 이루는 듯한 그래프를 얻을 수 있었다. 이를 train data에 EDA로 돌리고, ML을 진행 하면 더 좋은 score를 얻을 수도 있고, 아닐 수도 있다. just Test","link":"/2021/12/21/python/DTS_Outlier_detection/"},{"title":"DTS: PipeLine 만들고 활용하기","text":"§ 다음 posting ☞ PipeLine ☞ Learning curve sklearn.pipeline.Pipeline class sklearn.pipeline.Pipeline(steps, *, memory=None, verbose=False) data : ref Model을 바로 확인 하기 어렵다. 과대적합 하는지 확인 하기 위해 pipeLine을 이용하여 쉽게 파악 할 수 있다. mlops? 때문이다. sklearn.pipeline pipeLine : 최종 추정을 위한 변환 파이프라인 매개변수를 바꿔가며 교차 검증 할 수 있는 여러 단계를 묶어 놓아 하나의 함수로 만들어 사용하기 쉽게 한 것. 해당 이름의 매개 변수를chaining estimators 을 위해 설정하거나,제거 할 수 있다. convenience and encapsulation joint parameter selection safety 뭘 한건지 모르겠지만, 오늘 할 것 정리 해 보자 .12345import pandas as pdfrom sklearn.preprocessing import LabelEncoderfrom sklearn.model_selection import train_test_splitfrom sklearn.model_selection import StratifiedKFoldimport numpy as np 일단 sklearn을 이용한 ML을 하기 위해 library를 import 해 보자. data 불러오기12345678data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data'column_name = ['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst']df = pd.read_csv(data_url, names=column_name)print(df.info()) test, Train 나누기12345678X = df.loc[:, &quot;radius_mean&quot;:].valuesy = df.loc[:, &quot;diagnosis&quot;].valuesle = LabelEncoder()y = le.fit_transform(y)print(&quot;종속변수 클래스:&quot;, le.classes_)X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, stratify = y, random_state=1) 이 코드 하나가 pipe Line LogisticRegression 1234from sklearn.linear_model import LogisticRegressionpipe_lr = make_pipeline(StandardScaler(), PCA(n_components=2), LogisticRegression(solver=&quot;liblinear&quot;, random_state=1)) DecisionTreeClassifier 1234from sklearn.tree import DecisionTreeClassifierpipe_lr = make_pipeline(StandardScaler(), PCA(n_components=2), DecisionTreeClassifier(random_state=0)) LGBM 12345from lightgbm import LGBMClassifierpipe_lr = make_pipeline(StandardScaler(), PCA(n_components=2), LGBMClassifier(objective='multiclass', random_state=5)) LGBMC : 이거 아닌거같은데 못봤다. 안됨 여튼 이런식으로 바꿔 끼워가며 확인 할 수 있다. pipeLine만들기12345678910111213141516171819202122232425262728from sklearn.linear_model import LogisticRegressionfrom sklearn.pipeline import make_pipelinefrom sklearn.preprocessing import StandardScalerfrom sklearn.decomposition import PCApipe_lr = make_pipeline(StandardScaler(), PCA(n_components=2), LogisticRegression(solver=&quot;liblinear&quot;, random_state=1))kfold = StratifiedKFold(n_splits = 10, random_state=1, shuffle=True).split(X_train, y_train)scores = []for k, (train, test) in enumerate(kfold): pipe_lr.fit(X_train[train], y_train[train]) score = pipe_lr.score(X_train[test], y_train[test]) scores.append(score) print(&quot;폴드: %2d, 클래스 분포: %s, 정확도: %.3f&quot; % (k+1, np.bincount(y_train[train]), score))print(&quot;\\nCV 정확도: %.3f +/- %.3f&quot; % (np.mean(scores), np.std(scores)))from sklearn.model_selection import cross_val_scorescores = cross_val_score(estimator=pipe_lr, X = X_train, y = y_train, cv = 10, n_jobs = 1)print(&quot;CV 정확도 점수 : %s&quot; % scores)print(&quot;CV 정확도 : %.3f +/- %.3f&quot; % (np.mean(scores), np.std(scores))) Kaggle data랑 뭐가 다른지 확인 해 보라고 하는데 Kaggle에서 어디 있는지 잘 모르겠다. 자바는 어느정도 감이 왔는데 python은 당최 아얘 감조차 안온다. 그냥 python 강의나 들어야 하나 고민중 …","link":"/2021/12/22/python/DTS_PipeLine/"},{"title":"DecisionTreeMachineLearning(03)","text":"machine Learning Model Algoridms 비 선형 모델 : KNN, 선형 모델 : Decision Tree MachineLearning Introduction 과적합 : 모델의 정확도만 높이기 위해 분류 조건(depth)만 강조하여 실제 상황에서 유연하게 대처하는 능력이 떨어지게 되는 문제가 발생하게 되는것. 가지치기(pruning)을 통해 유연성을 유지. Max_depth를 대략적으로 잡아서 (3, 5, 10…) RMS 값 비교 Random search 하이퍼파라미터 (grid Search) 분류기준 (수식은 아래서 책에서 확인) 정보이득 : 자식노드의 불순도가 낮을 수록 정보의 이득이 커진다.(효율성 Up) 정보 이득이 높은 속성을 기준으로 알아서 나누어 준다. 엔트로피의 정의 : 엔트로피는 높을 수록 좋다. 지니불순도 : 순도는 높을 수록 좋다. 분류오차 : 어떤 시나리오가 더 좋은가에 대한 계산 1이 되면 균등, 완벽하게 나누어 졌다고 ㅇㅇ 공식은 이쪽에 가면 있다. 계산은 컴퓨터가 다 해준다. 우리는 보고 좋은 분류 기준을 선택 하며 됩니다. 분류기준 1. 분류 오차 분류기준 2. 지니 불순도 분류기준 2. 엔트로피 정보이득을 최대로 하는 옵션을 찾는다. 실습12345678from sklearn import datasets import numpy as np iris = datasets.load_iris()X = iris.data[:, [2, 3]]y = iris.target print(&quot;클래스 레이블:&quot;, np.unique(y)) 클래스 레이블: [0 1 2] 1234567from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.3, random_state = 1)print(&quot;y 레이블 갯수:&quot;, np.bincount(y)) y 레이블 갯수: [50 50 50] 시각화12345678910111213141516171819202122232425262728293031323334353637383940414243from matplotlib.colors import ListedColormapimport matplotlib.pyplot as pltdef plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02): # 마커와 컬러맵을 설정합니다. markers = ('s', 'x', 'o', '^', 'v') colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan') cmap = ListedColormap(colors[:len(np.unique(y))]) # 결정 경계를 그립니다. x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1 x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution)) Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T) Z = Z.reshape(xx1.shape) plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap) plt.xlim(xx1.min(), xx1.max()) plt.ylim(xx2.min(), xx2.max()) for idx, cl in enumerate(np.unique(y)): plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8, c=colors[idx], marker=markers[idx], label=cl, edgecolor='black') # 테스트 샘플을 부각하여 그립니다. if test_idx: X_test, y_test = X[test_idx, :], y[test_idx] plt.scatter(X_test[:, 0], X_test[:, 1], c='', edgecolor='black', alpha=1.0, linewidth=1, marker='o', s=100, label='test set') 12345678910111213141516171819202122232425262728293031323334353637383940import matplotlib.pyplot as pltimport numpy as np# 지니 불순도 함수def gini(p): return p * (1 - p) + (1 - p) * (1 - (1 - p))# 엔트로피 함수 def entropy(p): return - p * np.log2(p) - (1 - p) * np.log2((1 - p))# 분류 오차def error(p): return 1 - np.max([p, 1 - p])x = np.arange(0.0, 1.0, 0.01)ent = [entropy(p) if p != 0 else None for p in x]sc_ent = [e * 0.5 if e else None for e in ent]err = [error(i) for i in x]fig = plt.figure()ax = plt.subplot(111)for i, lab, ls, c, in zip([ent, sc_ent, gini(x), err], ['Entropy', 'Entropy (scaled)', 'Gini Impurity', 'Misclassification Error'], ['-', '-', '--', '-.'], ['black', 'lightgray', 'red', 'green', 'cyan']): line = ax.plot(x, i, label=lab, linestyle=ls, lw=2, color=c)ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=5, fancybox=True, shadow=False)ax.axhline(y=0.5, linewidth=1, color='k', linestyle='--')ax.axhline(y=1.0, linewidth=1, color='k', linestyle='--')plt.ylim([0, 1.1])plt.xlabel('p(i=1)')plt.ylabel('Impurity Index')plt.show() 정보 이득을 최대로 하는 옵션을 찾아서 1234from sklearn.tree import DecisionTreeClassifiertree_gini = DecisionTreeClassifier(criterion=&quot;gini&quot;, max_depth=3)tree_gini.fit(X_train, y_train) DecisionTreeClassifier(max_depth=3) depth를 3으로 해 주었기 때문에 과적합 X 123456789X_combined = np.vstack((X_train, X_test))y_combined = np.hstack((y_train, y_test))plot_decision_regions(X_combined, y_combined, classifier=tree_gini, test_idx = range(105, 150))plt.xlabel(&quot;petal length&quot;)plt.ylabel(&quot;petal width&quot;)plt.legend(loc = &quot;upper left&quot;)plt.tight_layout()plt.show() 1234567891011121314from pydotplus import graph_from_dot_datafrom sklearn.tree import export_graphvizdot_data = export_graphviz(tree_gini, filled=True, rounded=True, class_names=['Setosa', 'Versicolor', 'Virginica'], feature_names=['petal length', 'petal width'], out_file=None) graph = graph_from_dot_data(dot_data) graph.write_png('gini_tree.png') True gini 로 1개 Entripy 로 1개 짜서 해야함 gini: default Entropy : 도 해보고 비교 123456789101112tree_entropy = DecisionTreeClassifier(criterion=&quot;entropy&quot;, max_depth=3)tree_entropy.fit(X_train, y_train)X_combined = np.vstack((X_train, X_test))y_combined = np.hstack((y_train, y_test))plot_decision_regions(X_combined, y_combined, classifier=tree_entropy, test_idx = range(105, 150))plt.xlabel(&quot;petal length&quot;)plt.ylabel(&quot;petal width&quot;)plt.legend(loc = &quot;upper left&quot;)plt.tight_layout()plt.show() 모형을 도식화로 1234567891011121314from pydotplus import graph_from_dot_datafrom sklearn.tree import export_graphvizdot_data = export_graphviz(tree_entropy, filled=True, rounded=True, class_names=['Setosa', 'Versicolor', 'Virginica'], feature_names=['petal length', 'petal width'], out_file=None) graph = graph_from_dot_data(dot_data) graph.write_png('Entropy_tree.png') entropy가 0이 되면 더이상 나눌 필요가 없다. sklearn에서는 분류오차는 없다. 지니 와 엔트로피 두개를 보고 더 나은 것을 선택 머신러닝 배우기 &lt;아직 안배운 부분&gt; 스태킹 알고리즘 (앙상블)","link":"/2021/12/10/python/DecisionTreeMachineLearning/"},{"title":"python_basic_Exeption","text":"pythonException123456789101112131415161718192021222324252627282930# /c/Users/brill/Desktop/PyThon_Function/venv/Scripts/python# -*- coding : UTF-8def error01(): a=10 a/0 #ZeroDivisionError: division by zerodef error02(): a= [1, 2, 3, 4, 5] a[10] #IndexError: list index out of rangedef error03(): a = 1000 a + &quot;Hello&quot; #TypeError: unsupported operand type(s) for +: 'int' and 'str'def error04(): a=10 a+b #NameError: name 'b' is not definedif __name__ == &quot;__main__&quot;: error01() error02() error03() error04() print(&quot;program is done&quot;) 크롤링 코드를 작성 했을때 “https://sports.news.naver.com/news?oid=109&amp;aid=0004526080&quot; : 페이지있음,“https://sports.news.naver.com/news?oid=109&amp;aid=0004526081&quot; : 페이지가 없다면,“https://sports.news.naver.com/news?oid=109&amp;aid=0005526080&quot; : 페이지 있음, 크롤링 코드 멈춤 프로그램이 멈춰서 안됨 Exeption의 종류 java 의 try catch 구문과 같음1234567891011121314151617181920212223242526272829303132333435# /c/Users/brill/Desktop/PyThon_Function/venv/Scripts/python# -*- coding : UTF-8def try_func(x, idx): try: return 100/x[idx] except ZeroDivisionError: print(&quot;did't divide zero&quot;) except IndexError: print(&quot;not in range of Index&quot;) except TypeError: print(&quot;there is type Error&quot;) except NameError: print(&quot;it is not definated parameter&quot;) finally: print(&quot;무조건 실행됨&quot;)def main(): a = [50, 60, 0, 70] print(try_func(a,1)) # Zero Division Error print(try_func(a,0)) # Index Error print(try_func(a,5)) # type Error print(try_func(a, &quot;hi&quot;))if __name__ == &quot;__main__&quot;: main() 어떻게던 프로그램이 돌아 갈 수 있도록만들어 주는 것이 중요하다. class 정리 __init__ : set_name, set_id 해 주지 않고, 통합시켜주는 역할 __eq__, __ne__ : 부등호 연산자 상속, 다형성(서로다른 클래스에서 공통으로 쓰는 함수) Exception class attribute / instance attribute / instance method 차이 추상 class (안배웠음) data incapsulation 12345678910111213141516171819202122232425262728293031323334353637# /c/Users/brill/Desktop/PyThon_Function/venv/Scripts/python# -*- coding : UTF-8class SalaryExcept(ValueError): pass # 상속class TipExept(SalaryExcept): pass # 상속class Employee: MIN_SALARY = 30000 MAX_Bonus = 20000 def __init__(self, name, salary = 30000): self.name = name if salary&lt; Employee.MIN_SALARY: raise SalaryExcept(&quot;급여가 너무 낮아요!&quot;) self.salary = salary def give_bonus(self, amount): if amount &gt; Employee.MAX_Bonus: print(&quot;보너스가 너무 많아 &quot;) elif self.salary + amount &lt; Employee.MIN_SALARY : print(&quot;보너스 지급 후의 급여도 매우 낮다. &quot;) else: self.salary += amountif __name__ == &quot;__main__&quot;: emp = Employee(&quot;YH&quot;, salary= 10000) try: emp.give_bonus(70000) except SalaryExcept: print(&quot;Error Salary&quot;) try: emp.give_bonus(-10000) except tipExcept: print(&quot;Error Tip&quot;) 여전히 Error가 나는 코드나는 Exception 안됨","link":"/2021/12/14/python/Exeption_python/"},{"title":"DTS: ML_Grid search(Hyper Parameter)","text":"§ 이전 posting ☞ PipeLine ☞ Learning curve ML pipeLine 검증 곡선 그리기 ML 그리드 서치 grid search를 이용한 파이프라인(pipeLine) 설계및하이퍼 파라미터 튜닝(hyper parameter) 그리드 서치와 랜덤 서치가 있다. 랜덤 서치로 먼저 뽑아 낸 후 그리드 서치를 이용하여 안정적으로 서치 ! 나도 공부 하기 싫으닌까 그냥 남 이 하는거 따라 쓰고 싶다. 남 : Kaggle competition 1234567891011121314151617181920212223242526272829303132333435363738import pandas as pd from sklearn.preprocessing import LabelEncoderfrom sklearn.model_selection import train_test_splitimport numpy as npfrom sklearn.model_selection import StratifiedKFoldfrom sklearn.linear_model import LogisticRegressionfrom sklearn.pipeline import make_pipelinefrom sklearn.preprocessing import StandardScalerfrom sklearn.decomposition import PCAimport matplotlib.pyplot as plt from sklearn.model_selection import learning_curvefrom sklearn.model_selection import validation_curvefrom sklearn.tree import DecisionTreeClassifierfrom sklearn.model_selection import GridSearchCV from sklearn.svm import SVC data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data'column_name = ['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst']df = pd.read_csv(data_url, names=column_name)X = df.loc[:, &quot;radius_mean&quot;:].valuesy = df.loc[:, &quot;diagnosis&quot;].valuesle = LabelEncoder()y = le.fit_transform(y)X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, # stratify = y, random_state=1)kfold = StratifiedKFold(n_splits = 10, random_state=1, shuffle=True)pipe_tree = make_pipeline(StandardScaler(), PCA(n_components=2), DecisionTreeClassifier(random_state=1)) 123456789101112131415161718192021# 이 Line이 핵쉼 !!# estimator.get_params().keys()# pipe_tree.get_params().keys() ---&gt; 이렇게 씀. print(pipe_tree.get_params().keys())param_grid = [{&quot;decisiontreeclassifier__max_depth&quot;: [1, 2, 3, 4, 5, 6, 7, None]}]gs = GridSearchCV(estimator = pipe_tree, param_grid = param_grid, scoring=&quot;accuracy&quot;, cv = kfold)gs = gs.fit(X_train, y_train)print(gs.best_score_)print(gs.best_params_)clf = gs.best_estimator_# 자동으로 제일 좋은 것을 뽑아서 알려줌.clf.fit(X_train, y_train) print(&quot;테스트 정확도:&quot;, clf.score(X_test, y_test)) dict_keys([‘memory’, ‘steps’, ‘verbose’, ‘standardscaler’, ‘pca’, ‘decisiontreeclassifier’, ‘standardscaler__copy’, ‘standardscaler__with_mean’, ‘standardscaler__with_std’, ‘pca__copy’, ‘pca__iterated_power’, ‘pca__n_components’, ‘pca__random_state’, ‘pca__svd_solver’, ‘pca__tol’, ‘pca__whiten’, ‘decisiontreeclassifier__ccp_alpha’, ‘decisiontreeclassifier__class_weight’, ‘decisiontreeclassifier__criterion’, ‘decisiontreeclassifier__max_depth’, ‘decisiontreeclassifier__max_features’, ‘decisiontreeclassifier__max_leaf_nodes’, ‘decisiontreeclassifier__min_impurity_decrease’, ‘decisiontreeclassifier__min_samples_leaf’, ‘decisiontreeclassifier__min_samples_split’, ‘decisiontreeclassifier__min_weight_fraction_leaf’, ‘decisiontreeclassifier__random_state’, ‘decisiontreeclassifier__splitter’])0.927536231884058{‘decisiontreeclassifier__max_depth’: 7}테스트 정확도: 0.9210526315789473 svc를 이용한 hyperparameter tuenning12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import pandas as pd from sklearn.preprocessing import LabelEncoderfrom sklearn.model_selection import train_test_splitimport numpy as npfrom sklearn.model_selection import StratifiedKFoldfrom sklearn.linear_model import LogisticRegressionfrom sklearn.pipeline import make_pipelinefrom sklearn.preprocessing import StandardScalerfrom sklearn.decomposition import PCAimport matplotlib.pyplot as plt from sklearn.model_selection import learning_curvefrom sklearn.model_selection import validation_curvefrom lightgbm import LGBMClassifierfrom sklearn.model_selection import GridSearchCV from sklearn.svm import SVC data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data'column_name = ['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst']df = pd.read_csv(data_url, names=column_name)X = df.loc[:, &quot;radius_mean&quot;:].valuesy = df.loc[:, &quot;diagnosis&quot;].valuesle = LabelEncoder()y = le.fit_transform(y)X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, # stratify = y, random_state=1)pipe_svc = make_pipeline(StandardScaler(), PCA(n_components=2), SVC(random_state=1))param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]param_grid = [{&quot;svc__C&quot;: param_range, &quot;svc__gamma&quot;: param_range, &quot;svc__kernel&quot;: [&quot;linear&quot;]}]gs = GridSearchCV(estimator = pipe_svc, param_grid = param_grid, scoring=&quot;accuracy&quot;, cv = 10)gs = gs.fit(X_train, y_train)print(gs.best_score_)print(gs.best_params_)clf = gs.best_estimator_clf.fit(X_train, y_train) print(&quot;테스트 정확도:&quot;, clf.score(X_test, y_test)) 효효효","link":"/2021/12/25/python/ML_GridWearch_HP/"},{"title":"DTS: ML_Learning CurveG(01)","text":"§ 이전 posting ☞ PipeLine § 다음 posting ☞ PipeLine Learning curve 그리기 pipeLine 이용하여 ML 돌림 이후 ML 을 확인 하기 위해 Learning, validation curve를 그려 확인 일반적으로 두 curve 를 함께 그린다. data 불러오기, 훈련 세트 분리, 교차검증 정의12345678910111213141516171819202122232425262728293031import pandas as pd from sklearn.preprocessing import LabelEncoderfrom sklearn.model_selection import train_test_splitdata_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data'column_name = ['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst']df = pd.read_csv(data_url, names=column_name)print(df.info())X = df.loc[:, &quot;radius_mean&quot;:].valuesy = df.loc[:, &quot;diagnosis&quot;].valuesle = LabelEncoder()y = le.fit_transform(y)print(&quot;종속변수 클래스:&quot;, le.classes_)X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, stratify = y, random_state=1)from sklearn.linear_model import LogisticRegressionfrom sklearn.pipeline import make_pipelinefrom sklearn.preprocessing import StandardScalerfrom sklearn.decomposition import PCApipe_lr = make_pipeline(StandardScaler(), PCA(n_components=2), LogisticRegression(solver=&quot;liblinear&quot;, random_state=1)) Learning curve 결과 값 구하기12345678910111213141516171819from sklearn.model_selection import learning_curveimport numpy as nptrain_sizes, train_scores, test_scores = learning_curve( estimator = pipe_lr, X = X_train, y = y_train, train_sizes = np.linspace(0.1, 1.0, 10), cv = 10)train_mean = np.mean(train_scores, axis = 1)train_std = np.std(train_scores, axis = 1)test_mean = np.mean(test_scores, axis = 1)test_std = np.std(test_scores, axis = 1)print(&quot;mean(test)-----------------\\n&quot;, train_mean,&quot;\\n mean(train)-----------------\\n&quot;,test_mean )print(&quot;STD(test)-----------------\\n&quot;, train_std,&quot;\\n STD(train)-----------------\\n&quot;,test_std ) mean(test)—————– [0.9525 0.96049383 0.93032787 0.92822086 0.93382353 0.93469388 0.94090909 0.94740061 0.94945652 0.95378973] mean(train)—————– [0.92763285 0.92763285 0.93415459 0.93415459 0.93855072 0.94516908 0.94956522 0.947343 0.94516908 0.94956522]STD(test)—————– [0.0075 0.00493827 0.00839914 0.01132895 0.00395209 0.00730145 0.00862865 0.0072109 0.00656687 0.00632397] STD(train)—————– [0.0350718 0.02911549 0.02165313 0.02743013 0.02529372 0.02426857 0.0238436 0.02421442 0.02789264 0.02919026] Learning Curve Graph12345678910111213141516171819202122232425262728293031323334import matplotlib.pyplot as pltfig, ax = plt.subplots(figsize = (8,5))ax.plot(train_sizes, train_mean, color = &quot;blue&quot;, marker = &quot;o&quot;, markersize = 10, label = &quot;training acc.&quot;)ax.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha = 0.15, color = &quot;darkblue&quot;)ax.plot(train_sizes, test_mean, color = &quot;green&quot;, marker = &quot;s&quot;, linestyle = &quot;--&quot;, # 점선으로 표시 markersize = 10, label = &quot;testing acc.&quot;)ax.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha = 0.15, color = &quot;salmon&quot;)plt.grid()plt.xlabel(&quot;Number of training samples&quot;)plt.ylabel(&quot;Accuracy&quot;)plt.legend(loc = &quot;lower right&quot;)plt.ylim([0.8, 1.03])plt.tight_layout()plt.show# sample 수가 많아지면, 점점 가까워 진다. 분야 좋은데 인거 알겠고, 재미있는데 참 ㅎㅎ","link":"/2021/12/24/python/ML_LearningCurveG(01)/"},{"title":"DTS: ML_Validation CurveG(01)","text":"§ 이전 posting ☞ PipeLine ☞ Learning curve 검증 곡선 그려 보기123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import pandas as pd from sklearn.preprocessing import LabelEncoderfrom sklearn.model_selection import train_test_splitimport numpy as npfrom sklearn.model_selection import StratifiedKFoldfrom sklearn.linear_model import LogisticRegressionfrom sklearn.pipeline import make_pipelinefrom sklearn.preprocessing import StandardScalerfrom sklearn.decomposition import PCAimport matplotlib.pyplot as plt from sklearn.model_selection import learning_curvefrom sklearn.model_selection import validation_curvefrom lightgbm import LGBMClassifierdata_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data'column_name = ['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst']df = pd.read_csv(data_url, names=column_name)X = df.loc[:, &quot;radius_mean&quot;:].valuesy = df.loc[:, &quot;diagnosis&quot;].valuesle = LabelEncoder()y = le.fit_transform(y)X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, # stratify = y, random_state=1)kfold = StratifiedKFold(n_splits = 10, random_state=1, shuffle=True)pipe_lr = make_pipeline(StandardScaler(), PCA(n_components=2), LogisticRegression(solver = &quot;liblinear&quot;, penalty = &quot;l2&quot;, random_state=1))param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]train_scores, test_scores = validation_curve(estimator=pipe_lr, X = X_train, y = y_train, param_name = &quot;logisticregression__C&quot;, param_range = param_range, cv = kfold)train_mean = np.mean(train_scores, axis = 1)train_std = np.std(train_scores, axis = 1)test_mean = np.mean(test_scores, axis = 1)test_std = np.std(test_scores, axis = 1)fig, ax = plt.subplots(figsize = (16, 10))ax.plot(param_range, train_mean, color = &quot;blue&quot;, marker = &quot;o&quot;, markersize=5, label = &quot;training accuracy&quot;)ax.fill_between(param_range, train_mean + train_std, train_mean - train_std, alpha = 0.15, color = &quot;blue&quot;) # 추정 분산ax.plot(param_range, test_mean, color = &quot;green&quot;, marker = &quot;s&quot;, linestyle = &quot;--&quot;, markersize=5, label = &quot;Validation accuracy&quot;)ax.fill_between(param_range, test_mean + test_std, test_mean - test_std, alpha = 0.15, color = &quot;green&quot;)plt.grid()plt.xscale(&quot;log&quot;)plt.xlabel(&quot;Parameter C&quot;)plt.ylabel(&quot;Accuracy&quot;)plt.legend(loc = &quot;lower right&quot;)plt.ylim([0.8, 1.03])plt.tight_layout()plt.show() data 불러오기12345678910111213141516171819202122232425import pandas as pd from sklearn.preprocessing import LabelEncoderfrom sklearn.model_selection import train_test_splitimport numpy as npfrom sklearn.model_selection import StratifiedKFoldfrom sklearn.linear_model import LogisticRegressionfrom sklearn.pipeline import make_pipelinefrom sklearn.preprocessing import StandardScalerfrom sklearn.decomposition import PCAimport matplotlib.pyplot as plt from sklearn.model_selection import learning_curvefrom sklearn.model_selection import validation_curvefrom lightgbm import LGBMClassifierdata_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data'column_name = ['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst']df = pd.read_csv(data_url, names=column_name)X = df.loc[:, &quot;radius_mean&quot;:].valuesy = df.loc[:, &quot;diagnosis&quot;].values train, test 나누고 pipe line 설계12345678910111213le = LabelEncoder()y = le.fit_transform(y)X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, # stratify = y, random_state=1)kfold = StratifiedKFold(n_splits = 10, random_state=1, shuffle=True)pipe_lr = make_pipeline(StandardScaler(), PCA(n_components=2), LogisticRegression(solver = &quot;liblinear&quot;, penalty = &quot;l2&quot;, random_state=1)) 그리드 서치123456789101112131415161718192021222324252627param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]train_scores, test_scores = validation_curve(estimator=pipe_lr, X = X_train, y = y_train, param_name = &quot;logisticregression__C&quot;, param_range = param_range, cv = kfold)train_mean = np.mean(train_scores, axis = 1)train_std = np.std(train_scores, axis = 1)test_mean = np.mean(test_scores, axis = 1)test_std = np.std(test_scores, axis = 1)fig, ax = plt.subplots(figsize = (8, 5))ax.plot(param_range, train_mean, color = &quot;blue&quot;, marker = &quot;o&quot;, markersize=5, label = &quot;training accuracy&quot;)ax.fill_between(param_range, train_mean + train_std, train_mean - train_std, alpha = 0.15, color = &quot;blue&quot;) # 추정 분산ax.plot(param_range, test_mean, color = &quot;green&quot;, marker = &quot;s&quot;, linestyle = &quot;--&quot;, markersize=5, label = &quot;Validation accuracy&quot;)ax.fill_between(param_range, test_mean + test_std, test_mean - test_std, alpha = 0.15, color = &quot;green&quot;)plt.grid()plt.xscale(&quot;log&quot;)plt.xlabel(&quot;Parameter C&quot;)plt.ylabel(&quot;Accuracy&quot;)plt.legend(loc = &quot;lower right&quot;)plt.ylim([0.8, 1.03])plt.tight_layout()plt.show()","link":"/2021/12/24/python/ML_ValidationCurveG(01)/"},{"title":"Text Mining in Python","text":"개요 빅데이터 분석 및 시각화 &amp; 텍스트 마이닝 Ref01_ Matplotlib 히스토그램 그리기 Ref02_ 딥 러닝을 이용한 자연어 처리 입문 네이버 쇼핑 리뷰 감성 분류하기(Naver Shopping Review Sentiment Analysis) 평가 다음은 네이버 쇼핑 리뷰 감성 분류하기 예제입니다. 빈칸에 # 코드 입력란에 적당한 코드를 작성하시기를 바랍니다. 각 빈칸당 10점입니다. Colab에 Mecab 설치1234# Colab에 Mecab 설치!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git%cd Mecab-ko-for-Google-Colab!bash install_mecab-ko_on_colab190912.sh Cloning into 'Mecab-ko-for-Google-Colab'... remote: Enumerating objects: 91, done.\u001b[K remote: Total 91 (delta 0), reused 0 (delta 0), pack-reused 91\u001b[K Unpacking objects: 100% (91/91), done. /content/Mecab-ko-for-Google-Colab Installing konlpy..... Collecting konlpy Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB) \u001b[K |████████████████████████████████| 19.4 MB 2.4 MB/s \u001b[?25hCollecting JPype1&gt;=0.7.0 Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB) \u001b[K |████████████████████████████████| 448 kB 23.5 MB/s \u001b[?25hRequirement already satisfied: lxml&gt;=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6) Collecting colorama Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB) Requirement already satisfied: tweepy&gt;=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0) Requirement already satisfied: numpy&gt;=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5) Collecting beautifulsoup4==4.6.0 Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB) \u001b[K |████████████████████████████████| 86 kB 2.4 MB/s \u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1&gt;=0.7.0-&gt;konlpy) (3.10.0.2) Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy&gt;=3.7.0-&gt;konlpy) (1.3.0) Requirement already satisfied: requests[socks]&gt;=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy&gt;=3.7.0-&gt;konlpy) (2.23.0) Requirement already satisfied: six&gt;=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy&gt;=3.7.0-&gt;konlpy) (1.15.0) Requirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;tweepy&gt;=3.7.0-&gt;konlpy) (3.1.1) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]&gt;=2.11.1-&gt;tweepy&gt;=3.7.0-&gt;konlpy) (1.24.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]&gt;=2.11.1-&gt;tweepy&gt;=3.7.0-&gt;konlpy) (3.0.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]&gt;=2.11.1-&gt;tweepy&gt;=3.7.0-&gt;konlpy) (2021.10.8) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]&gt;=2.11.1-&gt;tweepy&gt;=3.7.0-&gt;konlpy) (2.10) Requirement already satisfied: PySocks!=1.5.7,&gt;=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]&gt;=2.11.1-&gt;tweepy&gt;=3.7.0-&gt;konlpy) (1.7.1) Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy Attempting uninstall: beautifulsoup4 Found existing installation: beautifulsoup4 4.6.3 Uninstalling beautifulsoup4-4.6.3: Successfully uninstalled beautifulsoup4-4.6.3 Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 Done Installing mecab-0.996-ko-0.9.2.tar.gz..... Downloading mecab-0.996-ko-0.9.2.tar.gz....... from https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz --2021-12-15 08:19:45-- https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22c0:3470, 2406:da00:ff00::22e9:9f55, ... Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected. HTTP request sent, awaiting response... 302 Found Location: https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=Djk%2BX4VYfoZUGHzDRgTrcVVdFvE%3D&amp;Expires=1639557778&amp;AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&amp;versionId=null&amp;response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&amp;response-content-encoding=None [following] --2021-12-15 08:19:46-- https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=Djk%2BX4VYfoZUGHzDRgTrcVVdFvE%3D&amp;Expires=1639557778&amp;AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&amp;versionId=null&amp;response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&amp;response-content-encoding=None Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.216.113.163 Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.216.113.163|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 1414979 (1.3M) [application/x-tar] Saving to: ‘mecab-0.996-ko-0.9.2.tar.gz’ mecab-0.996-ko-0.9. 100%[===================&gt;] 1.35M 1.07MB/s in 1.3s 2021-12-15 08:19:48 (1.07 MB/s) - ‘mecab-0.996-ko-0.9.2.tar.gz’ saved [1414979/1414979] Done Unpacking mecab-0.996-ko-0.9.2.tar.gz....... Done Change Directory to mecab-0.996-ko-0.9.2....... installing mecab-0.996-ko-0.9.2.tar.gz........ configure make make check make install ldconfig Done Change Directory to /content Downloading mecab-ko-dic-2.1.1-20180720.tar.gz....... from https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz --2021-12-15 08:21:19-- https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::6b17:d1f5, 2406:da00:ff00::22cd:e0db, ... Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected. HTTP request sent, awaiting response... 302 Found Location: https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=ZNAR2x6%2FNWxJ4p%2BOkG%2BjdG77Dqk%3D&amp;Expires=1639558279&amp;AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&amp;versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&amp;response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&amp;response-content-encoding=None [following] --2021-12-15 08:21:19-- https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=ZNAR2x6%2FNWxJ4p%2BOkG%2BjdG77Dqk%3D&amp;Expires=1639558279&amp;AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&amp;versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&amp;response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&amp;response-content-encoding=None Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 54.231.82.195 Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|54.231.82.195|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 49775061 (47M) [application/x-tar] Saving to: ‘mecab-ko-dic-2.1.1-20180720.tar.gz’ mecab-ko-dic-2.1.1- 100%[===================&gt;] 47.47M 13.0MB/s in 4.5s 2021-12-15 08:21:25 (10.5 MB/s) - ‘mecab-ko-dic-2.1.1-20180720.tar.gz’ saved [49775061/49775061] Done Unpacking mecab-ko-dic-2.1.1-20180720.tar.gz....... Done Change Directory to mecab-ko-dic-2.1.1-20180720 Done installing........ configure make make install apt-get update apt-get upgrade apt install curl apt install git bash &lt;(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh) Done Successfully Installed Now you can use Mecab from konlpy.tag import Mecab mecab = Mecab() 사용자 사전 추가 방법 : https://bit.ly/3k0ZH53 NameError: name 'Tagger' is not defined 오류 발생 시 런타임을 재실행 해주세요 블로그에 해결 방법을 남겨주신 tana님 감사합니다. 네이버 쇼핑 리뷰 데이터에 대한 이해와 전처리 12345678910import reimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport urllib.requestfrom collections import Counterfrom konlpy.tag import Mecabfrom sklearn.model_selection import train_test_splitfrom tensorflow.keras.preprocessing.text import Tokenizerfrom tensorflow.keras.preprocessing.sequence import pad_sequences 데이터 불러오기1urllib.request.urlretrieve(&quot;https://raw.githubusercontent.com/bab2min/corpus/master/sentiment/naver_shopping.txt&quot;, filename=&quot;ratings_total.txt&quot;) ('ratings_total.txt', &lt;http.client.HTTPMessage at 0x7f7d3557f750&gt;) 해당 데이터에는 열 제목이 별도로 없음. 그래서 임의로 두 개의 열제목인 “ratings”와 “reviews” 추가 123# (1) 데이터 불러오고, 전체 리뷰 개수 출력 # 200,000totalDt = pd.read_table('ratings_total.txt', names=['ratings', 'reviews'])print('전체 리뷰 개수 :',len(totalDt)) # 전체 리뷰 개수 출력 전체 리뷰 개수 : 200000 1totalDt[:5] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ratings reviews 0 5 배공빠르고 굿 1 2 택배가 엉망이네용 저희집 밑에층에 말도없이 놔두고가고 2 5 아주좋아요 바지 정말 좋아서2개 더 구매했어요 이가격에 대박입니다. 바느질이 조금 ... 3 2 선물용으로 빨리 받아서 전달했어야 하는 상품이었는데 머그컵만 와서 당황했습니다. 전... 4 5 민트색상 예뻐요. 옆 손잡이는 거는 용도로도 사용되네요 ㅎㅎ 훈련 데이터와 테스트 데이터 분리하기 12totalDt['label'] = np.select([totalDt.ratings &gt; 3], [1], default=0)totalDt[:5] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ratings reviews label 0 5 배공빠르고 굿 1 1 2 택배가 엉망이네용 저희집 밑에층에 말도없이 놔두고가고 0 2 5 아주좋아요 바지 정말 좋아서2개 더 구매했어요 이가격에 대박입니다. 바느질이 조금 ... 1 3 2 선물용으로 빨리 받아서 전달했어야 하는 상품이었는데 머그컵만 와서 당황했습니다. 전... 0 4 5 민트색상 예뻐요. 옆 손잡이는 거는 용도로도 사용되네요 ㅎㅎ 1 각 열에 대해서 중복을 제외한 샘플의 수 카운트 1totalDt['ratings'].nunique(), totalDt['reviews'].nunique(), totalDt['label'].nunique() (4, 199908, 2) ratings열의 경우 1, 2, 4, 5라는 네 가지 값을 가지고 있습니다. reviews열에서 중복을 제외한 경우 199,908개입니다. 현재 20만개의 리뷰가 존재하므로 이는 현재 갖고 있는 데이터에 중복인 샘플들이 있다는 의미입니다. 중복인 샘플들을 제거해줍니다. 123# (2) review열에서 중복 데이터 제거 drop_duplicates() 함수 활용totalDt.drop_duplicates(subset=['reviews'], inplace=True)print('총 샘플의 수 :',len(totalDt)) 총 샘플의 수 : 199908 NULL 값 유무 확인 1print(totalDt.isnull().values.any()) False 훈련 데이터와 테스트 데이터를 3:1 비율로 분리 123train_data, test_data = train_test_split(totalDt, test_size = 0.25, random_state = 42)print('훈련용 리뷰의 개수 :', len(train_data))print('테스트용 리뷰의 개수 :', len(test_data)) 훈련용 리뷰의 개수 : 149931 테스트용 리뷰의 개수 : 49977 레이블의 분포 확인12345678910111213# (3) label 1, 0 막대그래프 그리기import matplotlib.pyplot as pltimport numpy as npfig, ax = plt.subplots(1,1,figsize=(7,5))width = 0.15plot_Dt= train_data['label'].value_counts().plot(kind = 'bar', color='orange', edgecolor='black').legend()plt.title('train_data',fontsize=20) ## 타이틀 출력plt.ylabel('Count',fontsize=10) ## y축 라벨 출력plt.show() 1print(train_data.groupby('label').size().reset_index(name = 'count')) label count 0 0 74918 1 1 75013 두 레이블 모두 약 7만 5천개로 50:50 비율을 가짐 데이터 정제하기 정규 표현식을 사용하여 한글을 제외하고 모두 제거해줍니다. 12345# 한글과 공백을 제외하고 모두 제거# (4) 한글 및 공백 제외한 모든 글자 제거train_data['reviews'] = train_data['reviews'].str.replace(&quot;[^ㄱ-ㅎㅏ-ㅣ가-힣 ]&quot;,&quot;&quot;)train_data['reviews'].replace('', np.nan, inplace=True)print(train_data.isnull().sum()) ratings 0 reviews 0 label 0 dtype: int64 테스트 데이터에 대해서도 같은 과정을 거칩니다. 12345678910# (5) 데스트 데이터에 적용하기# 코드 1 중복 제거# 코드 2 정규 표현식 수행# 코드 3 공백은 Null 값으로 변경# 코드 4 Null 값 제거test_data.drop_duplicates(subset = ['reviews'], inplace=True) # 중복 제거test_data['reviews'] = test_data['reviews'].str.replace(&quot;[^ㄱ-ㅎㅏ-ㅣ가-힣 ]&quot;,&quot;&quot;) # 정규 표현식 수행test_data['reviews'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경test_data = test_data.dropna(how='any') # Null 값 제거print('전처리 후 테스트용 샘플의 개수 :',len(test_data)) 전처리 후 테스트용 샘플의 개수 : 49977 토큰화 형태소 분석기 Mecab을 사용하여 토큰화 작업을 수행한다. 123# (6) Mecab 클래스 호출하기mecab = Mecab()print(mecab.morphs('와 이런 것도 상품이라고 차라리 내가 만드는 게 나을 뻔')) ['와', '이런', '것', '도', '상품', '이', '라고', '차라리', '내', '가', '만드', '는', '게', '나을', '뻔'] 불용어를 지정하여 필요없는 토큰들을 제거하도록 한다. 12# (7) 불용어 만들기stopwords = ['도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를', '인', '듯', '과', '와', '네', '들', '듯', '지', '임', '게'] 훈련 데이터와 테스트 데이터에 대해서 동일한 과정을 거친다. 12train_data['tokenized'] = train_data['reviews'].apply(mecab.morphs)train_data['tokenized'] = train_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords]) 12test_data['tokenized'] = test_data['reviews'].apply(mecab.morphs)test_data['tokenized'] = test_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords]) 단어와 길이 분포 확인하기긍정 리뷰에는 주로 어떤 단어들이 많이 등장하고, 부정 리뷰에는 주로 어떤 단어들이 등장하는지 두 가지 경우에 대해서 각 단어의 빈도수를 계산해보겠습니다. 각 레이블에 따라서 별도로 단어들의 리스트를 저장해줍니다. 1234negative_W = np.hstack(train_data[train_data.label == 0]['tokenized'].values)positive_W = np.hstack(train_data[train_data.label == 1]['tokenized'].values)negative_Wpositive_W array(['적당', '만족', '합니다', ..., '잘', '삿', '어요'], dtype='&lt;U25') Counter()를 사용하여 각 단어에 대한 빈도수를 카운트한다. 우선 부정 리뷰에 대해서 빈도수가 높은 상위 20개 단어 출력 12negative_word_count = Counter(negative_W)print(negative_word_count.most_common(20)) [('네요', 31799), ('는데', 20295), ('안', 19718), ('어요', 14849), ('있', 13200), ('너무', 13058), ('했', 11783), ('좋', 9812), ('배송', 9677), ('같', 8997), ('구매', 8876), ('어', 8869), ('거', 8854), ('없', 8670), ('아요', 8642), ('습니다', 8436), ('그냥', 8355), ('되', 8345), ('잘', 8029), ('않', 7984)] ‘네요’, ‘는데’, ‘안’, ‘않’, ‘너무’, ‘없’ 등과 같은 단어들이 부정 리뷰에서 주로 등장합니다. 긍정 리뷰에 대해서도 동일하게 출력해봅시다. 12positive_word_count = Counter(positive_W)print(positive_word_count.most_common(20)) [('좋', 39488), ('아요', 21184), ('네요', 19895), ('어요', 18686), ('잘', 18602), ('구매', 16171), ('습니다', 13320), ('있', 12391), ('배송', 12275), ('는데', 11670), ('했', 9818), ('합니다', 9801), ('먹', 9635), ('재', 9273), ('너무', 8397), ('같', 7868), ('만족', 7261), ('거', 6482), ('어', 6294), ('쓰', 6292)] ‘좋’, ‘아요’, ‘네요’, ‘잘’, ‘너무’, ‘만족’ 등과 같은 단어들이 주로 많이 등장합니다. 두 가지 경우에 대해서 각각 길이 분포를 확인해봅시다. 1234567891011121314151617181920# (8) 긍정 리뷰와 부정 리뷰 히스토그램 작성하기fig,(ax1,ax2) = plt.subplots(1,2,figsize=(9,5))text_len = train_data[train_data['label']==1]['tokenized'].map(lambda x: len(x))ax1.hist(text_len, color='pink', edgecolor='black')ax1.set_title('Positive Reviews')ax1.set_xlabel('length of samples')ax1.set_ylabel('number of samples')print('긍정 리뷰의 평균 길이 :', np.mean(text_len))text_len = train_data[train_data['label']==0]['tokenized'].map(lambda x: len(x))ax2.hist(text_len, color='skyblue', edgecolor='black')ax2.set_title('부정 리뷰')ax2.set_title('Negative Reviews')fig.suptitle('Words in texts')ax2.set_xlabel('length of samples')ax2.set_ylabel('number of samples')print('부정 리뷰의 평균 길이 :', np.mean(text_len))plt.show() 긍정 리뷰의 평균 길이 : 13.5877381253916 부정 리뷰의 평균 길이 : 17.02948557089084 긍정 리뷰보다는 부정 리뷰가 좀 더 길게 작성된 경향이 있는 것 같다. 1234X_train = train_data['tokenized'].valuesy_train = train_data['label'].valuesX_test= test_data['tokenized'].valuesy_test = test_data['label'].values 정수 인코딩 이제 기계가 텍스트를 숫자로 처리할 수 있도록 훈련 데이터와 테스트 데이터에 정수 인코딩을 수행해야 합니다. 우선, 훈련 데이터에 대해서 단어 집합(vocaburary)을 만들어봅시다. 123# (9) 정수 인코딩 클래스 호출 및 X_train 데이터에 적합하기tokenizer = Tokenizer()tokenizer.fit_on_texts(X_train) 단어 집합이 생성되는 동시에 각 단어에 고유한 정수가 부여되었습니다. 이는 tokenizer.word_index를 출력하여 확인 가능합니다. 등장 횟수가 1회인 단어들은 자연어 처리에서 배제하고자 합니다. 이 단어들이 이 데이터에서 얼만큼의 비중을 차지하는지 확인해봅시다. 12345678910111213141516171819threshold = 2total_cnt = len(tokenizer.word_index) # 단어의 수rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.for key, value in tokenizer.word_counts.items(): total_freq = total_freq + value # 단어의 등장 빈도수가 threshold보다 작으면 if(value &lt; threshold): rare_cnt = rare_cnt + 1 rare_freq = rare_freq + valueprint('단어 집합(vocabulary)의 크기 :',total_cnt)print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))print(&quot;단어 집합에서 희귀 단어의 비율:&quot;, (rare_cnt / total_cnt)*100)print(&quot;전체 등장 빈도에서 희귀 단어 등장 빈도 비율:&quot;, (rare_freq / total_freq)*100) 단어 집합(vocabulary)의 크기 : 39998 등장 빈도가 1번 이하인 희귀 단어의 수: 18213 단어 집합에서 희귀 단어의 비율: 45.53477673883694 전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 0.7935698749320282 단어가 약 40,000개가 존재합니다. 등장 빈도가 threshold 값인 2회 미만. 즉, 1회인 단어들은 단어 집합에서 약 45%를 차지합니다. 하지만, 실제로 훈련 데이터에서 등장 빈도로 차지하는 비중은 매우 적은 수치인 약 0.8%밖에 되지 않습니다. 아무래도 등장 빈도가 1회인 단어들은 자연어 처리에서 별로 중요하지 않을 듯 합니다. 그래서 이 단어들은 정수 인코딩 과정에서 배제시키겠습니다. 등장 빈도수가 1인 단어들의 수를 제외한 단어의 개수를 단어 집합의 최대 크기로 제한하겠습니다. 1234# 전체 단어 개수 중 빈도수 2이하인 단어 개수는 제거.# 0번 패딩 토큰과 1번 OOV 토큰을 고려하여 +2vocab_size = total_cnt - rare_cnt + 2print('단어 집합의 크기 :',vocab_size) 단어 집합의 크기 : 21787 이제 단어 집합의 크기는 21,787개입니다. 이를 토크나이저의 인자로 넘겨주면, 토크나이저는 텍스트 시퀀스를 숫자 시퀀스로 변환합니다. 이러한 정수 인코딩 과정에서 이보다 큰 숫자가 부여된 단어들은 OOV로 변환하겠습니다. 123456789# (10) 토크나이저 클래스 호출 및 OOV 변환 코드 작성# 코드 1# 코드 2tokenizer = Tokenizer(vocab_size, oov_token = 'OOV') tokenizer.fit_on_texts(X_train)X_train = tokenizer.texts_to_sequences(X_train)X_test = tokenizer.texts_to_sequences(X_test) 정수 인코딩이 진행되었는지 확인하고자 X_train과 X_test에 대해서 상위 3개의 샘플만 출력합니다. 1print(X_train[:3]) [[67, 2060, 299, 14259, 263, 73, 6, 236, 168, 137, 805, 2951, 625, 2, 77, 62, 207, 40, 1343, 155, 3, 6], [482, 409, 52, 8530, 2561, 2517, 339, 2918, 250, 2357, 38, 473, 2], [46, 24, 825, 105, 35, 2372, 160, 7, 10, 8061, 4, 1319, 29, 140, 322, 41, 59, 160, 140, 7, 1916, 2, 113, 162, 1379, 323, 119, 136]] 1print(X_test[:3]) [[14, 704, 767, 116, 186, 252, 12], [339, 3904, 62, 3816, 1651], [11, 69, 2, 49, 164, 3, 27, 15, 6, 1, 513, 289, 17, 92, 110, 564, 59, 7, 2]] 패딩이제 서로 다른 길이의 샘플들의 길이를 동일하게 맞춰주는 패딩 작업을 진행해보겠습니다. 전체 데이터에서 가장 길이가 긴 리뷰와 전체 데이터의 길이 분포를 알아보겠습니다. 123456print('리뷰의 최대 길이 :',max(len(l) for l in X_train))print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))plt.hist([len(s) for s in X_train], bins=35, label='bins=35', color=&quot;skyblue&quot;)plt.xlabel('length of samples')plt.ylabel('number of samples')plt.show() 리뷰의 최대 길이 : 85 리뷰의 평균 길이 : 15.307521459871541 리뷰의 최대 길이는 85, 평균 길이는 약 15입니다. 그리고 그래프로 봤을 때, 전체적으로는 60이하의 길이를 가지는 것으로 보입니다. 12345678def below_threshold_len(max_len, nested_list): count = 0 for sentence in nested_list: if(len(sentence) &lt;= max_len): count = count + 1 print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(nested_list))*100)) 최대 길이가 85이므로 만약 80으로 패딩할 경우, 몇 개의 샘플들을 온전히 보전할 수 있는지 확인해봅시다. 12max_len = 80below_threshold_len(max_len, X_train) 전체 샘플 중 길이가 80 이하인 샘플의 비율: 99.99933302652553 훈련용 리뷰의 99.99%가 80이하의 길이를 가집니다. 훈련용 리뷰를 길이 80으로 패딩하겠습니다. 12X_train = pad_sequences(X_train, maxlen = max_len)X_test = pad_sequences(X_test, maxlen = max_len) GRU로 네이버 쇼핑 리뷰 감성 분류하기12345678910111213141516171819202122232425262728293031from tensorflow.keras.layers import Embedding, Dense, GRUfrom tensorflow.keras.models import Sequentialfrom tensorflow.keras.models import load_modelfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpointembedding_dim = 100hidden_units = 128model = Sequential()model.add(Embedding(vocab_size, embedding_dim))model.add(GRU(hidden_units))model.add(Dense(1, activation='sigmoid'))es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)def sentiment_predict(new_sentence): new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence) new_sentence = mecab.morphs(new_sentence) # 토큰화 new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거 encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩 pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩 score = float(model.predict(pad_new)) # 예측 if(score &gt; 0.5): print(&quot;{:.2f}% 확률로 긍정 리뷰입니다.&quot;.format(score * 100)) else: print(&quot;{:.2f}% 확률로 부정 리뷰입니다.&quot;.format((1 - score) * 100)) Epoch 1/15 1875/1875 [==============================] - ETA: 0s - loss: 0.2725 - acc: 0.8967 Epoch 00001: val_acc improved from -inf to 0.91916, saving model to best_model.h5 1875/1875 [==============================] - 54s 25ms/step - loss: 0.2725 - acc: 0.8967 - val_loss: 0.2301 - val_acc: 0.9192 Epoch 2/15 1875/1875 [==============================] - ETA: 0s - loss: 0.2158 - acc: 0.9213 Epoch 00002: val_acc improved from 0.91916 to 0.92240, saving model to best_model.h5 1875/1875 [==============================] - 43s 23ms/step - loss: 0.2158 - acc: 0.9213 - val_loss: 0.2137 - val_acc: 0.9224 Epoch 3/15 1875/1875 [==============================] - ETA: 0s - loss: 0.1985 - acc: 0.9289 Epoch 00003: val_acc improved from 0.92240 to 0.92637, saving model to best_model.h5 1875/1875 [==============================] - 44s 24ms/step - loss: 0.1985 - acc: 0.9289 - val_loss: 0.2060 - val_acc: 0.9264 Epoch 4/15 1873/1875 [============================&gt;.] - ETA: 0s - loss: 0.1878 - acc: 0.9332 Epoch 00004: val_acc did not improve from 0.92637 1875/1875 [==============================] - 43s 23ms/step - loss: 0.1878 - acc: 0.9332 - val_loss: 0.2031 - val_acc: 0.9260 Epoch 5/15 1874/1875 [============================&gt;.] - ETA: 0s - loss: 0.1783 - acc: 0.9369 Epoch 00005: val_acc improved from 0.92637 to 0.92670, saving model to best_model.h5 1875/1875 [==============================] - 46s 24ms/step - loss: 0.1783 - acc: 0.9369 - val_loss: 0.2030 - val_acc: 0.9267 Epoch 6/15 1873/1875 [============================&gt;.] - ETA: 0s - loss: 0.1698 - acc: 0.9405 Epoch 00006: val_acc improved from 0.92670 to 0.92764, saving model to best_model.h5 1875/1875 [==============================] - 44s 24ms/step - loss: 0.1697 - acc: 0.9405 - val_loss: 0.2055 - val_acc: 0.9276 Epoch 7/15 1873/1875 [============================&gt;.] - ETA: 0s - loss: 0.1611 - acc: 0.9436 Epoch 00007: val_acc did not improve from 0.92764 1875/1875 [==============================] - 44s 24ms/step - loss: 0.1610 - acc: 0.9437 - val_loss: 0.2098 - val_acc: 0.9244 Epoch 8/15 1875/1875 [==============================] - ETA: 0s - loss: 0.1526 - acc: 0.9473 Epoch 00008: val_acc did not improve from 0.92764 1875/1875 [==============================] - 44s 23ms/step - loss: 0.1526 - acc: 0.9473 - val_loss: 0.2269 - val_acc: 0.9189 Epoch 9/15 1875/1875 [==============================] - ETA: 0s - loss: 0.1435 - acc: 0.9507 Epoch 00009: val_acc did not improve from 0.92764 1875/1875 [==============================] - 44s 24ms/step - loss: 0.1435 - acc: 0.9507 - val_loss: 0.2258 - val_acc: 0.9204 Epoch 00009: early stopping 1sentiment_predict('이 상품 진짜 싫어요... 교환해주세요') 99.03% 확률로 부정 리뷰입니다. 1sentiment_predict('이 상품 진짜 좋아여... 강추합니다. ') 99.51% 확률로 긍정 리뷰입니다.","link":"/2021/12/15/python/MachineLearning_Test_/"},{"title":"Regression in python(01)","text":"Chepter 5 _파이썬 머신러닝 완벽 가이드 ref. &amp; copyright(c) Book 회귀 Regression: 여러개의 독립변수와 한개의 종속변수 간의 상관관계를 모델링 하는 기법 Regression conefficients : 독립변수의 값에 영향을 미치는 회기 계수로 선형 회기 식의 기울기에 해당 러닝머신의 관점 독립변수 : 피처 종속변수 : 결정값 = > 주어진 피처와 결정값 데이터 기반에서 학습을 통해 최적의 **회귀계수** 를 찾아 내는 것이 목표 ✌지도학습 2가지 유형 CLASSIFICATION + category, 이산 값 일때 REGRESSION + 숫자, 연속 값 일때 ⚡ 회귀의 4가지 유형 독립변수 개수 - 단일 회귀 - 다중 회귀 회귀 계수의 결합 - 선형 회귀 : 실제 값과 예측 값의 차이 (오류의 제곱값)를 최소화 하는 직선형 회귀선을 최적화 하는 방식 Regularization(규제방법) : 일반적 선형 회귀의 과적합 문제를 해결 하기 위해 회귀 계수를 조정 하는것 (패널티 값 적용) - 비선형 회귀 일반선형회귀 : 예측값과 실제값의 RSS를 최소화 할 수 있도록 회귀계수 최적화 (Regularization X) Ridge(릿지) : 선형 회귀 + L2 Regularization L2 : 상대적으로 큰 회귀 계수 값의 예측 영향도를 감소 시키기 위해 회귀 계수값을 더 작게 만듦. Lasso(라쏘) : ElasticNet(엘라스틱넷) : Rogistic Regression(로지스틱 회귀) : Ref. scikit-learn y = 4x + 6 + error 시뮬레이션 데이터 값 생성12345678910import numpy as npimport matplotlib.pyplot as plt%matplotlib inlinenp.random.seed(123) # 실험 재현성X = 2 * np.random.rand(100, 1) # 100개의 랜덤값 만들기y = 4 * X + 6 + np.random.rand(100, 1)plt.scatter(X, y) 1X.shape, y.shape ((100, 1), (100, 1)) 경사하강법으로 최적의 기울기 찾기123456789101112131415161718192021# w1과, w0를 업데이트할 w1_update, w0_update 값 반환def get_weight_updates(w1, w0, X, y, learning_rate=0.01): N = len(y) # w1_update, w0_update 초기화 w1_update = np.zeros_like(w1) w0_update = np.zeros_like(w0) # 예측 배열 계산하고, 예측값과 실젯 값의 차이 계산 y_pred = np.dot(X, w1.T) + w0 diff = y - y_pred # 실제갓, 예측값 == 오차 # w0_update를 dot 행렬 연산으로 구하기 위해 모두 1 값을 가진 행렬 생성 w0_factors = np.ones((N, 1)) # w1과 w0을 업데이트할 w1_update, w0_update 계산 w1_update = -(2/N) * learning_rate * (np.dot(X.T, diff)) w0_update = -(2/N) * learning_rate * (np.dot(w0_factors.T, diff)) return w1_update, w0_update 12345678910111213w0 = np.zeros((1, 1))w1 = np.zeros((1, 1))y_pred = np.dot(X, w1.T) + w0diff = y-y_predprint(diff.shape)w0_factors = np.ones((100, 1))w1_update = -(2/100) * 0.01 * (np.dot(X.T, diff))w0_update = -(2/100) * 0.01 * (np.dot(w0_factors.T, diff))print(w1_update.shape, w0_update.shape)print(w1, w0) (100, 1) (1, 1) (1, 1) [[0.]] [[0.]] 123456789101112131415# 입력 인자 반복문 코드 def gradient_descent_steps(X, y, iters = 100000): # w0와 w1을 모두 0으로 초기화 w0 = np.zeros((1, 1)) w1 = np.zeros((1, 1)) # iters 만큼 반복 수행 # get_weight_updates for ind in range(iters): w1_update, w0_update = get_weight_updates(w1, w0, X, y, learning_rate=0.01) w1 = w1 - w1_update w0 = w0 - w0_update return w1, w0 예측 오차 비용 계산하는 함수 생성 및 경사 하강법 수행 1234567891011def get_cost(y, y_pred): N = len(y) cost = np.sum(np.square(y - y_pred)) / N return cost w1, w0 = gradient_descent_steps(X, y, iters = 100000)print(&quot;w1:{0:.4f}, w0:{1:.4f}&quot;.format(w1[0, 0], w0[0, 0]))y_pred = w1[0,0] * X + w0print(&quot;Total Cost:{0:.4f}&quot;.format(get_cost(y, y_pred))) w1:3.9462, w0:6.5590 Total Cost:0.0803 12plt.scatter(X, y)plt.plot(X, y_pred, color = &quot;r&quot;) 12345import pandas as pdbostonDF = pd.read_csv(&quot;https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv&quot;)bostonDF.head() EDA 종속변수가 기준, y값, medv 1234567891011import matplotlib.pyplot as pltimport seaborn as snsfig, ax = plt.subplots(figsize = (16, 8), ncols = 4, nrows = 2)lm_features = [&quot;rm&quot;, &quot;zn&quot;, &quot;indus&quot;, &quot;nox&quot;, &quot;age&quot;, &quot;ptratio&quot;, &quot;lstat&quot;, &quot;rad&quot;]for i, feature in enumerate(lm_features): row = int(i/4) col = i%4 print(&quot;row is {}, col is {}&quot;.format(row, col)) sns.regplot(x = feature, y = &quot;medv&quot;, data = bostonDF, ax = ax[row][col]) 두 연속형 변수를 활용한 산점도나 회귀식 가능. 박스플롯 (x: 명목형, y: medv) rm 3.4chas 3.0rad 0.4zn 0.1b 0.0tax -0.0age 0.0indus 0.0crim -0.1lstat -0.6ptratio -0.9dis -1.7nox -19.8 1234567from sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LinearRegression # modely_target = bostonDF[&quot;medv&quot;] # 종속변수, YX_data = bostonDF.drop(['medv', 'rad', 'zn', 'b', 'tax', 'age', 'indus', 'crim', 'lstat'], axis = 1, inplace = False) # 독립변수y_target.shape, X_data.shape requirejs.config({ paths: { base: '/static/base', plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext', }, }); ((506,), (506, 5)) 데이터셋 분리 예측, 시뮬레이션, 가상의 데이터를 가지고 예측 &amp; 시뮬레이션 예측한 결괏값 vs 실젯값 비교 1234# 임의 샘플링X_train, X_test, y_train, y_test = train_test_split(X_data, y_target, test_size = 0.3, random_state=156)X_train.shape, X_test.shape, y_train.shape, y_test.shape requirejs.config({ paths: { base: '/static/base', plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext', }, }); ((354, 5), (152, 5), (354,), (152,)) ML 모형 만들기123456lr = LinearRegression()lr.fit(X_train, y_train)y_preds = lr.predict(X_test)y_preds requirejs.config({ paths: { base: '/static/base', plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext', }, }); array([26.78074859, 16.40377991, 34.38443472, 19.13328473, 32.89690238, 19.25298249, 28.32071818, 22.76654888, 9.87108567, 14.66339227, 21.55844556, 17.27788854, 28.55574467, 38.50512646, 23.60848806, 24.03347202, 23.82317119, 15.9119451 , 28.65132167, 20.98388455, 20.29188703, 18.37003455, 18.58675839, 14.89143225, 35.24799305, 7.70600921, 19.39133905, 15.97963635, 16.90296718, 15.484303 , 29.67753869, 17.58268684, 16.91992352, 22.47407959, 16.57706526, 18.5381101 , 13.34337954, 24.11893098, 15.48185399, 24.3234222 , 36.24776797, 19.60882283, 20.95016211, 6.85667164, 20.32077896, 23.05614583, 24.65371876, 35.25609168, 22.32959594, 25.96437918, 27.29101785, 43.32992941, 41.76994078, 19.34288261, 24.8690423 , 25.99270875, 20.76285715, 33.13792328, 25.00439224, 16.82906893, 22.80895172, 23.72489982, 24.53360315, 11.82722067, 17.55728132, 37.43371362, 33.37256916, 25.65966256, 20.90725715, 21.09529467, 15.22097444, 30.6234335 , 37.42143489, 26.22092177, 16.71532104, 32.62735407, 23.41004013, 23.86575538, 18.75430877, 15.9914079 , 30.87778491, 16.04423898, 19.01496945, 20.04269634, 28.30832805, 15.1948795 , 30.47430322, 33.93480059, 23.87721263, 29.7167635 , 29.85142798, 19.10737457, 28.49523963, 27.69846662, 25.49534489, 24.59255802, 12.34870184, 26.65951587, 31.26197918, 17.86101862, 27.3059424 , 18.18058484, 15.67184217, 13.17304165, 17.91281425, 23.48894551, 24.53921273, 28.14530028, 16.05340908, 24.22120622, 21.94517346, 26.62930956, 11.39298015, 18.53099857, 22.75407122, 33.6679728 , 23.35342973, 20.85267956, 19.69347759, 28.12264641, 28.56541499, 17.91759633, 27.83520695, 33.8011824 , 21.75436813, 26.6360736 , 14.70682076, 19.99114889, 21.81029849, 31.72247354, 21.33041025, 23.52438417, 35.55842163, 20.54294729, 38.34696416, 19.25750865, 17.07595035, 18.31764392, 17.66658651, 23.12171447, 19.58446231, 19.90774119, 14.84809066, 19.50652744, 38.83812958, 15.26095952, 28.56874885, 17.62298514, 22.46794555, 23.28435884, 18.8439135 , 31.16286012]) 모형 평가1234from sklearn.metrics import mean_squared_error, r2_scoremse = mean_squared_error(y_test, y_preds)print(&quot;MSE: {0:.3f}&quot;.format(mse)) requirejs.config({ paths: { base: '/static/base', plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext', }, }); MSE: 21.369 y = 상수값 + rm 기울기 x rm의 값 + 1234567import numpy as np print(&quot;절편 값:&quot;, lr.intercept_) # 절편 값print(&quot;회귀 계수값&quot;, np.round(lr.coef_, 1))coeff_df = pd.Series(data=np.round(lr.coef_, 1), index = X_data.columns)coeff_df.sort_values(ascending=False) requirejs.config({ paths: { base: '/static/base', plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext', }, }); 절편 값: 26.830373506191982 회귀 계수값 [ 4.3 -33.1 6.5 -1.1 -1.2] rm 6.5 chas 4.3 dis -1.1 ptratio -1.2 nox -33.1 dtype: float64 아직 배우지 않았지만, 유용한 기능1!pip install pycaret Collecting pycaret Downloading pycaret-2.3.5-py3-none-any.whl (288 kB) \u001b[K |████████████████████████████████| 288 kB 32.5 MB/s \u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pycaret) (1.1.0) Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (from pycaret) (0.15.3) Collecting pandas-profiling&gt;=2.8.0 Downloading pandas_profiling-3.1.0-py2.py3-none-any.whl (261 kB) \u001b[K |████████████████████████████████| 261 kB 53.7 MB/s \u001b[?25hCollecting pyLDAvis Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB) \u001b[K |████████████████████████████████| 1.7 MB 42.3 MB/s \u001b[?25h Installing build dependencies ... \u001b[?25l\u001b[?25hdone Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone Installing backend dependencies ... \u001b[?25l\u001b[?25hdone Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone Requirement already satisfied: spacy&lt;2.4.0 in /usr/local/lib/python3.7/dist-packages (from pycaret) (2.2.4) Collecting scikit-learn==0.23.2 Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB) \u001b[K |████████████████████████████████| 6.8 MB 58.0 MB/s \u001b[?25hRequirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from pycaret) (7.6.5) Requirement already satisfied: cufflinks&gt;=0.17.0 in /usr/local/lib/python3.7/dist-packages (from pycaret) (0.17.3) Collecting scikit-plot Downloading scikit_plot-0.3.7-py3-none-any.whl (33 kB) Requirement already satisfied: yellowbrick&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from pycaret) (1.3.post1) Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pycaret) (3.2.5) Collecting umap-learn Downloading umap-learn-0.5.2.tar.gz (86 kB) \u001b[K |████████████████████████████████| 86 kB 6.0 MB/s \u001b[?25hCollecting Boruta Downloading Boruta-0.3-py3-none-any.whl (56 kB) \u001b[K |████████████████████████████████| 56 kB 4.7 MB/s \u001b[?25hRequirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (from pycaret) (1.5.0) Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from pycaret) (0.11.2) Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from pycaret) (5.5.0) Requirement already satisfied: gensim&lt;4.0.0 in /usr/local/lib/python3.7/dist-packages (from pycaret) (3.6.0) Collecting lightgbm&gt;=2.3.1 Downloading lightgbm-3.3.1-py3-none-manylinux1_x86_64.whl (2.0 MB) \u001b[K |████████████████████████████████| 2.0 MB 47.4 MB/s \u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pycaret) (1.1.5) Collecting mlxtend&gt;=0.17.0 Downloading mlxtend-0.19.0-py2.py3-none-any.whl (1.3 MB) \u001b[K |████████████████████████████████| 1.3 MB 60.5 MB/s \u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from pycaret) (3.2.2) Requirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (from pycaret) (1.19.5) Collecting pyod Downloading pyod-0.9.5.tar.gz (113 kB) \u001b[K |████████████████████████████████| 113 kB 58.7 MB/s \u001b[?25hRequirement already satisfied: plotly&gt;=4.4.1 in /usr/local/lib/python3.7/dist-packages (from pycaret) (4.4.1) Collecting mlflow Downloading mlflow-1.22.0-py3-none-any.whl (15.5 MB) \u001b[K |████████████████████████████████| 15.5 MB 50.3 MB/s \u001b[?25hRequirement already satisfied: scipy&lt;=1.5.4 in /usr/local/lib/python3.7/dist-packages (from pycaret) (1.4.1) Collecting imbalanced-learn==0.7.0 Downloading imbalanced_learn-0.7.0-py3-none-any.whl (167 kB) \u001b[K |████████████████████████████████| 167 kB 62.1 MB/s \u001b[?25hCollecting kmodes&gt;=0.10.1 Downloading kmodes-0.11.1-py2.py3-none-any.whl (19 kB) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2-&gt;pycaret) (3.0.0) Requirement already satisfied: colorlover&gt;=0.2.1 in /usr/local/lib/python3.7/dist-packages (from cufflinks&gt;=0.17.0-&gt;pycaret) (0.3.0) Requirement already satisfied: setuptools&gt;=34.4.1 in /usr/local/lib/python3.7/dist-packages (from cufflinks&gt;=0.17.0-&gt;pycaret) (57.4.0) Requirement already satisfied: six&gt;=1.9.0 in /usr/local/lib/python3.7/dist-packages (from cufflinks&gt;=0.17.0-&gt;pycaret) (1.15.0) Requirement already satisfied: smart-open&gt;=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim&lt;4.0.0-&gt;pycaret) (5.2.1) Requirement already satisfied: traitlets&gt;=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (5.1.1) Requirement already satisfied: simplegeneric&gt;0.8 in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (0.8.1) Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (2.6.1) Requirement already satisfied: prompt-toolkit&lt;2.0.0,&gt;=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (1.0.18) Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (4.4.2) Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (4.8.0) Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (0.7.5) Requirement already satisfied: jupyterlab-widgets&gt;=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;pycaret) (1.0.2) Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;pycaret) (3.5.2) Requirement already satisfied: ipykernel&gt;=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;pycaret) (4.10.1) Requirement already satisfied: nbformat&gt;=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;pycaret) (5.1.3) Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;pycaret) (0.2.0) Requirement already satisfied: tornado&gt;=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets-&gt;pycaret) (5.1.1) Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets-&gt;pycaret) (5.3.5) Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm&gt;=2.3.1-&gt;pycaret) (0.37.0) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;pycaret) (2.8.2) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;pycaret) (0.11.0) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;pycaret) (1.3.2) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;pycaret) (3.0.6) Requirement already satisfied: jsonschema!=2.5.0,&gt;=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets-&gt;pycaret) (2.6.0) Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets-&gt;pycaret) (4.9.1) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas-&gt;pycaret) (2018.9) Requirement already satisfied: tqdm&gt;=4.48.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling&gt;=2.8.0-&gt;pycaret) (4.62.3) Collecting tangled-up-in-unicode==0.1.0 Downloading tangled_up_in_unicode-0.1.0-py3-none-any.whl (3.1 MB) \u001b[K |████████████████████████████████| 3.1 MB 47.3 MB/s \u001b[?25hRequirement already satisfied: markupsafe~=2.0.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling&gt;=2.8.0-&gt;pycaret) (2.0.1) Collecting pydantic&gt;=1.8.1 Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB) \u001b[K |████████████████████████████████| 10.1 MB 37.6 MB/s \u001b[?25hCollecting htmlmin&gt;=0.1.12 Downloading htmlmin-0.1.12.tar.gz (19 kB) Collecting multimethod&gt;=1.4 Downloading multimethod-1.6-py3-none-any.whl (9.4 kB) Collecting PyYAML&gt;=5.0.0 Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB) \u001b[K |████████████████████████████████| 596 kB 40.0 MB/s \u001b[?25hCollecting phik&gt;=0.11.1 Downloading phik-0.12.0-cp37-cp37m-manylinux2010_x86_64.whl (675 kB) \u001b[K |████████████████████████████████| 675 kB 62.8 MB/s \u001b[?25hRequirement already satisfied: jinja2&gt;=2.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling&gt;=2.8.0-&gt;pycaret) (2.11.3) Collecting visions[type_image_path]==0.7.4 Downloading visions-0.7.4-py3-none-any.whl (102 kB) \u001b[K |████████████████████████████████| 102 kB 12.8 MB/s \u001b[?25hCollecting joblib Downloading joblib-1.0.1-py3-none-any.whl (303 kB) \u001b[K |████████████████████████████████| 303 kB 71.5 MB/s \u001b[?25hCollecting requests&gt;=2.24.0 Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB) \u001b[K |████████████████████████████████| 62 kB 995 kB/s \u001b[?25hRequirement already satisfied: missingno&gt;=0.4.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling&gt;=2.8.0-&gt;pycaret) (0.5.0) Requirement already satisfied: attrs&gt;=19.3.0 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (21.2.0) Requirement already satisfied: networkx&gt;=2.4 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (2.6.3) Collecting imagehash Downloading ImageHash-4.2.1.tar.gz (812 kB) \u001b[K |████████████████████████████████| 812 kB 49.7 MB/s \u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (7.1.2) Collecting scipy&lt;=1.5.4 Downloading scipy-1.5.4-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB) \u001b[K |████████████████████████████████| 25.9 MB 1.6 MB/s \u001b[?25hRequirement already satisfied: retrying&gt;=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly&gt;=4.4.1-&gt;pycaret) (1.3.3) Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit&lt;2.0.0,&gt;=1.0.4-&gt;IPython-&gt;pycaret) (0.2.5) Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic&gt;=1.8.1-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (3.10.0.2) Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (2.10) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (2021.10.8) Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (2.0.8) Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (1.24.3) Requirement already satisfied: blis&lt;0.5.0,&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (0.4.1) Requirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (1.0.6) Requirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (3.0.6) Requirement already satisfied: plac&lt;1.2.0,&gt;=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (1.1.3) Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (7.4.0) Requirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (2.0.6) Requirement already satisfied: wasabi&lt;1.1.0,&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (0.8.2) Requirement already satisfied: catalogue&lt;1.1.0,&gt;=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (1.0.0) Requirement already satisfied: srsly&lt;1.1.0,&gt;=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (1.0.5) Requirement already satisfied: importlib-metadata&gt;=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&lt;2.4.0-&gt;pycaret) (4.8.2) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata&gt;=0.20-&gt;catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&lt;2.4.0-&gt;pycaret) (3.6.0) Requirement already satisfied: notebook&gt;=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0-&gt;ipywidgets-&gt;pycaret) (5.3.1) Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets-&gt;pycaret) (5.6.1) Requirement already satisfied: terminado&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets-&gt;pycaret) (0.12.1) Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets-&gt;pycaret) (1.8.0) Requirement already satisfied: pyzmq&gt;=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client-&gt;ipykernel&gt;=4.5.1-&gt;ipywidgets-&gt;pycaret) (22.3.0) Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado&gt;=0.8.1-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets-&gt;pycaret) (0.7.0) Requirement already satisfied: PyWavelets in /usr/local/lib/python3.7/dist-packages (from imagehash-&gt;visions[type_image_path]==0.7.4-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (1.2.0) Collecting docker&gt;=4.0.0 Downloading docker-5.0.3-py2.py3-none-any.whl (146 kB) \u001b[K |████████████████████████████████| 146 kB 70.7 MB/s \u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (0.3) Requirement already satisfied: protobuf&gt;=3.7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (3.17.3) Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (1.3.0) Collecting databricks-cli&gt;=0.8.7 Downloading databricks-cli-0.16.2.tar.gz (58 kB) \u001b[K |████████████████████████████████| 58 kB 5.9 MB/s \u001b[?25hRequirement already satisfied: click&gt;=7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (7.1.2) Collecting querystring-parser Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB) Collecting alembic&lt;=1.4.1 Downloading alembic-1.4.1.tar.gz (1.1 MB) \u001b[K |████████████████████████████████| 1.1 MB 59.1 MB/s \u001b[?25hCollecting prometheus-flask-exporter Downloading prometheus_flask_exporter-0.18.6-py3-none-any.whl (17 kB) Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (1.4.27) Requirement already satisfied: Flask in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (1.1.4) Collecting gitpython&gt;=2.1.0 Downloading GitPython-3.1.24-py3-none-any.whl (180 kB) \u001b[K |████████████████████████████████| 180 kB 58.3 MB/s \u001b[?25hCollecting gunicorn Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB) \u001b[K |████████████████████████████████| 79 kB 8.7 MB/s \u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (21.3) Requirement already satisfied: sqlparse&gt;=0.3.1 in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (0.4.2) Collecting Mako Downloading Mako-1.1.6-py2.py3-none-any.whl (75 kB) \u001b[K |████████████████████████████████| 75 kB 4.4 MB/s \u001b[?25hCollecting python-editor&gt;=0.3 Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB) Requirement already satisfied: tabulate&gt;=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli&gt;=0.8.7-&gt;mlflow-&gt;pycaret) (0.8.9) Collecting websocket-client&gt;=0.32.0 Downloading websocket_client-1.2.3-py3-none-any.whl (53 kB) \u001b[K |████████████████████████████████| 53 kB 2.1 MB/s \u001b[?25hCollecting gitdb&lt;5,&gt;=4.0.1 Downloading gitdb-4.0.9-py3-none-any.whl (63 kB) \u001b[K |████████████████████████████████| 63 kB 1.9 MB/s \u001b[?25hCollecting smmap&lt;6,&gt;=3.0.1 Downloading smmap-5.0.0-py3-none-any.whl (24 kB) Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy-&gt;mlflow-&gt;pycaret) (1.1.2) Requirement already satisfied: Werkzeug&lt;2.0,&gt;=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask-&gt;mlflow-&gt;pycaret) (1.0.1) Requirement already satisfied: itsdangerous&lt;2.0,&gt;=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask-&gt;mlflow-&gt;pycaret) (1.1.0) Requirement already satisfied: mistune&lt;2,&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets-&gt;pycaret) (0.8.4) Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets-&gt;pycaret) (4.1.0) Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets-&gt;pycaret) (0.5.0) Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets-&gt;pycaret) (0.7.1) Requirement already satisfied: pandocfilters&gt;=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets-&gt;pycaret) (1.5.0) Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach-&gt;nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets-&gt;pycaret) (0.5.1) Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from prometheus-flask-exporter-&gt;mlflow-&gt;pycaret) (0.12.0) Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis-&gt;pycaret) (0.16.0) Collecting pyLDAvis Downloading pyLDAvis-3.3.0.tar.gz (1.7 MB) \u001b[K |████████████████████████████████| 1.7 MB 37.5 MB/s \u001b[?25h Installing build dependencies ... \u001b[?25l\u001b[?25hdone Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone Installing backend dependencies ... \u001b[?25l\u001b[?25hdone Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone Downloading pyLDAvis-3.2.2.tar.gz (1.7 MB) \u001b[K |████████████████████████████████| 1.7 MB 45.7 MB/s \u001b[?25hRequirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis-&gt;pycaret) (2.7.3) Collecting funcy Downloading funcy-1.16-py2.py3-none-any.whl (32 kB) Requirement already satisfied: numba&gt;=0.35 in /usr/local/lib/python3.7/dist-packages (from pyod-&gt;pycaret) (0.51.2) Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from pyod-&gt;pycaret) (0.10.2) Requirement already satisfied: llvmlite&lt;0.35,&gt;=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba&gt;=0.35-&gt;pyod-&gt;pycaret) (0.34.0) Requirement already satisfied: patsy&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels-&gt;pyod-&gt;pycaret) (0.5.2) Collecting pynndescent&gt;=0.5 Downloading pynndescent-0.5.5.tar.gz (1.1 MB) \u001b[K |████████████████████████████████| 1.1 MB 49.9 MB/s \u001b[?25hBuilding wheels for collected packages: htmlmin, imagehash, alembic, databricks-cli, pyLDAvis, pyod, umap-learn, pynndescent Building wheel for htmlmin (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27098 sha256=d7dfcc5cb8473dd5eae3fcf51c538f92f876faa04e78c8b36d9c790b9fac7e10 Stored in directory: /root/.cache/pip/wheels/70/e1/52/5b14d250ba868768823940c3229e9950d201a26d0bd3ee8655 Building wheel for imagehash (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for imagehash: filename=ImageHash-4.2.1-py2.py3-none-any.whl size=295207 sha256=8b1e1a54f9880fb8de0530e8e168811d3264000c0375d179b04677d7db738f6f Stored in directory: /root/.cache/pip/wheels/4c/d5/59/5e3e297533ddb09407769762985d134135064c6831e29a914e Building wheel for alembic (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for alembic: filename=alembic-1.4.1-py2.py3-none-any.whl size=158172 sha256=3a382d7a8aa3f735be58614dc83527e0801ccb0bc893eb96cc388ee8f0a5dd91 Stored in directory: /root/.cache/pip/wheels/be/5d/0a/9e13f53f4f5dfb67cd8d245bb7cdffe12f135846f491a283e3 Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for databricks-cli: filename=databricks_cli-0.16.2-py3-none-any.whl size=106811 sha256=ada21177391b9688188e6f778b0ec6b6001615c2b2f13bef53090805b2f183bf Stored in directory: /root/.cache/pip/wheels/f4/5c/ed/e1ce20a53095f63b27b4964abbad03e59cf3472822addf7d29 Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for pyLDAvis: filename=pyLDAvis-3.2.2-py2.py3-none-any.whl size=135618 sha256=29ef50e1603fe00d18a256b833c7feddebc16ef3ac82f37f109f991b0f95b4b0 Stored in directory: /root/.cache/pip/wheels/f8/b1/9b/560ac1931796b7303f7b517b949d2d31a4fbc512aad3b9f284 Building wheel for pyod (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for pyod: filename=pyod-0.9.5-py3-none-any.whl size=132699 sha256=851491ca675bc8eb4d9ecfb52396f362de25c1443531f442a9528c0b9b3f7b21 Stored in directory: /root/.cache/pip/wheels/3d/bb/b7/62b60fb451b33b0df1ab8006697fba7a6a49709a629055cf77 Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for umap-learn: filename=umap_learn-0.5.2-py3-none-any.whl size=82709 sha256=f4bae757148b4cf4930e495a816ecb3f6fcc3a16d1014c85ce052bb2acccb378 Stored in directory: /root/.cache/pip/wheels/84/1b/c6/aaf68a748122632967cef4dffef68224eb16798b6793257d82 Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for pynndescent: filename=pynndescent-0.5.5-py3-none-any.whl size=52603 sha256=e19d78d031a739792e30a3bf2d93865296b6eb66226835f66a8287b1330882f1 Stored in directory: /root/.cache/pip/wheels/af/e9/33/04db1436df0757c42fda8ea6796d7a8586e23c85fac355f476 Successfully built htmlmin imagehash alembic databricks-cli pyLDAvis pyod umap-learn pynndescent Installing collected packages: tangled-up-in-unicode, smmap, scipy, multimethod, joblib, websocket-client, visions, scikit-learn, requests, python-editor, Mako, imagehash, gitdb, querystring-parser, PyYAML, pynndescent, pydantic, prometheus-flask-exporter, phik, htmlmin, gunicorn, gitpython, funcy, docker, databricks-cli, alembic, umap-learn, scikit-plot, pyod, pyLDAvis, pandas-profiling, mlxtend, mlflow, lightgbm, kmodes, imbalanced-learn, Boruta, pycaret Attempting uninstall: scipy Found existing installation: scipy 1.4.1 Uninstalling scipy-1.4.1: Successfully uninstalled scipy-1.4.1 Attempting uninstall: joblib Found existing installation: joblib 1.1.0 Uninstalling joblib-1.1.0: Successfully uninstalled joblib-1.1.0 Attempting uninstall: scikit-learn Found existing installation: scikit-learn 1.0.1 Uninstalling scikit-learn-1.0.1: Successfully uninstalled scikit-learn-1.0.1 Attempting uninstall: requests Found existing installation: requests 2.23.0 Uninstalling requests-2.23.0: Successfully uninstalled requests-2.23.0 Attempting uninstall: PyYAML Found existing installation: PyYAML 3.13 Uninstalling PyYAML-3.13: Successfully uninstalled PyYAML-3.13 Attempting uninstall: pandas-profiling Found existing installation: pandas-profiling 1.4.1 Uninstalling pandas-profiling-1.4.1: Successfully uninstalled pandas-profiling-1.4.1 Attempting uninstall: mlxtend Found existing installation: mlxtend 0.14.0 Uninstalling mlxtend-0.14.0: Successfully uninstalled mlxtend-0.14.0 Attempting uninstall: lightgbm Found existing installation: lightgbm 2.2.3 Uninstalling lightgbm-2.2.3: Successfully uninstalled lightgbm-2.2.3 Attempting uninstall: imbalanced-learn Found existing installation: imbalanced-learn 0.8.1 Uninstalling imbalanced-learn-0.8.1: Successfully uninstalled imbalanced-learn-0.8.1 \u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible. datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible. albumentations 0.1.12 requires imgaug&lt;0.2.7,&gt;=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m Successfully installed Boruta-0.3 Mako-1.1.6 PyYAML-6.0 alembic-1.4.1 databricks-cli-0.16.2 docker-5.0.3 funcy-1.16 gitdb-4.0.9 gitpython-3.1.24 gunicorn-20.1.0 htmlmin-0.1.12 imagehash-4.2.1 imbalanced-learn-0.7.0 joblib-1.0.1 kmodes-0.11.1 lightgbm-3.3.1 mlflow-1.22.0 mlxtend-0.19.0 multimethod-1.6 pandas-profiling-3.1.0 phik-0.12.0 prometheus-flask-exporter-0.18.6 pyLDAvis-3.2.2 pycaret-2.3.5 pydantic-1.8.2 pynndescent-0.5.5 pyod-0.9.5 python-editor-1.0.4 querystring-parser-1.2.4 requests-2.26.0 scikit-learn-0.23.2 scikit-plot-0.3.7 scipy-1.5.4 smmap-5.0.0 tangled-up-in-unicode-0.1.0 umap-learn-0.5.2 visions-0.7.4 websocket-client-1.2.3 12from pycaret.utils import enable_colabenable_colab() Colab mode enabled. 12from pycaret.datasets import get_datadataset = get_data('diamond') 12345678data = dataset.sample(frac=0.9, random_state=786)data_unseen = dataset.drop(data.index)data.reset_index(drop=True, inplace=True)data_unseen.reset_index(drop=True, inplace=True)print('Data for Modeling: ' + str(data.shape))print('Unseen Data For Predictions: ' + str(data_unseen.shape)) Data for Modeling: (5400, 8) Unseen Data For Predictions: (600, 8) 12from pycaret.regression import *exp_reg101 = setup(data = data, target = 'Price', session_id=123)","link":"/2021/12/08/python/Rgression_py/"},{"title":"House_price prediction Practice 01","text":"Kaggle 주택가격 예측Kaggle house oruces advabced regression 1234567891011121314151617# This Python 3 environment comes with many helpful analytics libraries installed# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python# For example, here's several helpful packages to loadimport numpy as np # linear algebraimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)# Input data files are available in the read-only &quot;../input/&quot; directory# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directoryimport osfor dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename))# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using &quot;Save &amp; Run All&quot; # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session 데이터 다운로드 및 불러오기1234567import pandas as pdtrain = pd.read_csv(&quot;/kaggle/input/house-prices-advanced-regression-techniques/train.csv&quot;)test = pd.read_csv(&quot;/kaggle/input/house-prices-advanced-regression-techniques/test.csv&quot;)train.shape, test.shape#변수를 줄여야 겠다. 어떤 변수를 줄여야 할까 ? EDA 이상치과 중복값 제거 overallQual (주택의 상태를 1~10등급으로 책정) 평점 1 : 판매가가 높음 = 이상치라고 판단 할 수 있따. : 이걸 제거 해 줘야 함. 12train.info()#80개 컬럼, SalePrice(독립변수)를 제거 하고는 나머지가 종속변수 = 너무 많다 !! 123train.drop(train[(train['OverallQual'] &lt; 5) &amp; (train['SalePrice']&gt; 200000)].index, inplace = True)train.reset_index(drop = True, inplace = True)train.shape 종속변수 시각화123456789101112131415161718import seaborn as snsimport matplotlib.pyplot as pltfrom scipy.stats import norm(mu, sigma) = norm.fit(train['SalePrice'])print(&quot;The value of mu before log transformation is:&quot;, mu)print(&quot;The value of sigma before log transformation is:&quot;, sigma)fig, ax = plt.subplots(figsize=(10, 6))sns.histplot(train['SalePrice'], color=&quot;b&quot;, stat=&quot;probability&quot;)ax.xaxis.grid(False)ax.set(ylabel=&quot;Frequency&quot;)ax.set(xlabel=&quot;SalePrice&quot;)ax.set(title=&quot;SalePrice distribution&quot;)plt.axvline(mu, color='r', linestyle='--')plt.text(mu + 10000, 0.11, 'Mean of SalePrice', rotation=0, color='r')fig.show() 12345678910111213141516171819import numpy as np train[&quot;SalePrice&quot;] = np.log1p(train[&quot;SalePrice&quot;]) # 로그 변환 후 종속 변수 시각화 (mu, sigma) = norm.fit(train['SalePrice'])print(&quot;The value of mu before log transformation is:&quot;, mu)print(&quot;The value of sigma before log transformation is:&quot;, sigma)fig, ax = plt.subplots(figsize=(10, 6))sns.histplot(train['SalePrice'], color=&quot;b&quot;, stat=&quot;probability&quot;)ax.xaxis.grid(False)ax.set(ylabel=&quot;Frequency&quot;)ax.set(xlabel=&quot;SalePrice&quot;)ax.set(title=&quot;SalePrice distribution&quot;)plt.axvline(mu, color='r', linestyle='--')plt.text(mu + 0.05, 0.111, 'Mean of SalePrice', rotation=0, color='r')plt.ylim(0, 0.12)fig.show() data feature 제거 모형 학습 시간 감소 연산 시 noise 감소 그래서 : train ID 를 빼주기로함 12345train_ID = train['Id']test_ID = test['Id']train.drop(['Id'], axis=1, inplace=True)test.drop(['Id'], axis=1, inplace=True)train.shape, test.shape 12345678# y 값 추출, dataset 분리할때 사용y = train['SalePrice'].reset_index(drop=True)# 뽑고 나면 원래 df에서 제거train = train.drop('SalePrice', axis = 1)train.shape, test.shape, y.shape 12345# data 합치기 # - train data 와 Test를 같이 전처리 하기 위해 all_df = pd.concat([train, test]).reset_index(drop=True)all_df.shape 결측치 확인 결측치 처리 제거하기 : column 제거, 특정 행만 제거하기 채우기 : 1) numeric(수치형) : 평균 또는 중간값으로 채우기 2) String(문자형) : 최빈값으로 채우기 통계 기법이용, 채우기 (data 보간) 실무에서는 (KNNImput)등, 시계열 자료 or 산업군에 따라 다르므로 가서 배워라. 12345678910#결측치 확인 def check_na(data, head_num = 6): isnull_na = (data.isnull().sum() / len(data)) * 100 data_na = isnull_na.drop(isnull_na[isnull_na == 0].index).sort_values(ascending=False) missing_data = pd.DataFrame({'Missing Ratio' :data_na, 'Data Type': data.dtypes[data_na.index]}) print(&quot;결측치 데이터 컬럼과 건수:\\n&quot;, missing_data.head(head_num))check_na(all_df, 20) 1234all_df.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'LotFrontage'], axis=1, inplace=True)check_na(all_df)#아직도 결측치가 많이 있다. 채우기1. 문자열 채우기 2. object column 추출 1234567891011121314151617181920212223242526#a = all_df['BsmtCond'].value_counts().mode() #mode() : 최빈값 가장 빈도수가 높은 값 찾기#aprint(all_df['BsmtCond'].value_counts())print()print(all_df['BsmtCond'].mode()[0])#object column, 갯수 확인import numpy as npcat_all_vars = train.select_dtypes(exclude=[np.number]) #숫자인 것을 제외한 type column 이름 추출print(&quot;The whole number of all_vars(문자형data)&quot;, len(list(cat_all_vars)))#column 이름 뽑아내기 final_cat_vars = []for v in cat_all_vars: if v not in ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'LotFrontage']: final_cat_vars.append(v)print(&quot;The whole number of final_cat_vars&quot;, len(final_cat_vars))#최빈값을 찾아 넣어주기for i in final_cat_vars: all_df[i] = all_df[i].fillna(all_df[i].mode()[0])check_na(all_df, 20)print(&quot;숫자형 data set의 결측치만 남은 것을 알 수 있다. &quot;) 123456789101112import numpy as npnum_all_vars = list(train.select_dtypes(include=[np.number]))print(&quot;The whole number of all_vars&quot;, len(num_all_vars))num_all_vars.remove('LotFrontage')print(&quot;The whole number of final_cat_vars&quot;, len(num_all_vars))for i in num_all_vars: all_df[i].fillna(value=all_df[i].median(), inplace=True)print(&quot;결측치가 존재 하지 않은 것을 알 수 있다. &quot;)check_na(all_df, 20) 1all_df.info() 왜도(Skewnewss) 처리하기 : 정규 분포를 이룰 수 있게 (설문조사 논문 통계의 경우 -1&lt; 외도 &lt;1) boxcose를 사용 할 예정 왜도가 양수일때, 음수일때 (좌, 우로 치우친 정도) 첨도가 양수일때, 음수일때 (뽀족한 정도) RMSE를 최적(낮게)으로 만들기 위해 조정. 1234567891011121314151617from scipy.stats import skew#외도 판정을 받을 만한 data set을 확인 def find_skew(x): return skew(x)#앞에서 뽑은 numeric columns : num_all_vars#사용자 정의함수를 쓰기 위해 apply(find_skew)를 사용, 오름차순정렬skewness_features = all_df[num_all_vars].apply(find_skew).sort_values(ascending=False)skewness_features#high_skew = skew_valrs[slew_var &gt; 1]#0~1사이에 있는 것이 기준. 기준 밖으로 나간 경우 조정이 필요(정규분포를 만들어 주기 위해)# 1. 박스코스 변환 : ML -&gt; RMSE (2.5) # 2. 로그변환 : ML -&gt; RMSE (2.1)# =&gt; RMSE는 적은 것이 좋기 때문에, 로그 변환으로 사용 하는 것이 좋다. 1234567891011121314skewnewss_index = list(skewness_features.index)skewnewss_index.remove('LotArea')#외도 정도가 너무 높은 LotArea를 날려주는 것.all_numeric_df = all_df.loc[:, skewnewss_index]fig, ax = plt.subplots(figsize=(10, 6))ax.set_xlim(0, all_numeric_df.max().sort_values(ascending=False)[0])ax = sns.boxplot(data=all_numeric_df[skewnewss_index] , orient=&quot;h&quot;, palette=&quot;Set1&quot;)ax.xaxis.grid(False)ax.set(ylabel=&quot;Feature names&quot;)ax.set(xlabel=&quot;Numeric values&quot;)ax.set(title=&quot;Numeric Distribution of Features Before Box-Cox Transformation&quot;)sns.despine(trim=True, left=True) 123456789101112from scipy.special import boxcox1pfrom scipy.stats import boxcox_normmaxhigh_skew = skewness_features[skewness_features &gt; 1]high_skew_index = high_skew.indexprint(&quot;The data before Box-Cox Transformation: \\n&quot;, all_df[high_skew_index].head())for num_var in high_skew_index: all_df[num_var] = boxcox1p(all_df[num_var], boxcox_normmax(all_df[num_var] + 1))print(&quot;The data after Box-Cox Transformation: \\n&quot;, all_df[high_skew_index].head()) 12345678fig, ax = plt.subplots(figsize=(10, 6))ax.set_xscale('log')ax = sns.boxplot(data=all_df[high_skew_index] , orient=&quot;h&quot;, palette=&quot;Set1&quot;)ax.xaxis.grid(False)ax.set(ylabel=&quot;Feature names&quot;)ax.set(xlabel=&quot;Numeric values&quot;)ax.set(title=&quot;Numeric Distribution of Features Before Box-Cox Transformation&quot;)sns.despine(trim=True, left=True) 도출 변수Feature Engineering 의 Key step 판매량, 단가, 매출액 X 판매량 X 단가 = 매출액(New Value) : 도출 변수 ML은 수식이기 때문에 도출변수가 생성 되는것은 연산의 증가로 이어진다. 시간이 오래 걸린다. 결론 : 변수를 줄이는 것이 좋다. 1234#집의 층수 를 더해서 전체면적이라는 변수를 도출 all_df['TotalSF'] = all_df['TotalBsmtSF'] + all_df['1stFlrSF'] + all_df['2ndFlrSF']all_df = all_df.drop(['TotalBsmtSF', '1stFlrSF', '2ndFlrSF'], axis=1)print(all_df.shape) 1234all_df['Total_Bathrooms'] = (all_df['FullBath'] + (0.5 * all_df['HalfBath']) + all_df['BsmtFullBath'] + (0.5 * all_df['BsmtHalfBath']))all_df['Total_porch_sf'] = (all_df['OpenPorchSF'] + all_df['3SsnPorch'] + all_df['EnclosedPorch'] + all_df['ScreenPorch'])all_df = all_df.drop(['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath', 'OpenPorchSF', '3SsnPorch', 'EnclosedPorch', 'ScreenPorch'], axis=1)print(all_df.shape) 따라서 data 정의서 먼저 봐야 한다. : data description.txt를 먼저 봐야 한다. !!! (실무에서는 없는 경우가 많다.) 시각화 : 각각의 data 무한작업, 도메인 공부 1234567891011# 연도와 관련된.num_all_vars = list(train.select_dtypes(include=[np.number]))year_feature = []for var in num_all_vars: if 'Yr' in var: year_feature.append(var) elif 'Year' in var: year_feature.append(var) else: print(var, &quot;is not related with Year&quot;)print(year_feature) 12345678fig, ax = plt.subplots(3, 1, figsize=(10, 6), sharex=True, sharey=True)for i, var in enumerate(year_feature): if var != 'YrSold': ax[i].scatter(train[var], y, alpha=0.3) ax[i].set_title('{}'.format(var), size=15) ax[i].set_ylabel('SalePrice', size=15, labelpad=12.5)plt.tight_layout()plt.show() 12all_df = all_df.drop(['YearBuilt', 'GarageYrBlt'], axis=1)print(all_df.shape) 123456# 리모델링 시점으로 부터 얼마나 되었나 + 팔리는거YearsSinceRemodel = train['YrSold'].astype(int) - train['YearRemodAdd'].astype(int)fig, ax = plt.subplots(figsize=(10, 6))ax.scatter(YearsSinceRemodel, y, alpha=0.3)fig.show() 123all_df['YearsSinceRemodel'] = all_df['YrSold'].astype(int) - all_df['YearRemodAdd'].astype(int)all_df = all_df.drop(['YrSold', 'YearRemodAdd'], axis=1)print(all_df.shape) 더미변수String data (non-Numeric) 명목형 : 남학생, 여학생… 서열형(순서) : 1등급, 2등급, 3등급 (가중치, 등급숫자 등으로 바꿀 수 있다. ) 세부적으로 customize 하는 것이 낫다. 명목형 series에 따라 17개의 model을 개별적으로 만들되, 하나의 모델처럼 보이게 시각화 대시보드로 만들어 줘야 합니다. 12all_df['PoolArea'].value_counts()#0과 다른값들.. 으로 되어있어서 123456# 0과 1로 나누어 적용def count_dummy(x): if x &gt; 0: return 1 else: return 0 1234all_df['PoolArea'] = all_df['PoolArea'].apply(count_dummy)all_df['PoolArea'].value_counts()# 전체 경향 등에 거의 영향을 주지 않음 12all_df['GarageArea'] = all_df['GarageArea'].apply(count_dummy)all_df['GarageArea'].value_counts() 12all_df['Fireplaces'] = all_df['Fireplaces'].apply(count_dummy)all_df['Fireplaces'].value_counts() Label Encoding, Ordinal Encoding, One-Hot Encoding Label Encoding : 종속변수에만 Ordinal Encoding : 독립변수에만 써야 하지만, 개념은 같다. One-Hot Encoding : 12345678910from sklearn.preprocessing import LabelEncoderimport pandas as pdtemp = pd.DataFrame({'Food_Name': ['Apple', 'Chicken', 'Broccoli'], 'Calories': [95, 231, 50]})encoder = LabelEncoder()encoder.fit(temp['Food_Name'])labels = encoder.transform(temp['Food_Name'])print(list(temp['Food_Name']), &quot;==&gt;&quot;, labels) 1234567891011from sklearn.preprocessing import OrdinalEncoderimport pandas as pdtemp = pd.DataFrame({'Food_Name': ['Apple', 'Chicken', 'Broccoli'], 'Calories': [95, 231, 50]})encoder = OrdinalEncoder()labels = encoder.fit_transform(temp[['Food_Name']])print(list(temp['Food_Name']), &quot;==&gt;&quot;, labels.tolist()) 12345678910# import pandas as pd# temp = pd.DataFrame({'Food_Name': ['Apple', 'Chicken', 'Broccoli'], # 'Calories': [95, 231, 50]})# temp[['Food_No']] = temp.Food_Name.replace(['Chicken', 'Broccoli', 'Apple'],[1, 2, 3])# print(temp[['Food_Name', 'Food_No']])#ValueError: Columns must be same length as key 12345678import pandas as pdtemp = pd.DataFrame({'Food_Name': ['Apple', 'Chicken', 'Broccoli'], 'Calories': [95, 231, 50]})temp = pd.get_dummies(temp)print(temp)print(temp.shape) 12all_df = pd.get_dummies(all_df).reset_index(drop=True)all_df.shape 머신러닝 모형 학습 및 평가데이터셋 분리 및 교차 검증 123X = all_df.iloc[:len(y), :]X_test = all_df.iloc[len(y):, :]X.shape, y.shape, X_test.shape 123from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)X_train.shape, X_test.shape, y_train.shape, y_test.shape 평가지표MAE 12345678910import numpy as npdef mean_absolute_error(y_true, y_pred): error = 0 for yt, yp in zip(y_true, y_pred): error = error + np.abs(yt-yp) mae = error / len(y_true) return mae 12345678910import numpy as npdef mean_squared_error(y_true, y_pred): error = 0 for yt, yp in zip(y_true, y_pred): error = error + (yt - yp) ** 2 mse = error / len(y_true) return mse RMSE 1234567891011import numpy as npdef root_rmse_squared_error(y_true, ypred): error = 0 for yt, yp in zip(y_true, y_pred): error = error + (yt - yp) ** 2 mse = error / len(y_true) rmse = np.round(np.sqrt(mse), 3) return rmse Test1123456y_true = [400, 300, 800]y_pred = [380, 320, 777]print(&quot;MAE:&quot;, mean_absolute_error(y_true, y_pred))print(&quot;MSE:&quot;, mean_squared_error(y_true, y_pred))print(&quot;RMSE:&quot;, root_rmse_squared_error(y_true, y_pred)) Test2123456y_true = [400, 300, 800, 900]y_pred = [380, 320, 777, 600]print(&quot;MAE:&quot;, mean_absolute_error(y_true, y_pred))print(&quot;MSE:&quot;, mean_squared_error(y_true, y_pred))print(&quot;RMSE:&quot;, root_rmse_squared_error(y_true, y_pred)) RMSE with Sklean 1234from sklearn.metrics import mean_squared_errordef rmsle(y_true, y_pred): return np.sqrt(mean_squared_error(y_true, y_pred)) 모형 정의 및 검증 평가 1234567891011121314from sklearn.metrics import mean_squared_errorfrom sklearn.model_selection import KFold, cross_val_scorefrom sklearn.linear_model import LinearRegressiondef cv_rmse(model, n_folds=5): cv = KFold(n_splits=n_folds, random_state=42, shuffle=True) rmse_list = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv)) print('CV RMSE value list:', np.round(rmse_list, 4)) print('CV RMSE mean value:', np.round(np.mean(rmse_list), 4)) return (rmse_list)n_folds = 5rmse_scores = {}lr_model = LinearRegression() 123score = cv_rmse(lr_model, n_folds)print(&quot;linear regression - mean: {:.4f} (std: {:.4f})&quot;.format(score.mean(), score.std()))rmse_scores['linear regression'] = (score.mean(), score.std()) 첫번째 최종 예측 값 제출123456789from sklearn.model_selection import cross_val_predictX = all_df.iloc[:len(y), :]X_test = all_df.iloc[len(y):, :]X.shape, y.shape, X_test.shapelr_model_fit = lr_model.fit(X, y)final_preds = np.floor(np.expm1(lr_model_fit.predict(X_test)))print(final_preds) 1234submission = pd.read_csv(&quot;sample_submission.csv&quot;)submission.iloc[:,1] = final_predsprint(submission.head())submission.to_csv(&quot;The_first_regression.csv&quot;, index=False) 모형 알고리즘 추가123456789101112131415from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressorfrom sklearn.tree import DecisionTreeRegressorfrom sklearn.linear_model import LinearRegression# LinearRegresisonlr_model = LinearRegression()# Tree Decision tree_model = DecisionTreeRegressor()# Random Forest Regressorrf_model = RandomForestRegressor()# Gradient Boosting Regressorgbr_model = GradientBoostingRegressor() 123score = cv_rmse(lr_model, n_folds)print(&quot;linear regression - mean: {:.4f} (std: {:.4f})&quot;.format(score.mean(), score.std()))rmse_scores['linear regression'] = (score.mean(), score.std()) 123score = cv_rmse(tree_model, n_folds)print(&quot;Decision Tree Regressor - mean: {:.4f} (std: {:.4f})&quot;.format(score.mean(), score.std()))rmse_scores['Decision Tree Regressor'] = (score.mean(), score.std()) 123score = cv_rmse(rf_model, n_folds)print(&quot;RandomForest Regressor - mean: {:.4f} (std: {:.4f})&quot;.format(score.mean(), score.std()))rmse_scores['RandomForest Regressor'] = (score.mean(), score.std()) 123score = cv_rmse(gbr_model, n_folds)print(&quot;Gradient Boosting Regressor - mean: {:.4f} (std: {:.4f})&quot;.format(score.mean(), score.std()))rmse_scores['Gradient Boosting Regressor'] = (score.mean(), score.std()) 1234567891011121314fig, ax = plt.subplots(figsize=(10, 6))ax = sns.pointplot(x=list(rmse_scores.keys()), y=[score for score, _ in rmse_scores.values()], markers=['o'], linestyles=['-'], ax=ax)for i, score in enumerate(rmse_scores.values()): ax.text(i, score[0] + 0.002, '{:.6f}'.format(score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')ax.set_ylabel('Score (RMSE)', size=20, labelpad=12.5)ax.set_xlabel('Model', size=20, labelpad=12.5)ax.tick_params(axis='x', labelsize=13.5, rotation=10)ax.tick_params(axis='y', labelsize=12.5)ax.set_ylim(0, 0.25)ax.set_title('Rmse Scores of Models without Blended_Predictions', size=20)fig.show() RMSE 가 적은 것이 좋다. : 예측이 잘 된 Model이라고 할 수 있다. 1234567891011lr_model_fit = lr_model.fit(X, y)tree_model_fit = tree_model.fit(X, y)rf_model_fit = rf_model.fit(X, y)gbr_model_fit = gbr_model.fit(X, y)def blended_learning_predictions(X): blended_score = (0.3 * lr_model_fit.predict(X)) + \\ (0.1 * tree_model_fit.predict(X)) + \\ (0.3 * gbr_model_fit.predict(X)) + \\ (0.3* rf_model_fit.predict(X)) return blended_score 1234blended_score = rmsle(y, blended_learning_predictions(X))rmse_scores['blended'] = (blended_score, 0)print('RMSLE score on train data:')print(blended_score) 123456789101112131415fig, ax = plt.subplots(figsize=(10, 6))ax = sns.pointplot(x=list(rmse_scores.keys()), y=[score for score, _ in rmse_scores.values()], markers=['o'], linestyles=['-'], ax=ax)for i, score in enumerate(rmse_scores.values()): ax.text(i, score[0] + 0.002, '{:.6f}'.format(score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')ax.set_ylabel('Score (RMSE)', size=20, labelpad=12.5)ax.set_xlabel('Model', size=20, labelpad=12.5)ax.tick_params(axis='x', labelsize=13.5, rotation=10)ax.tick_params(axis='y', labelsize=12.5)ax.set_ylim(0, 0.25)ax.set_title('Rmse Scores of Models with Blended_Predictions', size=20)fig.show() 12submission.iloc[:,1] = np.floor(np.expm1(blended_learning_predictions(X_test)))submission.to_csv(&quot;The_second_regression.csv&quot;, index=False)","link":"/2021/12/09/python/house-price-pred-practice01/"},{"title":"Python 함수 실행하기","text":"day 1 Lecture (02) 사용자 함수 만들고 실행 하기 personal function 만들기 python에서 함수 만들기 사용자 정의 함수&lt;형식&gt; 12345def func_name(parameter): # ... # do something codes # ... return parameter 사용자가 직접 만들어 사용하는 함술로 매개변수 (parameter: 함수에 입력으로 전달 된 값을 받는 변수)와인자(argument: 함수를 호출 할 때 전달 하는 입력값)이 있다. 1234567#/c/Users/brill/Desktop/PyThon_Function/venv/Scripts/python# -*- coding : utf-8 -*-def cnt_letter(): &quot;&quot;&quot;안에 있는 문자를 세는 함수입니다. &quot;&quot;&quot; # 함수를 설명 하는 문구 print(&quot;hi&quot;) return None python 문서를 만들때 어떤 위치 (directory)에서 작업 했는지 제일 위에 써 줘야 한다. 또한 cording을 어떤 언어로 했는지 확인 해 줘야 한다. (한글 : UTF-8) def : definition 함수를 정의한다. 뒤에 cnt_letter라는 함수 이름을 써준다. def cnt_letter(): ()안에 args를 넣어 주면 된다. Doc : “”” 여기 “”” 여기 안에 이 함수가 어떤 함수인지 설명을 써 주어야 한다. 혼자 일 하는 것이 아니기 때문에 함수를 공통적으로 사용 하기때문. print : print return : return None 함수 실행하기1234if __name__ == &quot;__main__&quot;: print(cnt_letter()) print(help(cnt_letter())) &quot;&quot;&quot;도움말 : 어떤 함수인지 확인 할 수 있다. &quot;&quot;&quot; 함수를 실행한다. name 은 글로벌 변수로 보통 __main__으로 할당된다. import 할 때와 구별 하려고 사용한다고 하는데 잘 모르겠다. (외워) cnt_letter() 함수를 print 한다. help 함수는 python 내장 함수인데, class NoneType(object) | Methods defined here: | | bool(self, /) | self != 0 | | repr(self, /) | Return repr(self). | | ———————————————————– | Static methods defined here: | | new(*args, **kwargs) from builtins.type | Create and return a new object. See help(type) for accurate signature. None 실행창에 위와 같은 도움을 준다. 실행이 성공하면 아래와 같은 결과값이 나온다. Process finished with exit code 0 오늘 들은 강의보다 100배 나은 tistory가 있어서 공유 해본다. .. 아… 내 인생 사용자 정의 함수","link":"/2021/12/07/python/pythonL_functionRun/"},{"title":"name &amp; namespace","text":"Name name : identifier, variable, reference object의 구분을 위해 사용 object object는 value, type, 주소 의 3원소를 가지고 있다. Ex) 12345a = 10b = 0.12print(a + b) Memory에 저장된 형태 object(10, int, ref_cnt=1)0x100 = 100번째 저장된(주소) object는 10(value), int(type)의 속성을 갖는다. ** 하나의 object는 여러개의 이름(name or ref.)를 가질 수 있다. ** ref_cnt는 reference count를 의미한다. name binding- assignment - import - class 정의 - function 정의 ... assignment dictionary (key : value) name binding (name : address) 객체(object)에 이름과 주소를 할당 이름 작성 규칙 하나의 단어로 작성, 문자(한글), 숫자, _ 가능(띄워쓰기 불가능) Keyword(+ 다른곳의 함수 이름)은 안쓰는 것이 좋고 쓰지만자 숫자로 시작할 수 없으며, 대소문자 구분가능 _로 시작하는 이름은 class에서 쓰이므로 지양 name space namespace는 이름 관리를 위한 dict container이다. 모든 class, instance, fuction은 자신의 namespace를 가지고 있다. built-in namespacepython에서 제공하는 함수, class, instance 등이 들어있는 곳. Built-in Functions global name space내가 만든 함수, 인수, class들이 들어 있는 곳 12345a = 100b = ac = aa = 101print(a, b, c) ◎ namespace (global) a : 0x100 b : 0x100 c : 0x100 ◎ in memory object(100, int ref_cnt=3) 0x100 object(101, int ref_cnt=1) 0x200 그렇다면, 왜 ref_cnt 를 하는 것일까 ?? 사용되지 않는 객체를 삭제 하기 위해 하나의 객체가 여러개의 이름을 가질 수있다. 하지만, 하나의 이름은 하나의 객체만 가질 수 있다. In 1234567891011import sysa= &quot;Python&quot;b= ac= a a= &quot;python&quot;print(f'a=[a], b=[b], c=[c]')sys.getrefcount(a)sys.getrefcount(b)sys.getrefcount(c) Out a=[a], b=[b], c=[c] 4 5 5 키워드pprint 안의 함수 pprint를 사용하여 키워드 리스트를 출력해 본다. in 1234import keywordimport pprint as pppp.pprint(keyword.kwlist) out [‘False’, ‘None’, ‘True’, ‘_peg_parser_‘, ‘and’, ‘as’, ‘assert’, ‘async’, ‘await’, ‘break’, ‘class’, ‘continue’, ‘def’, ‘del’, ‘elif’, ‘else’, ‘except’, ‘finally’, ‘for’, ‘from’, ‘global’, ‘if’, ‘import’, ‘in’, ‘is’, ‘lambda’, ‘nonlocal’, ‘not’, ‘or’, ‘pass’, ‘raise’, ‘return’, ‘try’, ‘while’, ‘with’, ‘yield’] assignment assignment는 Expression이 아닌 statment이다. Expression은 한개의 객체로 Evaluate될 수 있는것 이름에 binding할 수 있다. (syntax에서 사용 할 수 있는 위치가 다르기 때문) ver 3.8~ assignment expression (:=)이 추가되어 제공 assignment의 종류 종류 기호 ex exp assignment = a = 10 10에 a 를 바인딩 assignmented assignment **=, +=, -=, *=, //=, %=. &lt;&lt;=, &gt;&gt;=, &amp;=, |=, ^=, @= a+= 10 a에 10을 더한 결과 객체에 a를 바인딩 덧셈의 ref_cnt123456print(&quot;**********&quot;)i = 10print(id(i))i += 1print(id(i))print(&quot;**********&quot;) i가 10이때, i += 1 을 하면 11이라는 것이 만들어 진다. 이 11은 새로운 객체이다. 새로운 객체가 생성 되기 때문에 id가 달라진다. (memory의 adress) pack &amp; unPack pack : (,) 콤마를 이용하여 Tuple 객체 하나 생성 unpack : 1개의 묶음에 있는 여러개의 객체 아이템이 분리되어 각각의 이름에 바인딩 됨. 이와 같은 pack과 unpack을 이용 하면, 여러 이름에 여러 값을 부여하기 쉽다. in 123456789101112131415data_int = 1data_Tuple = 1,data = 10, 20, 30first, second, third = dataprint(first, second, third)def function(): a = 10 b = 20 print(locals()) del b print(locals())function() Out 10 20 30 {‘a’: 10, ‘b’: 20} {‘a’: 10} del (변수) : python에 있는 좋은 기능 변수를 삭제 할 수 있다. locals() : local안에 있는 변수를 확인 할 수 있다. type() : 변수의 data type을 확인 할 수 있다. Ref. youtube, 1hr","link":"/2021/12/06/python/nameNnamespace/"},{"title":"Python Class and function","text":"day 1 Lecture (01) 1. 새로운 Project 시작 python project를 새로 시작 해 보자. pycham을 실행하여 project file을 만들어도 되지만, 원하는 directory에 file 을 만들고, 그 file에서 pycharm을 실행 해도 된다. 2. main.py :stop point 12345678910111213# This is a sample Python script.# Press Shift+F10 to execute it or replace it with your code.# Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.# 인공지능 / 머신러닝 --&gt; 리서치 관점 (논문리뷰, 정리, End User) / Engineer 관점def print_hi(name): # Use a breakpoint in the code line below to debug your script. print(f'Hi, {name}') # Press Ctrl+F8 to toggle the breakpoint.# f string# Press the green button in the gutter to run the script. BreakPoint def name(name): 함수 정의 하기1234567def print_hi(name): print(f'Hi, {name}')if __name__ == '__main__': print_hi('PyCharm')print(&quot;hi&quot;, name) Ctrl + shift + F10 : Run def 로 함수 정의 이름(print_hi), 인수(name)을 설정 : 원하는 동작(print(f’Hi, {name})) 넣기 함수 실행 : 왜 이렇게 해야 하는가를 공부 해 오자. if name == ‘main‘: print_hi(‘PyCharm’) print(“hi”, print_hi(‘YH’)) Hi, PyCharm Hi, YH hi None 3. Import.123456numpy == 1.21.4pandas ==1.3.4seaborn==0.11.2matplotlib==3.5.0scipy ==1.7.3scikit-learn == 1.0.1 가상환경 설정을 하는 이유 Python PKG : 20~30 만개 있다. 라이브러리 : 종속성이 있다. 범용성이 좋아서 data analysis, web site development, 앱, 게임 등 개발, GUI, 개발 그 외 여러가지 가능 Matplotlib (기초가 되는 라이브러리, 3.5.0)를 참조하여 seaborn, plotly, … 버전 관리 불가. data 분석을 위한 가상 환경같은 Local machine 위에Game 개발을 위한 가상 환경 조성 (다른 버전을 사용 할 수 있다. ) PKG 설정: https://pypi.org 에 들어가서 PKG 버전을보고 설치 하면 된다. 4. venu : 가상 환경 설정 1234567891011#terminal$ source ./venv/Scripts/activate(venv)#terminal$ which python/c/Users/brill/Desktop/PyThon_Function/venv/Scripts/python(venv) 가상환경 설정 $ which python /c/ProgramData/Anaconda3/python $ source ./venv/Scripts/activate (venv) $ which python /c/Users/brill/Desktop/PyThon_Function/venv/Scripts/python (venv) (venv) 가 있어야 가상환경 설정이 된것이다. terminal에서 pandas import 하는 방법12brill@DESKTOP-1IO6A45 MINGW64 ~/Desktop/PyThon_Function (master)$ pip install pandas Requirement already satisfied: pandas in c:\\users\\brill\\desktop\\python_function\\venv\\lib\\site-packages (1.3.4)Requirement already satisfied: pytz&gt;=2017.3 in c:\\users\\brill\\desktop\\python_function\\venv\\lib\\site-packages (from pandas) (2021.3)Requirement already satisfied: numpy&gt;=1.17.3 in c:\\users\\brill\\desktop\\python_function\\venv\\lib\\site-packages (from pandas) (1.21.4)Requirement already satisfied: python-dateutil&gt;=2.7.3 in c:\\users\\brill\\desktop\\python_function\\venv\\libsite-packages (from pandas) (2.8.2)Requirement already satisfied: six&gt;=1.5 in c:\\users\\brill\\desktop\\python_function\\venv\\lib\\site-packages(from python-dateutil&gt;=2.7.3-&gt;pandas) (1.16.0)WARNING: You are using pip version 21.1.2; however, version 21.3.1 is available.You should consider upgrading via the ‘C:\\Users\\brill\\Desktop\\PyThon_Function\\venv\\Scripts\\python.exe -m pip install –upgrade pip’ command.(venv) pandas 등을 하나씩 install 하는 방법도 있지만, file(requirements)를 만들어 install 하는 방법도 있다. 123456789101112131415161718192021# /c/Users/brill/Desktop/PyThon_Function/venv/Scripts/python# -*- coding : utf-8 -*-&quot;&quot;&quot;모방 : 부모 클래스를 모방 후 자식이 창조&quot;&quot;&quot;import pandas as pdclass newDataFrame(pd.dataFrame): passtemp_dic = {&quot;A&quot;: [1, 2, 3], &quot;B&quot;:[4, 5, 6]}if __name__ == &quot;__main__&quot;: temp = pd.DataFrame(temp_dic, columns=[&quot;A&quot;,&quot;B&quot;]) print(temp) print(&quot;-----------------&quot;) temp2 = newDataFrame(temp_dic, columns=[&quot;B&quot;, &quot;A&quot;]) print(temp2)","link":"/2021/12/06/python/pythonLectureD1/"},{"title":"python_basic_Bank","text":"pythonBank _ 계좌 만들기12345678910111213141516# /c/Users/brill/Desktop/PyThon_Function/venv/Scripts/python# -*- coding : UTF-8class Human: def __init__(self, name): self.name = nameif __name__ == &quot;__main__&quot;: human01 = Human(name=&quot;A&quot;) human02 = Human(name=&quot;A&quot;) print(human01 == human02) print(&quot;human 01 : &quot;, human01) print(&quot;human 02 : &quot;, human02) Falsehuman 01 : &lt;__main__.Human object at 0x000001686E41CC10&gt;human 02 : &lt;__main__.Human object at 0x000001686E41CE50&gt; 저장되는 장소가 다르기 때문에 다르다. Bank _ customer ID 확인하기1234567891011121314151617181920212223242526# /c/Users/brill/Desktop/PyThon_Function/venv/Scripts/python# -*- coding : UTF-8class Bank: #instance attribute def __init__(self, cust_id, balance=0): self.balance = balance self.cust_id = cust_id #instance methode def withdraw(self, amount): self.balance -= amount def __eq__(self, other): print(&quot;__eq()__ is called&quot;) return self.cust_id == other.cust_idif __name__ == &quot;__main__&quot;: account01 = Bank(123, 1000) account02 = Bank(123, 1000) account03 = Bank(456, 1000) print(account01 == account02) print(account02 == account03) print(account01 == account03) eq() is calledTrueeq() is calledFalseeq() is calledFalse 부등호 연산자 != : ne() &gt;= : ge() &lt;= : le() &gt; : gt() &lt; : lt() eq() 함수 사용하기123456789101112131415161718192021222324252627282930313233# /c/Users/brill/Desktop/PyThon_Function/venv/Scripts/python# -*- coding : UTF-8class Bank: #instance attribute def __init__(self, cust_id, balance=0): self.balance, self.cust_id = balance, cust_id #instance methode def withdraw(self, amount): self.balance -= amount def __eq__(self, other): print(&quot;__eq()__ is called&quot;) return (self.cust_id == other.cust_id) and (type(self) == type(other))class Phone: def __init__(self, cust_id): self.cust_id = cust_id def __eq__(self, other): return self.cust_id == other.cust_idif __name__ == &quot;__main__&quot;: account01 = Bank(1234) phone01 = Phone(1234) print(account01 == phone01) eq() is calledFalse eq를 불러와서 같은지 확인 할 수 있다. 접근기록, log 기록 확인하기12345678910111213141516171819202122# /c/Users/brill/Desktop/PyThon_Function/venv/Scripts/python# -*- coding : UTF-8class Bank: def __init__(self, cust_id, name, balance = 0): self.cust_id, self.name, self.balance = cust_id, name, balance def __str__(self): cust_str = &quot;&quot;&quot; customer: cust_id : {cust_id} name : {name} balance : {balance} &quot;&quot;&quot;.format(cust_id = self.cust_id, name = self.name, balance= self.balance) return cust_strif __name__ == &quot;__main__&quot;: bank_cust = Bank(123, &quot;YH&quot;) print(bank_cust) DB에 저장 되지 않지만, 로그 기록을 확인 할 수 있다. str() and repr() 비교1234567891011121314151617181920212223242526# /c/Users/brill/Desktop/PyThon_Function/venv/Scripts/python# -*- coding : UTF-8class Bank: def __init__(self, cust_id, name, balance = 0): self.cust_id, self.name, self.balance = cust_id, name, balance def __str__(self): cust_str = &quot;&quot;&quot; customer: cust_id : {cust_id} name : {name} balance : {balance} &quot;&quot;&quot;.format(cust_id = self.cust_id, name = self.name, balance= self.balance) return cust_str def __repr__(self): cust_str = &quot;Bank({cust_id}, '{name}', {balance})&quot;.format(cust_id = self.cust_id, name = self.name, balance= self.balance) return cust_strif __name__ == &quot;__main__&quot;: bank_cust = Bank(123, &quot;YH&quot;) print(str(bank_cust)) print(repr(bank_cust)) difference of str() and repr()","link":"/2021/12/13/python/python_basic211213/"},{"title":"sklearn_mL_04_ModuleSelection(2.4)","text":"Chepter 2 _사이킷런으로 시작하는 머신러닝(04)파이썬 머신러닝 완벽 가이드 ref. &amp; copyright(c) Book Model Selection 모듈 소개 사이킷런 (scikit-Learn) : 파이썬 머신러닝 라이브러리 중 가장 많이 사용되는 라이브러리 12345import sklearnprint(cklearn.__version__)","link":"/2021/12/10/python/sklearn_mL_04_ModuleSelection/"},{"title":"Make Timer function in python","text":"시간 측정 decorator 함수 만들기 import time1import time 실행 시간을 확인 하는 함수 만들기1234567891011121314151617def timer(func): &quot;&quot;&quot; 함수 실행 시간 확인 :param func: check할 함수 넣을거임 :return: 걸린 시간 &quot;&quot;&quot; def wrapper(*args, **kwargs): #현재 시간 time_start = time.time() #decorated function 불러오기 result = func(*args, **kwargs) time_total = time.time() - time_start# print(&quot;{}, Total time is {: .2f} sec.&quot;.format(func.__name__, time_total)) return result return wrapper 가변 매개 변수 args(*) 함수를 정의할때 앞에 *가 붙어 있으면, 정해지지 않은 수의 매개변수를 받겠다는 의미 가변 매개변수는 입력 받은 인수를 튜플형식으로 packing 한다. 일반적으로 *args (arguments의 약자로 관례적으로 사용) 를 사용한다. 다른 매개 변수와 혼용가능 키워드 매개변수 kwargs(**) 함수에서 정의되지 않은 매개변수를 받을 때 사용되며, 딕셔너리 형식으로 전달. 일반 매개변수, 가변 매개변수와 함께 일는 경우 순서를 지켜야함 (일반&gt;가변&gt;키워드 순) **kwargs (Keyword arguments의 약자로 관례적으로 사용 ) decorator 함수를 이용하여 시간 확인 함수 설정, 실행12345678@timerdef check_time(num): time.sleep(num)if __name__ == &quot;__main__&quot;: check_time(1.5) out check_time, Total time is 1.50 sec. 관련 이론들을 아래에 적어 놓았다. timestamp :python에서 time은 1970년 1월 1일 0시 0분 0초를 기준으로 경과한 시간을 나타냄 time_struct class timestamp가 주어 졌을때 날짜와 시간을 알아내기 위한 API 제공 name value Ex. tm_year year 1993, 2021 tm_mon month 1~12 tm_mday day 1~31 tm_hour hour 0~23 tm_min minute 0~59 tm_sec second 0~61 tm_wday 요일 0~6 (0 : MON) tm_yday 연중 경과일 1~366 tm_isdst summertime 0: unapply 1: apply time() 함수 현재 timestamp 얻기 in 123secs = time.time()print(secs) out 1638870356.8049076 unix timestamp는 소수로 값을 return, 정수 부분이 초 단위. 부가적인 time 함수들 gmtime() : GMT 기준의 time_struct type으로 변환 in 12tm = time.gmtime(secs)print(tm) out time.struct_time(tm_year=2021, tm_mon=12, tm_mday=7, tm_hour=9, tm_min=53, tm_sec=5, tm_wday=1, tm_yday=341, tm_isdst=0) localtime() : 현지 시간대 기준의 time_struct type으로 변환 in 1234567tm = time.localtime(secs)print(&quot;year:&quot;, tm.tm_year)print(&quot;month:&quot;, tm.tm_mon)print(&quot;day:&quot;, tm.tm_mday)print(&quot;hour:&quot;, tm.tm_hour)print(&quot;minute:&quot;, tm.tm_min)print(&quot;second:&quot;, tm.tm_sec) out year: 2021month: 12day: 7hour: 18minute: 53second: 5 ctime() : 요일 월 일 시:분:초 연도 형식으로 출력 in 1string = time.ctime(secs) out Tue Dec 7 18:56:03 2021 strftime() : strftime 과 같은 특정 형식으로 변환가능 parameter로 time_struct type data를 받기 때문에 위의 함수들을 사용해서 data를 strftime()으로 넘겨야 함. in 123tmt = time.localtime(secs)string = time.strftime('%Y-%m-%d %I:%M:%S %p', tmt)print(string) out 2021-12-07 07:00:54 PM 인자를 쓰는동안 secs로 계속 썻더니 시간이 계속 올라가고있다 하하하 ^0^ strptime() : strftime과 같은 특정 포멧의 시간을 time_struct type 으로 변경. in 123string = '2021-12-07 07:00:54 PM'tmm = time.strptime(string, '%Y-%m-%d %I:%M:%S %p')print(tmm) out time.struct_time(tm_year=2021, tm_mon=12, tm_mday=7, tm_hour=19, tm_min=0,tm_sec=54, tm_wday=1, tm_yday=341, tm_isdst=-1) sleep() : 일정 시간 동안 시간 지연 시키기 in 123print(&quot;Start--&gt;&quot;)time.sleep(1.5)print(&quot;&lt;--End&quot;) out Start–&gt;&lt;–End time.sleep(sec) : 초 단위로 시간을 지연 시킨다.","link":"/2021/12/07/python/timer_LectureD2/"}],"tags":[{"name":"naver","slug":"naver","link":"/tags/naver/"},{"name":"R4ds","slug":"R4ds","link":"/tags/R4ds/"},{"name":"dataScience","slug":"dataScience","link":"/tags/dataScience/"},{"name":"ggplot2","slug":"ggplot2","link":"/tags/ggplot2/"},{"name":"TextMining","slug":"TextMining","link":"/tags/TextMining/"},{"name":"Token","slug":"Token","link":"/tags/Token/"},{"name":"Study","slug":"Study","link":"/tags/Study/"},{"name":"markdown","slug":"markdown","link":"/tags/markdown/"},{"name":"table","slug":"table","link":"/tags/table/"},{"name":"table arrange","slug":"table-arrange","link":"/tags/table-arrange/"},{"name":"pipe","slug":"pipe","link":"/tags/pipe/"},{"name":"Test page,","slug":"Test-page","link":"/tags/Test-page/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"bioinformatics","slug":"bioinformatics","link":"/tags/bioinformatics/"},{"name":"BioDataScientist","slug":"BioDataScientist","link":"/tags/BioDataScientist/"},{"name":"BioData_Science","slug":"BioData-Science","link":"/tags/BioData-Science/"},{"name":"machineLearning","slug":"machineLearning","link":"/tags/machineLearning/"},{"name":"decisionTree","slug":"decisionTree","link":"/tags/decisionTree/"},{"name":"predictionModel","slug":"predictionModel","link":"/tags/predictionModel/"},{"name":"Classifier","slug":"Classifier","link":"/tags/Classifier/"},{"name":"google, colaboratory, github, upload","slug":"google-colaboratory-github-upload","link":"/tags/google-colaboratory-github-upload/"},{"name":"DataScience","slug":"DataScience","link":"/tags/DataScience/"},{"name":"github","slug":"github","link":"/tags/github/"},{"name":"makeBlog, makegithub","slug":"makeBlog-makegithub","link":"/tags/makeBlog-makegithub/"},{"name":"makeBlog","slug":"makeBlog","link":"/tags/makeBlog/"},{"name":"makegithub","slug":"makegithub","link":"/tags/makegithub/"},{"name":"draft","slug":"draft","link":"/tags/draft/"},{"name":"githubBlog","slug":"githubBlog","link":"/tags/githubBlog/"},{"name":"notion","slug":"notion","link":"/tags/notion/"},{"name":"Numpy","slug":"Numpy","link":"/tags/Numpy/"},{"name":"Pandas","slug":"Pandas","link":"/tags/Pandas/"},{"name":"List","slug":"List","link":"/tags/List/"},{"name":"studyPython","slug":"studyPython","link":"/tags/studyPython/"},{"name":"Tuple","slug":"Tuple","link":"/tags/Tuple/"},{"name":"kaggle","slug":"kaggle","link":"/tags/kaggle/"},{"name":"kaggleNote","slug":"kaggleNote","link":"/tags/kaggleNote/"},{"name":"googleColab","slug":"googleColab","link":"/tags/googleColab/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"Matplotlib","slug":"Matplotlib","link":"/tags/Matplotlib/"},{"name":"Seaborn","slug":"Seaborn","link":"/tags/Seaborn/"},{"name":"paperReview","slug":"paperReview","link":"/tags/paperReview/"},{"name":"title","slug":"title","link":"/tags/title/"},{"name":"coding","slug":"coding","link":"/tags/coding/"},{"name":"Plotly","slug":"Plotly","link":"/tags/Plotly/"},{"name":"plot","slug":"plot","link":"/tags/plot/"},{"name":"kaggle_Competition","slug":"kaggle-Competition","link":"/tags/kaggle-Competition/"},{"name":"DataFrame","slug":"DataFrame","link":"/tags/DataFrame/"},{"name":"Mow","slug":"Mow","link":"/tags/Mow/"},{"name":"green","slug":"green","link":"/tags/green/"},{"name":"howtoGithub","slug":"howtoGithub","link":"/tags/howtoGithub/"},{"name":"python_basic","slug":"python-basic","link":"/tags/python-basic/"},{"name":"multiplace","slug":"multiplace","link":"/tags/multiplace/"},{"name":"깃허브","slug":"깃허브","link":"/tags/%EA%B9%83%ED%97%88%EB%B8%8C/"},{"name":"method","slug":"method","link":"/tags/method/"},{"name":"repository","slug":"repository","link":"/tags/repository/"},{"name":"make","slug":"make","link":"/tags/make/"},{"name":"panel","slug":"panel","link":"/tags/panel/"},{"name":"Series","slug":"Series","link":"/tags/Series/"},{"name":"market","slug":"market","link":"/tags/market/"},{"name":"summary","slug":"summary","link":"/tags/summary/"},{"name":"kaggle_dictation","slug":"kaggle-dictation","link":"/tags/kaggle-dictation/"},{"name":"BarGraph","slug":"BarGraph","link":"/tags/BarGraph/"},{"name":"HorizontalBar","slug":"HorizontalBar","link":"/tags/HorizontalBar/"},{"name":"Subplots","slug":"Subplots","link":"/tags/Subplots/"},{"name":"Stacked_Bar","slug":"Stacked-Bar","link":"/tags/Stacked-Bar/"},{"name":"soil","slug":"soil","link":"/tags/soil/"},{"name":"Scatter","slug":"Scatter","link":"/tags/Scatter/"},{"name":"ScatterLine","slug":"ScatterLine","link":"/tags/ScatterLine/"},{"name":"Bargraph","slug":"Bargraph","link":"/tags/Bargraph/"},{"name":"Donut_Chart","slug":"Donut-Chart","link":"/tags/Donut-Chart/"},{"name":"Treemap","slug":"Treemap","link":"/tags/Treemap/"},{"name":"pycaret","slug":"pycaret","link":"/tags/pycaret/"},{"name":"machineLeaning","slug":"machineLeaning","link":"/tags/machineLeaning/"},{"name":"auto Machine Learning","slug":"auto-Machine-Learning","link":"/tags/auto-Machine-Learning/"},{"name":"Crawling","slug":"Crawling","link":"/tags/Crawling/"},{"name":"EDA","slug":"EDA","link":"/tags/EDA/"},{"name":"sklearn","slug":"sklearn","link":"/tags/sklearn/"},{"name":"Decision Tree","slug":"Decision-Tree","link":"/tags/Decision-Tree/"},{"name":"DTS","slug":"DTS","link":"/tags/DTS/"},{"name":"HyperParameter","slug":"HyperParameter","link":"/tags/HyperParameter/"},{"name":"regression","slug":"regression","link":"/tags/regression/"},{"name":"Rss","slug":"Rss","link":"/tags/Rss/"},{"name":"prediction","slug":"prediction","link":"/tags/prediction/"},{"name":"pythonFuction","slug":"pythonFuction","link":"/tags/pythonFuction/"},{"name":"namespace","slug":"namespace","link":"/tags/namespace/"},{"name":"model selection","slug":"model-selection","link":"/tags/model-selection/"},{"name":"timer","slug":"timer","link":"/tags/timer/"}],"categories":[{"name":"naver","slug":"naver","link":"/categories/naver/"},{"name":"R","slug":"R","link":"/categories/R/"},{"name":"data_science","slug":"R/data-science","link":"/categories/R/data-science/"},{"name":"Markdown","slug":"Markdown","link":"/categories/Markdown/"},{"name":"BDS","slug":"BDS","link":"/categories/BDS/"},{"name":"DecisionTree","slug":"DecisionTree","link":"/categories/DecisionTree/"},{"name":"index","slug":"index","link":"/categories/index/"},{"name":"python","slug":"python","link":"/categories/python/"},{"name":"kaggle","slug":"kaggle","link":"/categories/kaggle/"},{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"init","slug":"init","link":"/categories/init/"},{"name":"python - plotly","slug":"python-plotly","link":"/categories/python-plotly/"},{"name":"pharmacerical_company","slug":"pharmacerical-company","link":"/categories/pharmacerical-company/"},{"name":"Matplotlib","slug":"Python/Matplotlib","link":"/categories/Python/Matplotlib/"},{"name":"Seaborn","slug":"Python/Matplotlib/Seaborn","link":"/categories/Python/Matplotlib/Seaborn/"},{"name":"machineLeaning","slug":"python/machineLeaning","link":"/categories/python/machineLeaning/"},{"name":"Crawling","slug":"python/Crawling","link":"/categories/python/Crawling/"},{"name":"pycaret","slug":"python/machineLeaning/pycaret","link":"/categories/python/machineLeaning/pycaret/"},{"name":"theory","slug":"python/theory","link":"/categories/python/theory/"},{"name":"sklearn","slug":"python/machineLeaning/sklearn","link":"/categories/python/machineLeaning/sklearn/"}]}