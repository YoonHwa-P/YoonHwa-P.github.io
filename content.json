{"pages":[],"posts":[{"title":"to prepare kaggle Competition","text":"#Kaggle Competition 준비하기 Kaggle Note 에서 작성됨. files and Library import 1234567891011121314151617181920212223import numpy as npimport pandas as pdimport seaborn as snsimport matplotlib.pylab as pltimport plotly.io as pioimport plotly.express as pximport plotly.graph_objects as goimport plotly.figure_factory as fffrom plotly.subplots import make_subplotsfrom plotly.offline import init_notebook_mode, iplotinit_notebook_mode(connected=True)pio.templates.default = &quot;none&quot;# import plotly.offline as py# py.offline.init_notebook_mode()import osfor dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename))import warningswarnings.filterwarnings(&quot;ignore&quot;) result /kaggle/input/kaggle-survey-2018/SurveySchema.csv /kaggle/input/kaggle-survey-2018/freeFormResponses.csv /kaggle/input/kaggle-survey-2018/multipleChoiceResponses.csv /kaggle/input/kaggle-survey-2017/freeformResponses.csv /kaggle/input/kaggle-survey-2017/schema.csv /kaggle/input/kaggle-survey-2017/RespondentTypeREADME.txt /kaggle/input/kaggle-survey-2017/multipleChoiceResponses.csv /kaggle/input/kaggle-survey-2017/conversionRates.csv /kaggle/input/kaggle-survey-2020/kaggle_survey_2020_responses.csv /kaggle/input/kaggle-survey-2020/supplementary_data/kaggle_survey_2020_methodology.pdf /kaggle/input/kaggle-survey-2020/supplementary_data/kaggle_survey_2020_answer_choices.pdf /kaggle/input/kaggle-survey-2021/kaggle_survey_2021_responses.csv /kaggle/input/kaggle-survey-2021/supplementary_data/kaggle_survey_2021_methodology.pdf /kaggle/input/kaggle-survey-2021/supplementary_data/kaggle_survey_2021_answer_choices.pdf /kaggle/input/kaggle-survey-2019/survey_schema.csv /kaggle/input/kaggle-survey-2019/multiple_choice_responses.csv /kaggle/input/kaggle-survey-2019/other_text_responses.csv /kaggle/input/kaggle-survey-2019/questions_only.csv dataframe create 12345df17= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2017/multipleChoiceResponses.csv&quot;, encoding=&quot;ISO-8859-1&quot;)df18= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2018/multipleChoiceResponses.csv&quot;, )df19= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2019/multiple_choice_responses.csv&quot;, )df20= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2020/kaggle_survey_2020_responses.csv&quot;, )df21= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2021/kaggle_survey_2021_responses.csv&quot;, ) data 확인하기 12345df21 = df21.iloc[1:, :]#df21.value_counts()#df21.countdf21.head() data 1개씩 표로 만들어서 불러오기 12345678910111213141516171819country = ( df21['Q3'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'country', 'Q3':'Count'}) .sort_values(by=['country'], ascending=False) ) Wage = ( df21['Q25'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'Wage', 'Q25':'Count'}) .sort_values(by=['Wage'], ascending=True) ) Wage Problem나라별 임금을 확인 하기 위해temp에 나라이름과 임금에 대해 넣었다.C_ls : country list라는 List를 만들어 나라 별로 임금을 뽑을 수 있었다. 123456# India, Russia, Chinatemp = df21[['Q3','Q25']]print(temp['Q3'].unique())C_Ls = temp['Q3'].unique() #나라이름 뽑아주기 12345tempR = temp[temp['Q3'] == str(C_Ls[0])].value_counts()# 이 경우에는 temp가 가공 할 수 없는 templet이기 때문에 plot을 그릴 수가 없다. trouble shooting12345678910111213#선생님temp= df21[['Q3','Q25']]temp = temp.groupby(['Q3','Q25']) .size() .reset_index() .rename(columns = {0:&quot;Count&quot;})C_Ls = temp['Q3'].unique()for i in range(1,len(df21['Q3'])): temp2 = temp[temp['Q3'] == str(C_Ls[i])] fig = px.bar(temp2, x='Q25', y='Count',title=C_Ls[i]) fig.show() 선생님의 도움을 받아 for문 안에 나라 이름을 넣어서 모든 나라의 임금을 확인 할 수 있었다. 모든 나라의 임금을 확인 했는데, 차별이 심한 나라를 찾기 힘들었다. 뭐때문에 이 graph를 그리려고 했는지 잃어버렸다. 한국에서 돈 잘 벌려면 어떤 직업, 어떤 … 을 해야 하는가 한국보다 평균임금이 더 많은 나라는? 한국 보다 평균임금이 더 많은 나라는 뭐가 다를까? 임금에 대한 것을 버려야 할까… 이런 재미있는 Issue에 대해 Note를 만들어 보려고 했는데 ㅎㅎ 코딩 실력이 안된당 흐흐 대회에서 잘 하려는 마음보다는 코드를 더 잘 읽고, 쓰는데 집중하자. ㅠㅠ D-17 (대회 종료까지) 대회 종료 final version","link":"/2021/11/11/kgg/KggComp_/"},{"title":"kaggle in Africa_Fig","text":"1. IntroductionHelper functions 1.1 horizontal bar graphsGraph의 code 의 경우 해당 data와 연동해서 한꺼번에 보기로 한다. &lt; plot의 종류 &gt; plotly_hBar (df, q, title, height=400,l=250,r=50,b=50,t=100,) plotly_vBar(df, q, title, l=50,r=50,b=50,t=100) head_count(df, question_num, parts): head_count function copied from df_with_percentages(df, q, n, region) plot_barH_percent(df1, df2, title, l=150, r=50, b=50, t=100) annotated_heatmap(df_w, df_a, title, width=850) categorical_scatter(df1, df2, title, l=150, r=50, b=50, t=100) annotated_heatmap_Trans(df_w, df_a, title, width=850, height=750, l=150) head_count_suf(df, question_num, part, n) df_with_percentages_suf(df, q, part, n, region) 위의 코드의 경우 쓰여지지 않은 코드도 있는 것 같지만, 일단 List UP 해 놓음. 1.2 grouping african countries1.2.1 연도 별 Africa 국가 이름 df123456africa17 = ['Nigeria','Kenya', 'South Africa', 'Egypt']africa18 = ['Nigeria','Kenya', 'South Africa', 'Egypt', 'Tunisia', 'Morocco'] africa19 = ['Nigeria','Kenya', 'South Africa', 'Egypt', 'Tunisia', 'Morocco', 'Algeria']africa20 = ['Nigeria','Kenya', 'South Africa', 'Egypt', 'Tunisia', 'Morocco', 'Ghana']africa21 = ['Nigeria','Kenya', 'South Africa', 'Egypt', 'Tunisia', 'Morocco', 'Algeria', 'Ghana', 'Uganda', 'Ethiopia'] 아마도 직접 수기로 찾은 것 같다. 1.2.2 국가 이름 확인 pd.isin(): List에 존재하는 요소가 대상 dataFrame, series에 존재 여부를 Boolean type으로 반환. 123456789101112131415161718192021africa = ['Nigeria', 'Egypt', 'South Africa', 'Algeria', 'Tunisia', 'Morocco', 'Kenya', 'Uganda', 'Ghana', 'Ethiopia']df21_africa = df21[df21['Q3'].isin(africa)]df21_world = df21[~df21['Q3'].isin(africa )]df21['region']=[&quot;Africa&quot; if x in africa else &quot;World&quot; for x in df21['Q3']]df20_africa = df20[df20['Q3'].isin(africa)]df20_world = df20[~df20['Q3'].isin(africa )]df20['region']=[&quot;Africa&quot; if x in africa else &quot;World&quot; for x in df20['Q3']]df19_africa = df19[df19['Q3'].isin(africa)]df19_world = df19[~df19['Q3'].isin(africa)]df19['region']=[&quot;Africa&quot; if x in africa else &quot;World&quot; for x in df19['Q3']]df18_africa = df18[df18['Q3'].isin(africa)]df18_world = df18[~df18['Q3'].isin(africa)]df18['region']=[&quot;Africa&quot; if x in africa else &quot;World&quot; for x in df18['Q3']]df17_africa = df17[df17['Country'].isin(africa)]df17_world = df17[~df17['Country'].isin(africa )]df17['region']=[&quot;Africa&quot; if x in africa else &quot;World&quot; for x in df17['Country']] ‘africa’라는 배열을 만들어 df를 새로 정의 17’~21’까지 같은 내용이므로 21’의 내용만으로 정리 0&gt; df21 data 확인 1&gt; df21[‘Q3’]의 내용은 당신의 나라는 어디 입니까? 2&gt; 따라서 “ df21[‘Q3’].isin(africa) “ 코드의 의미는 Q3의 대답이 africa 이면 True 반환. 3&gt; 결론적으로 Q3의 대답이 Africa[]인 모든 대답을 추출 하게 된다. 4&gt; 반대로 dfworld의 경우 ~ ( not )을 사용하여 Q3이 false인 data frame을 추출 할 수 있는것. 1.2.3 region column을 추가1df21['region']=[&quot;Africa&quot; if x in africa else &quot;World&quot; for x in df21['Q3']] df21 dataframe에 Region이라는 column 을 추가해 보자. region 컬럼에 들어갈 값은 List의 끝까지 반복하되, 만약 df21[‘Q3’]의 값이 africa에 해당하면 “Africa”, 그 밖의 경우는 world를 입력해라. data science를 잘 하려면, python 문법도 잘 알아야 할 듯.","link":"/2021/11/10/kgg/Kgg_Africa_Fig/"},{"title":"How Popular is kaggle in Africa?","text":"1. Introduction1.1 Introduction800 사용자가 사용하고 있는 글로벌 온라인 커뮤니티 케글. 194개의 나라에서 사용중. 2017년부터 머신러닝과 data 과학자들을 대상으로 설문을 했는데, 본 저자는 아프리카 나라들의 참여를 알아보고자 한다. Historical overview 이용 1.2 Table of Contents How does Africa compares with rest of the world? (Region(Q3)) 응답자 수(Africa/전체, 2021): bar-H (Region(Q3)) Africa에서 kgg사용증가 : bar Which African countries are kaggle-aware? (Q3) kgg을 사용하는 Africa나라 (2021): plotly_choroplethMap (Q3) kgg 사옹 증가 나라별 in Africa : heatmap (Q3) 조사에 참여한 나라 비율 in Africa (2021): Pie-Chart(donut) Demography : Age and Gender (Q1) age / (Q2) gender (Africa/전체, 2021): bar /bar-H (Q2, Q2, Q2, Q1, Genderselect) African 여성 비율 : bar Education, Jobs and Experience (Q4)학력/ (Q5)직업/ (Q6)경력 (Africa/전체, 2021): bar-H Programming Languages &amp; IDE’s (Q7-13)프로그래밍 언어 선호 (Africa/전체, 2021): heatmap (Q9-13)IDE (Africa/전체, 2021): bar-H (Q8)추천 프로그래밍 언어 (Africa/전체, 2021): bar-H Machine Learning: Experience, Framework and Algorithms (Q15)Muchine Learning 경력 (Africa/전체, 2021): bar-H (Q16-18)Muchine Learning 플랫폼 (Africa/전체, 2021): bar-H (Q17-12)Muchine Learning 알고리즘 (Africa/전체, 2021): bar-H (Q18-7)computer vision Methods (Africa/전체, 2021): Heatmap (Q19-6)자연언어 처리방법 (Africa/전체, 2021): Heatmap (Q14-12)visualizations Library (Africa/전체, 2021): Heatmap Computing Resources (Q11)hardware platform (Africa/전체, 2021): bar-H (Q13)TPU 사용빈도 (Africa/전체, 2021): bar-H (Q12-6)특별한 하드웨어 (Africa/전체, 2021): Heatmap (Q27_A 12)온라인 플렛폼 (Africa/전체, 2021): bar-H (Q28)클라우드 플렛폼 (Africa/전체, 2021): bar-H (29_A, 5)클라우드 만들때 쓰는 기본 resource? (Africa/전체, 2021): bar-H (32_A, 12빅데이터 만들때 (Africa/전체, 2021): bar-H (Q33)가장많이 쓰는 big data 생성 프로그램 (Africa/전체, 2021): bar-H Employment and role at work (Q20) 최근 고용주 (Africa/전체, 2021): bar-H (Q24)일할때 중요한 활동 (Africa/전체, 2021): Heatmap (Q25) 급여 피라미드 (Africa/전체, 2021): dual bar-H Learning Platform and Media (Q40) data science 배우는 플랫폼 (Africa/전체, 2021): bar-H (Q42-12) 가장 좋아하는 DS Topics 미디어 소스 (Africa/전체, 2021): Heatmap Summary Reference 특정 연도를 넣지 않은 부분은 historical data 2. Import2.1 dataFrame &amp; visualization Module1234import numpy as npimport pandas as pdimport seaborn as snsimport matplotlib.pylab as plt Numpy dataframe Pandas dataframe seabornSeaborn은 Metplotlib를 기본으로 생상테마와 통계용 차트 기능을 추가한 시각화 패키지 기본적인 시각화 기능은 Matplotlib, 통계는 Statsmodels에 의존한다. Ref. seaborn tutorial/En matplotlibmatplotlib.pyplot 모듈은 명령어 스타일로 동작하는 함수의 Library. 함수를 이용하여 그래프를 만들고, grid를 조정하고, Label도 꾸미는 등을 할 수 있다. Ref. matplotlib.pyplot 2.2 plotly1234567891011121314import plotly.io as pioimport plotly.express as pximport plotly.graph_objects as goimport plotly.figure_factory as fffrom plotly.subplots import make_subplots #특정한 차트를 만들기 위해 넣어줌from plotly.offline import init_notebook_mode, iplot #offLine에서도 돌아갈 수 있게 해줌init_notebook_mode(connected=True)pio.templates.default = &quot;none&quot;# import plotly.offline as py# py.offline.init_notebook_mode() Plotlyplotly Livrary 전체를 pio로 받아옴.그 중에서 px : plotly-express go : graph_objects ff : figure_factory px가 존재하기 이전 go로 생성하기 어려운 특정 유형의 플롯을 생성 할 수 있는 전용 함수가 있다. 를 받아 왔다. plotly에서 G를 그리는 방법은 2가지가 있는다. px : 템플릿을 통해 제작 go : 그래프를 하나하나 설정하며 제작 plotly_python plolty-tutorial-guide/Ko offline plotly offline plotly 1234567import osfor dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename))import warningswarnings.filterwarnings(&quot;ignore&quot;) data input 하는 방법.여러개의 csv file을 경로를 지정 해주어 한번에 넣어주는 code 12345df17= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2017/multipleChoiceResponses.csv&quot;, encoding=&quot;ISO-8859-1&quot;)df18= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2018/multipleChoiceResponses.csv&quot;, )df19= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2019/multiple_choice_responses.csv&quot;, )df20= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2020/kaggle_survey_2020_responses.csv&quot;, )df21= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2021/kaggle_survey_2021_responses.csv&quot;, ) 연도 별로 dataFrame을 씌워서 csv file을 dfyy객체에 Loading해 준다.","link":"/2021/11/10/kgg/Kgg_Africa/"},{"title":"Subplots in python (kaggle in East-Asia)","text":"World Vs East Asia ##python을 이용한 plotly Library로 plot 그리기 subplots 를 이용하여 다중 그래프를 그려 보자. python Library Import1234567891011121314151617181920212223import numpy as npimport pandas as pdimport seaborn as snsimport matplotlib.pylab as pltimport plotly.io as pioimport plotly.express as pximport plotly.graph_objects as goimport plotly.figure_factory as fffrom plotly.subplots import make_subplotsfrom plotly.offline import init_notebook_mode, iplotinit_notebook_mode(connected=True)pio.templates.default = &quot;none&quot;# import plotly.offline as py# py.offline.init_notebook_mode()import osfor dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename))import warningswarnings.filterwarnings(&quot;ignore&quot;) data import123456df17= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2017/multipleChoiceResponses.csv&quot;, encoding=&quot;ISO-8859-1&quot;)df18= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2018/multipleChoiceResponses.csv&quot;, )df19= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2019/multiple_choice_responses.csv&quot;, )df20= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2020/kaggle_survey_2020_responses.csv&quot;, )df21= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2021/kaggle_survey_2021_responses.csv&quot;, ) data frame 전처리123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354total17 = ( df17['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index() )total18 = ( df18['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index() )total19 = ( df19['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index() )total20 = ( df20['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index() )total21 = ( df21['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index() ) plot 그리기make_subplots 로 여러개의 그래프를 한 plot에 담아 봅시다. pie그래프를 연도별로 담아 봅시다. 1234567891011121314151617181920212223242526272829303132333435# Create subplots: use 'domain' type for Pie subplot##subplots가 담길 행렬을 만들어 줍니다. rows와 cols를 맞춰서 제목도 달아 줍니다. fig = make_subplots(rows=1, cols=5, specs=[[{'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}]], subplot_titles=(&quot;2017&quot;, &quot;2018&quot;, &quot;2019&quot;, &quot;2020&quot;, &quot;2021&quot;))## scalegroup = one으로 설정 하게 되면, data의 크기에 따라 pie의 크기가 결정됩니다. fig.add_trace(go.Pie(labels=total21['type'], values=total21['respodents'], name=&quot;2021&quot;, scalegroup='one'), 1, 1)fig.add_trace(go.Pie(labels=total20['type'], values=total20['respodents'], name=&quot;2020&quot;, scalegroup='one'), 1, 2)fig.add_trace(go.Pie(labels=total19['type'], values=total19['respodents'], name=&quot;2019&quot;, scalegroup='one'), 1, 3)fig.add_trace(go.Pie(labels=total18['type'], values=total18['respodents'], name=&quot;2018&quot;, scalegroup='one'), 1, 4)fig.add_trace(go.Pie(labels=total17['type'], values=total17['respodents'], name=&quot;2017&quot;, scalegroup='one'), 1, 5)# Use `hole` to create a donut-like pie chartfig.update_traces(hole=.2, hoverinfo=&quot;label+percent+name&quot;)fig.update_layout( title_text=&quot;&lt;b&gt;World vs EastAsia&lt;/b&gt;&quot;, # Add annotations in the center of the donut pies. )fig.show()","link":"/2021/11/15/kgg/Kgg_EastAsia_KggHistorical/"},{"title":"kaggle in Africa_barH(1-1)","text":"1. Figure Helper functionsKgg_Africa 에서는 python 문법 중에서 함수를 만드는 def 을 이용하여 plot들을 정의 해 놓았다. def 함수명(매개변수): &lt;수행할 문장1&gt; &lt;수행할 문장2&gt; ... ref. python_Function/Ko. 1.1 horizontal bar graphs다음 results plot 을 뜯어보며 bar-H를 해석 해 보자. How does Africa compares with rest of the world? (Region(Q3)) 응답자 수(Africa/전체, 2021): bar-H 먼저, hBar는 다음과 같이 정의 되었다. 그동암 bar-H에대한 많은 부분을 공부 했으므로 간단히 함수를 중심으로 뜯어 보자. plotly.express.histogram 1234567891011121314151617181920212223242526272829303132333435363738394041424344def plotly_hBar(df, q, title, height=400,l=250,r=50,b=50,t=100,): fig = px.histogram(df.iloc[1:], y=q, orientation='h', width=700, height=height, histnorm='percent', color='region', color_discrete_map={ &quot;Africa&quot;: &quot;gold&quot;, &quot;World&quot;: &quot;salmon&quot; }, opacity=0.6 ) fig.update_layout(title=title, font_family=&quot;San Serif&quot;, bargap=0.2, barmode='group', titlefont={'size': 28}, paper_bgcolor='#F5F5F5', plot_bgcolor='#F5F5F5', legend=dict( orientation=&quot;v&quot;, y=1, yanchor=&quot;top&quot;, x=1.250, xanchor=&quot;right&quot;,) ).update_yaxes(categoryorder='total ascending') fig.update_traces(marker_line_color='black', marker_line_width=1.5) fig.update_layout(yaxis_title=None,yaxis_linewidth=2.5, autosize=False, margin=dict( l=l, r=r, b=b, t=t, ), ) fig.update_xaxes(showgrid=False) fig.update_yaxes(showgrid=False) fig.show() def plotly_hBar(df, q, title, height=400,l=250,r=50,b=50,t=100,) 함수 plotly_hBar의 정의 df, q, title등의 변수를 선언하고 값을 정해줌. fig 정의123456789101112fig = px.histogram(df.iloc[1:], y=q, orientation='h', width=700, height=height, histnorm='percent', color='region', color_discrete_map={ &quot;Africa&quot;: &quot;gold&quot;, &quot;World&quot;: &quot;salmon&quot; }, opacity=0.6 ) plotly.express.histogram() plotly의 express Library를 이용하여 histogram을 그려본다. df.iloc[1:] dataframe으로 iloc을 이용하여 컬럼을 가져옴 1행에서부터 끝까지 y= q, 나중에 q변수만 정해서 넣어주면 G가 그려진다. orientation= ‘h’, orientation이 h일땐, x orientation이 v일땐, y 를 하라고 공식문서에 써있는데 왜 얘는 이랬는지 알 수 없음 + Histogram plot ???. height = ‘height’, plot의 높이 지정 height=400이라고 함수 정의때 이미 지정 됨. color = ‘region’, 색은 region이라는 변수가 어떤것이냐에 따라 달라짐 color_discrete_map={“Africa”: “gold”, “World”: “salmon”}, dictionary처럼 Indexing 해 줌. opacity = 0.6 불 투명함의 정도 (0~1, flot) color_discrete_map 과 color_discrete_sequence 의 차이 dict with str keys and str values (default {}) , (list of str) plotly.express fig.update_layout()1234567891011121314fig.update_layout(title=title, font_family=&quot;San Serif&quot;, bargap=0.2, barmode='group', titlefont={'size': 28}, paper_bgcolor='#F5F5F5', plot_bgcolor='#F5F5F5', legend=dict( orientation=&quot;v&quot;, y=1, yanchor=&quot;top&quot;, x=1.250, xanchor=&quot;right&quot;,) ).update_yaxes(categoryorder='total ascending') fig.update_layout() :","link":"/2021/11/10/kgg/Kgg_Africa_bar-H/"},{"title":"Stacked Bar (kaggle in East-Asia)","text":"World Vs East Asia ##python을 이용한 plotly Library로 plot 그리기 subplots 를 이용하여 다중 그래프를 그려 보자. python Library Import1234567891011121314151617181920212223import numpy as npimport pandas as pdimport seaborn as snsimport matplotlib.pylab as pltimport plotly.io as pioimport plotly.express as pximport plotly.graph_objects as goimport plotly.figure_factory as fffrom plotly.subplots import make_subplotsfrom plotly.offline import init_notebook_mode, iplotinit_notebook_mode(connected=True)pio.templates.default = &quot;none&quot;# import plotly.offline as py# py.offline.init_notebook_mode()import osfor dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename))import warningswarnings.filterwarnings(&quot;ignore&quot;) data import123456df17= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2017/multipleChoiceResponses.csv&quot;, encoding=&quot;ISO-8859-1&quot;)df18= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2018/multipleChoiceResponses.csv&quot;, )df19= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2019/multiple_choice_responses.csv&quot;, )df20= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2020/kaggle_survey_2020_responses.csv&quot;, )df21= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2021/kaggle_survey_2021_responses.csv&quot;, ) data frame 전처리i) Q3를 기준으로 EastAsia에 속하는 나라만 연도별로 뽑아냅니다. 1234567891011121314151617181920212223242526272829303132df21_Ea=df21[df21['Q3'].isin(EastAsia21)]Ea21= ( df21_Ea['Q3'].value_counts().to_frame() .reset_index().rename(columns={'index':'Country', 'Q3':'21'}))df20_Ea=df20[df20['Q3'].isin(EastAsia)]Ea20= ( df20_Ea['Q3'].replace('Republic of Korea','South Korea') .value_counts().to_frame().reset_index() .rename(columns={'index':'Country', 'Q3':'20'}))df19_Ea=df19[df19['Q3'].isin(EastAsia)]Ea19= (df19_Ea['Q3'].replace('Republic of Korea','South Korea') .value_counts().to_frame().reset_index() .rename(columns={'index':'Country', 'Q3':'19'}))df18_Ea=df18[df18['Q3'].isin(EastAsia)]Ea18= (df18_Ea['Q3'].replace('Republic of Korea','South Korea') .value_counts().to_frame().reset_index() .rename(columns={'index':'Country', 'Q3':'18'}))Ea18.value_counts()#df18 열에 taiwan = 0을 추가 해야 합니다. df17_Ea = df17[df17['Country'].isin(EastAsia)]Ea17= (df17_Ea['Country'].replace(&quot;People 's Republic of China&quot;,'China') .value_counts().to_frame().reset_index() .rename(columns={'index':'Country', 'Country':'17'})) ii) data를 합쳐서 하나의 dataframe으로 만들음. 이 과정에서 pd.merge()를 사용 해 주었기 때문에 18’ taiwan data가 Nan으로 추가 되었다. 123456789fig = go.Figure(data=[ go.Bar(name='2017', x=df5years['Country'], y=df5years['17']), go.Bar(name='2018', x=df5years['Country'], y=df5years['18']), go.Bar(name='2019', x=df5years['Country'], y=df5years['19']), go.Bar(name='2020', x=df5years['Country'], y=df5years['20']), go.Bar(name='2021', x=df5years['Country'], y=df5years['21'])])fig.show() 123#Change the bar modefig.update_layout(barmode='stack', title='연도별 동아시아 Kaggle 사용자수' ) stacked bar로 할까말까 고민중. dictation 할 때 까지만 해도 bar 그래프 그리는 것이 뭐그리 어렵겠나? 했다. 그냥 복사 붙여넣기로 만드려고 했는데 그게 참 안되네 ㅂㄷㅂㄷ 123456789101112131415df5years_ =df5years.transpose()df5years_= df5years_.iloc[1:]fig2 = go.Figure(data=[ go.Bar(name='China', x=years, y=df5years_[0]), go.Bar(name='Japan', x=years, y=df5years_[1]), go.Bar(name='Taiwan', x=years, y=df5years_[2]), go.Bar(name='South Korea', x=years, y=df5years_[3]),])fig2.show() 축 reverse로 할까말까 고민중. 어떤게 더 잘 보여 줄 수 있을까 … ㅜㅜ","link":"/2021/11/15/kgg/Kgg_EastAsia_stackedBar/"},{"title":"kaggle HeatMap","text":"HeatMappython문법의 plotly Library를 이용하여 Heatmap을 알아보자 HeatMap의 Gradation 색을 바꾸고 싶다면, colorscales 을 참고 해 보자. HeatMap 112345678import plotly.figure_factory as ffz=[[1, 90, 30, 50, 1], [20, 1, 60, 80, 30], [30, 60, 1, 50, 20]]x=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']y=['Morning', 'Afternoon', 'Evening']fig = ff.create_annotated_heatmap(z, x = x, y = y, colorscale = &quot;Viridis&quot;)fig.show() Input data를 맞춰 줘야한다. x, y = List z= 배열 HeatMap 21234567891011121314151617181920212223242526272829import plotly.graph_objects as gofrom functools import reducefrom itertools import productz=[[1, 90, 30, 50, 1], [20, 1, 60, 80, 30], [30, 60, 1, 50, 20]]x=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']y=['Morning', 'Afternoon', 'Evening']def get_anno_text(z_value): annotations=[] a, b = len(z_value), len(z_value[0]) flat_z = reduce(lambda x,y: x+y, z_value) # z_value.flat if you deal with numpy coords = product(range(a), range(b)) for pos, elem in zip(coords, flat_z): annotations.append({'font': {'color': '#FFFFFF'}, 'showarrow': False, 'text': str(elem), 'x': pos[1], 'y': pos[0]}) return annotationsfig = go.Figure(data=go.Heatmap( z=z, x=x, y=y, hoverongaps = True))fig.update_layout(annotations = get_anno_text(z))fig.show() HeatMap 31234567891011121314151617181920212223import plotly.graph_objects as goz = df.groupby(['Q4', 'Q1']).size().unstack().fillna(0).astype('int16')# convert to correlation matrixz2 = z.apply(lambda x:x/x.sum(), axis = 1)x = z2.columnsy = z2.indexfig = go.Figure(data=go.Heatmap( z=z2.to_numpy(), #dataframe을 넘파이(배열)로 바꿔줌: List형태 x=x, y=y, type=&quot;heatmap&quot;, colorscale = &quot;Viridis&quot;, hoverongaps = False))fig.update_layout( title='Degree ~ Gender', xaxis_nticks=36)fig.show() z2 = z.apply(lambda x:x/x.sum(), axis = 1) 여기서 이 한줄로 인해 dataFrame이 Heatmap에 들어 갈 수 있는 상관관계G를 만들 수 있는 상태로 변환. data 자료형을 맞춰줘야 한다. (x, y, z) HeatMap Ref.Ref.","link":"/2021/11/16/kgg/Kgg_HeatMap/"},{"title":"kaggle in Korea(data둘러보기)","text":"#How popular is kaggle in South Korea? data 정제하기123456789101112131415161718192021222324df21_Ko = df21[df21['Q3'] == 'South Korea']df21_Wo = df21[~(df21['Q3'] == 'South Korea')]df21['region']=[&quot;Korea&quot; if x == 'South Korea' else &quot;World&quot; for x in df21['Q3']]df21['region'].value_counts()df20_Ko = df20[df20['Q3'] == 'South Korea']df20_Wo = df20[~(df20['Q3'] == 'South Korea')]df20['region']=[&quot;Korea&quot; if x == 'South Korea' else &quot;World&quot; for x in df20['Q3']]df20['region'].value_counts()df19_Ko = df19[df19['Q3'] == 'South Korea']df19_Wo = df19[~(df19['Q3'] == 'South Korea')]df19['region']=[&quot;Korea&quot; if x == 'South Korea' else &quot;World&quot; for x in df19['Q3']]df19['region'].value_counts()df18_Ko = df18[df18['Q3'] == 'South Korea']df18_Wo = df18[~(df18['Q3'] == 'South Korea')]df18['region']=[&quot;Korea&quot; if x == 'South Korea' else &quot;World&quot; for x in df18['Q3']]df18['region'].value_counts()df17_Ko = df17[df17['Country'] == 'South Korea']df17_Wo = df17[~(df17['Country'] == 'South Korea')]df17['region']=[&quot;Korea&quot; if x == 'South Korea' else &quot;World&quot; for x in df17['Country']]df17['region'].value_counts() &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD World 25615Korea 359Name: region, dtype: int64 World 19847Korea 190Name: region, dtype: int64 World 19536Korea 182Name: region, dtype: int64 World 23672Korea 188Name: region, dtype: int64 World 16522Korea 194Name: region, dtype: int64======= 2021 World 25615 Korea 359 Name: region, dtype: int64 2020 World 19847 Korea 190 Name: region, dtype: int64 2019 World 19536 Korea 182 Name: region, dtype: int64 2018 World 23672 Korea 188 Name: region, dtype: int64 2017 World 16522 Korea 194 Name: region, dtype: int64 trouble shootingdata 정제를 하다 보니 전체 data에서 korea가 1% 밖에 되지 않아 data set을 더 추가 하기로 했다. ##동아시아 Ref. East Asia 동아시아 East Asia에는 대한민국, 일본, 중국, 타이완, 몽골, 북조선 총 6개의 국가가 속해 있다. 알 수 없지만, 18년도엔 타이완이 없다. 12345678910111213141516171819202122232425262728293031323334## East Asia에는 대한민국, 일본, 중국, 타이완, 몽골, 북조선 총 6개의 국가가 속해 있다. ## 알 수 없지만, 18년도엔 타이완이 없다. EastAsia17 = ['China',&quot;People 's Republic of China&quot;, 'Taiwan', 'South Korea', 'Japan']EastAsia18= ['China', 'South Korea', 'Japan', 'Republic of Korea'] EastAsia19 = ['China','Taiwan', 'South Korea', 'Japan', 'Republic of Korea']EastAsia20 = ['China','Taiwan', 'South Korea','Republic of Korea', 'Japan']EastAsia21 = ['China','Taiwan', 'South Korea', 'Japan']EastAsia = ['Republic of Korea','China','Taiwan', 'South Korea', 'Japan', &quot;People 's Republic of China&quot; ]df21_Ea = df21[df21['Q3'].isin(EastAsia)]df21_Wo = df21[~df21['Q3'].isin(EastAsia )]df21['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df21['Q3']]df20_Ea = df20[df20['Q3'].isin(EastAsia)]df20_Wo = df20[~df20['Q3'].isin(EastAsia )]df20['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df20['Q3']]df19_Ea = df19[df19['Q3'].isin(EastAsia)]df19_Wo = df19[~df19['Q3'].isin(EastAsia )]df19['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df19['Q3']]df18_Ea = df18[df18['Q3'].isin(EastAsia)]df18_Wo = df18[~df18['Q3'].isin(EastAsia )]df18['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df18['Q3']]df17_Ea = df17[df17['Country'].isin(EastAsia)]df17_Wo = df17[~df17['Country'].isin(EastAsia )]df17['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df17['Country']]#df21['region'].to_frame().value_counts().to_frame().rename(columns={'region': '21y', '' : 'count'}) 21년도 를 .value_counts()로 뽑아 냈다. 1%대는 아니지만, 이제 10%대 data를 뽑아 냈다. 이것이 어떤 의미가 있을지 모르겠지만, 일단 주말동안 이 data로 궁금한 것을 Graph로 만들어 보자. 12345678910111213141516171819202122232425262728293031323334353637383940# % 계산을 위해 len() 을 통해 data 생성.Ea21 = len(df21_Ea)Wo21 = len(df21) - len(df21_Ea)Ea20 = len(df20_Ea)Wo20 = len(df20) - len(df20_Ea)Ea19 = len(df19_Ea)Wo19 = len(df19) - len(df19_Ea)Ea18 = len(df18_Ea)Wo18 = len(df18) - len(df18_Ea)Ea17 = len(df17_Ea)Wo17 = len(df17) - len(df17_Ea)def percent (a, b): result =a/(a+b)*100 return resultdef percentR (b, a): result =a/(a+b)*100 return resultcountry = ['East Asia', 'Rest of the World']years = ['2017', '2018', '2019', '2020', '2021']fig = go.Figure(data=[ go.Bar(name='Rest of the World', x=years, y=[percentR(Ea17, Wo17), percentR(Ea18, Wo18), percentR(Ea19, Wo19), percentR(Ea20, Wo20), percentR(Ea21, Wo21)]), go.Bar(name='East Asia', x=years, y=[percent(Ea17, Wo17), percent(Ea18, Wo18), percent(Ea19, Wo19), percent(Ea20, Wo20), percent(Ea21, Wo21)])])fig.update_layout(barmode='stack')fig.show() 일단, plot은 뽑아 보았는데 이래도 되나 싶다 ^^ 하하 노동력을 더해서 대륙 별로 뽑던지 해야겠다 ㅂㄷㅂㄷ 072bc76c34c0f369e5fe7e814291408da6a83ffc","link":"/2021/11/12/kgg/Kgg_Korea/"},{"title":"[object Object]","text":"#Plotly Tutorial For Kaggle Survey Competitions 진도가 너무 더디게 나가서 Teacher’s code를 조금더 뜯어 본후 python for문이나 if문을 조금 더 잘 쓸 수 있을기를 바란다.","link":"/2021/11/15/kgg/Kgg_Teacher/"},{"title":"kaggle in East-Asia(data둘러보기)","text":"Kaggle in East Asia 0. introduction1. World Vs East Asia Kgg 응답자 수 Kgg 응답자 중 직업1. 2. East Asia3. interestings Tenure: 17y 경력? Q6(20y) Q8(18y) Q15(19y) Q6(20, 21y) FormalEducation: 17y 학력, Q4 in 18y, 19y, 20y 전공 Q5 in18y compensation for year: Q10 in 18y,19y, Q24 in 20, 1234567891011121314151617181920212223242526272829# 연도별 나이 df21Age_Ea = df21_Ea.loc[:,['Q3','Q1']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'2021'}).fillna('etc')df20Age_Ea = df20_Ea.loc[:,['Q3','Q1']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'2020'}).fillna('etc')df19Age_Ea = df19_Ea.loc[:,['Q3','Q1']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'2019'}).fillna('etc')df18Age_Ea = df18_Ea.loc[:,['Q3','Q2']].reset_index().rename(columns={'Q3':'East_Asia', 'Q2':'2018'}).fillna('etc')df17Age_Ea = df17_Ea.loc[:,['Country','Age']].reset_index().rename(columns={'Country':'East_Asia', 'Age':'2017'}).fillna('etc')#data frame 정리dfAge21_percent =df21Age_Ea.groupby(['East_Asia','2021']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfAge21_percent['percent'] =((dfAge21_percent['Count'] / len(df21Age_Ea))*100).round(2)dfAge21_percent['percent_str'] =((dfAge21_percent['Count'] / len(df21Age_Ea))*100).round(2).astype(str) + '%'# dfAge21['percent'] = ((media['Count'] / len(df))*100).round(2).astype(str) + '%'# dfAge_percent21=dfAge21.value_counts('East_Asia',normalize=True).mul(100).round(1).astype(str)dfAge21_percent# 나라별 연령대 비율 in East Asiafig = go.Figure()for country, group in dfAge21_percent.groupby('East_Asia'): fig.add_trace(go.Bar( x = group['2021'], y = group['percent'], name = country, text=group['percent_str'] ))fig.update_layout(barmode=&quot;stack&quot;, plot_bgcolor = &quot;white&quot;, title='2021_ 연령별 나라 비율 in East Asia')fig.show() 123456789101112# 연령별 나라 비율 in East Asia // 여기 name 어떻게 넣는거야 !!!! fig = go.Figure()for country, group in dfAge21_percent.groupby('2021'): fig.add_trace(go.Bar( x = group['East_Asia'], y =group['percent'], text=dfAge21_percent['percent_str'] ))fig.update_layout(barmode=&quot;stack&quot;, plot_bgcolor = &quot;white&quot;, title='2021_ 나라별 연령 비율 in East Asia')fig.show() 못해먹겠다. Metaplotly","link":"/2021/11/15/kgg/Kgg_in_EastAsia/"},{"title":"kaggle HeatMap","text":"Kaggle 을 쓰는 East Asia의 사람들 Kaggle 정의 Kaggle 설문조사_개요 그런데 우리는 EA에살아서 궁긍한 걸 찾아보자. data import1234567891011121314151617181920212223import numpy as npimport pandas as pdimport seaborn as snsimport matplotlib.pylab as pltimport plotly.io as pioimport plotly.express as pximport plotly.graph_objects as goimport plotly.figure_factory as fffrom plotly.subplots import make_subplotsfrom plotly.offline import init_notebook_mode, iplotinit_notebook_mode(connected=True)pio.templates.default = &quot;none&quot;# import plotly.offline as py# py.offline.init_notebook_mode()import osfor dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename))import warningswarnings.filterwarnings(&quot;ignore&quot;) /kaggle/input/kaggle-survey-2018/SurveySchema.csv /kaggle/input/kaggle-survey-2018/freeFormResponses.csv /kaggle/input/kaggle-survey-2018/multipleChoiceResponses.csv /kaggle/input/kaggle-survey-2020/kaggle_survey_2020_responses.csv /kaggle/input/kaggle-survey-2020/supplementary_data/kaggle_survey_2020_methodology.pdf /kaggle/input/kaggle-survey-2020/supplementary_data/kaggle_survey_2020_answer_choices.pdf /kaggle/input/kaggle-survey-2021/kaggle_survey_2021_responses.csv /kaggle/input/kaggle-survey-2021/supplementary_data/kaggle_survey_2021_methodology.pdf /kaggle/input/kaggle-survey-2021/supplementary_data/kaggle_survey_2021_answer_choices.pdf /kaggle/input/kaggle-survey-2019/survey_schema.csv /kaggle/input/kaggle-survey-2019/multiple_choice_responses.csv /kaggle/input/kaggle-survey-2019/other_text_responses.csv /kaggle/input/kaggle-survey-2019/questions_only.csv /kaggle/input/kaggle-survey-2017/freeformResponses.csv /kaggle/input/kaggle-survey-2017/schema.csv /kaggle/input/kaggle-survey-2017/RespondentTypeREADME.txt /kaggle/input/kaggle-survey-2017/multipleChoiceResponses.csv /kaggle/input/kaggle-survey-2017/conversionRates.csv 123456df17= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2017/multipleChoiceResponses.csv&quot;, encoding=&quot;ISO-8859-1&quot;)df18= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2018/multipleChoiceResponses.csv&quot;, )df19= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2019/multiple_choice_responses.csv&quot;, )df20= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2020/kaggle_survey_2020_responses.csv&quot;, )df21= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2021/kaggle_survey_2021_responses.csv&quot;, ) EastAsia VS World1. data 전처리1.1 EastAsia / World 나누기123456789101112131415161718192021222324252627282930313233343536373839404142434445#질문 제거하기, replacedf17= df17.iloc[1:, :].replace(&quot;People 's Republic of China&quot;,'China')df18= df18.iloc[1:, :].replace('Republic of Korea','South Korea')df19= df19.iloc[1:, :].replace('Republic of Korea','South Korea')df20= df20.iloc[1:, :].replace('Republic of Korea','South Korea')df21= df21.iloc[1:, :]## East Asia에는 대한민국, 일본, 중국, 타이완, 몽골, 북조선 총 6개의 국가가 속해 있다. ## 알 수 없지만, 18년도엔 타이완이 없다. EastAsia17 = ['China',&quot;People 's Republic of China&quot;, 'Taiwan', 'South Korea', 'Japan']EastAsia18= ['China', 'South Korea', 'Japan', 'Republic of Korea'] EastAsia19 = ['China','Taiwan', 'South Korea', 'Japan', 'Republic of Korea']EastAsia20 = ['China','Taiwan', 'South Korea','Republic of Korea', 'Japan']EastAsia21 = ['China','Taiwan', 'South Korea', 'Japan']EastAsia = ['Republic of Korea','China','Taiwan', 'South Korea', 'Japan', &quot;People 's Republic of China&quot; ]years = ['2017', '2018', '2019', '2020', '2021']#East Asia 뽑기df21_Ea = df21[df21['Q3'].isin(EastAsia)]df21_Wo = df21[~df21['Q3'].isin(EastAsia)]df21['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df21['Q3']]df20_Ea = df20[df20['Q3'].isin(EastAsia)]df20_Wo = df20[~df20['Q3'].isin(EastAsia)]df20['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df20['Q3']]df19_Ea = df19[df19['Q3'].isin(EastAsia)]df19_Wo = df19[~df19['Q3'].isin(EastAsia)]df19['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df19['Q3']]df18_Ea = df18[df18['Q3'].isin(EastAsia)]df18_Wo = df18[~df18['Q3'].isin(EastAsia)]df18['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df18['Q3']]df17_Ea = df17[df17['Country'].isin(EastAsia)]df17_Wo = df17[~df17['Country'].isin(EastAsia)]df17['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df17['Country']]#df21['region'].to_frame().value_counts().to_frame().rename(columns={'region': '21y', '' : 'count'})print('OK') OK 123456789101112131415161718192021222324# 나라별 data 뽑기df21_Ch= df21_Ea[df21_Ea['Q3'] == 'China']df21_Tw= df21_Ea[df21_Ea['Q3'] == 'South Korea']df21_Ko= df21_Ea[df21_Ea['Q3'] == 'Taiwan']df21_Jp= df21_Ea[df21_Ea['Q3'] == 'Japan']df20_Ch= df20_Ea[df20_Ea['Q3'] == 'China']df20_Tw= df20_Ea[df20_Ea['Q3'] == 'South Korea']df20_Ko= df20_Ea[df20_Ea['Q3'] == 'Taiwan']df20_Jp= df20_Ea[df20_Ea['Q3'] == 'Japan']df19_Ch= df19_Ea[df19_Ea['Q3'] == 'China']df19_Tw= df19_Ea[df19_Ea['Q3'] == 'South Korea']df19_Ko= df19_Ea[df19_Ea['Q3'] == 'Taiwan']df19_Jp= df19_Ea[df19_Ea['Q3'] == 'Japan']df18_Ch= df18_Ea[df18_Ea['Q3'] == 'China']df18_Tw= df18_Ea[df18_Ea['Q3'] == 'South Korea']df18_Jp= df18_Ea[df18_Ea['Q3'] == 'Japan']df17_Ch= df17_Ea[df17_Ea['Country'] == 'China']df17_Tw= df17_Ea[df17_Ea['Country'] == 'South Korea']df17_Ko= df17_Ea[df17_Ea['Country'] == 'Taiwan']df17_Jp= df17_Ea[df17_Ea['Country'] == 'Japan'] 2. Kaggle 사용자수 (W/Ea)2.1 data 전처리2.2 그래프 그리기1234567891011121314151617181920212223242526272829303132333435def world_map(locations,counts,title): data = [ dict( type = 'choropleth', locations = locations, z = counts, colorscale = 'Blues', locationmode = 'country names', autocolorscale = False, reversescale = True, marker = dict( line = dict(color = '#F7F7F7', width = 1.5)), colorbar = dict(autotick = True, legth = 3, len=0.75, title = 'respodents', max = 1000, min = 0) ) ] layout = dict( title = title, titlefont={'size': 28, 'family': 'san serif'}, width=750, height=475, paper_bgcolor='#F7F7F7', geo = dict( showframe = True, showcoastlines = True, fitbounds=&quot;locations&quot;, ) ) fig = dict(data=data, layout=layout) iplot(fig, validate=False, filename='world-map') z = df21_Ea['Q3'].value_counts() ## 메서드 호출world_map(locations=z.index, counts=z.values, title= '&lt;b&gt; EastAsia Countries (2021 survey) &lt;b&gt;') 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# 수치 bar g = 사용자 수 비교.Ea21 = len(df21_Ea)Wo21 = len(df21) - len(df21_Ea)Ea20 = len(df20_Ea)Wo20 = len(df20) - len(df20_Ea)Ea19 = len(df19_Ea)Wo19 = len(df19) - len(df19_Ea)Ea18 = len(df18_Ea)Wo18 = len(df18) - len(df18_Ea)Ea17 = len(df17_Ea)Wo17 = len(df17) - len(df17_Ea)years = ['2017','2018','2019','2020', '2021']def percent (a, b): result =a/(a+b)*100 result = np.round(result) return resultdef percentR (b, a): result =a/(a+b)*100 result = np.round(result) return resultpercent = [percent(Ea17, Wo17), percent(Ea18, Wo18), percent(Ea19, Wo19), percent(Ea20, Wo20), percent(Ea21, Wo21)]# percentR = [percentR(Ea17, Wo17), percentR(Ea18, Wo18), percentR(Ea19, Wo19), # percentR(Ea20, Wo20), percentR(Ea21, Wo21)]fig = go.Figure()fig.add_trace(go.Bar(x=years, y=[len(df17), len(df18), len(df19), len(df20), len(df21)], base=[-len(df17), -len(df18), -len(df19), -len(df20), -len(df21)], marker_color='lightslategrey', name='World', textposition='outside', hovertemplate='&lt;b&gt;KaggleUser&lt;/b&gt;: %{x}&lt;br&gt;'+ '&lt;b&gt;Count&lt;/b&gt;: %{y}' ))fig.add_trace(go.Bar(x=years, y=[Ea17, Ea18, Ea19, Ea20, Ea21], base=0, marker_color='pink', name='East Asia', text= percent, texttemplate='%{text} %', textposition='outside', hovertemplate='&lt;b&gt;KaggleUser&lt;/b&gt;: %{x}&lt;br&gt;'+ '&lt;b&gt;Count&lt;/b&gt;: %{y}', textfont_size=12 ))fig.update_layout(width=600, height=700)fig.show() 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677from plotly.subplots import make_subplotsimport plotly.graph_objects as gototal17 = ( df17['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index())total18 = ( df18['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index())total19 = ( df19['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index())total20 = ( df20['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index())total21 = ( df21['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index())# Create subplots: use 'domain' type for Pie subplotfig = make_subplots(rows=1, cols=5, specs=[[{'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}]], subplot_titles=(&quot;2017&quot;, &quot;2018&quot;, &quot;2019&quot;, &quot;2020&quot;, &quot;2021&quot;))fig.add_trace(go.Pie(labels=total21['type'], values=total21['respodents'], name=&quot;2021&quot;, scalegroup='one'), 1, 1)fig.add_trace(go.Pie(labels=total20['type'], values=total20['respodents'], name=&quot;2020&quot;, scalegroup='one'), 1, 2)fig.add_trace(go.Pie(labels=total19['type'], values=total19['respodents'], name=&quot;2019&quot;, scalegroup='one'), 1, 3)fig.add_trace(go.Pie(labels=total18['type'], values=total18['respodents'], name=&quot;2018&quot;, scalegroup='one'), 1, 4)fig.add_trace(go.Pie(labels=total17['type'], values=total17['respodents'], name=&quot;2017&quot;, scalegroup='one'), 1, 5)# Use `hole` to create a donut-like pie chartfig.update_traces(hole=.2, hoverinfo=&quot;label+percent+name&quot;)fig.update_layout( title_text=&quot;&lt;b&gt;World vs EastAsia&lt;/b&gt;&quot;, # Add annotations in the center of the donut pies. )fig.show() 가설은 시간에따라서 응답자수가 증가 할 줄 알았는데 결과를 보니 오히려 감소하는 경향을 볼 수 있다. East Asia 응답자는 2020년도(10.5%)에 가장 많았고 2121년도(7.15%)로 가장 적엇다. 0. Kaggle Gender (in Ea)0.1 data 전처리0.2 그래프 그리기123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263df21_Ea=df21[df21['Q3'].isin(EastAsia21)]Ea21= ( df21_Ea['Q3'].value_counts().to_frame() .reset_index().rename(columns={'index':'Country', 'Q3':'21'}))df20_Ea=df20[df20['Q3'].isin(EastAsia)]Ea20= ( df20_Ea['Q3'].replace('Republic of Korea','South Korea') .value_counts().to_frame().reset_index() .rename(columns={'index':'Country', 'Q3':'20'}))df19_Ea=df19[df19['Q3'].isin(EastAsia)]Ea19= (df19_Ea['Q3'].replace('Republic of Korea','South Korea') .value_counts().to_frame().reset_index() .rename(columns={'index':'Country', 'Q3':'19'}))df18_Ea=df18[df18['Q3'].isin(EastAsia)]Ea18= (df18_Ea['Q3'].replace('Republic of Korea','South Korea') .value_counts().to_frame().reset_index() .rename(columns={'index':'Country', 'Q3':'18'}))Ea18.value_counts()#df18 열에 taiwan = 0을 추가 해야 합니다. df17_Ea = df17[df17['Country'].isin(EastAsia)]Ea17= (df17_Ea['Country'].replace(&quot;People 's Republic of China&quot;,'China') .value_counts().to_frame().reset_index() .rename(columns={'index':'Country', 'Country':'17'}))#data를 합쳐서 하나의 dataframe으로 만들어 줌.df5years = pd.merge(Ea17, Ea18, on='Country', how='outer')df5year =pd.merge(Ea19,Ea20, on='Country', how='outer')df5year=pd.merge(df5year, Ea21, on='Country', how='outer')df5years = pd.merge(df5years, df5year, on='Country', how='outer')fig = go.Figure(data=[ go.Bar(name='2017', x=df5years['Country'], y=df5years['17']), go.Bar(name='2018', x=df5years['Country'], y=df5years['18']), go.Bar(name='2019', x=df5years['Country'], y=df5years['19']), go.Bar(name='2020', x=df5years['Country'], y=df5years['20']), go.Bar(name='2021', x=df5years['Country'], y=df5years['21'])])#Change the bar modefig.update_layout(barmode='stack', title='연도별 동아시아 Kaggle 사용자수' )fig.show()# Text : percent 5년간 china의 Kaggle 응답자 수가 가장 많았다. taiwan이 가장 적지만, 2018년도에 어떤이유인지 모르겠지만, china로 결과 값이 흡수 된 것 같다. south korea나 Taiwan의 응답자수는 china의 절반에 미치지 못한다. 인구수 대비 몇 % 가 많다. 3. Kaggle Gender (W/Ea)3.1 data 전처리3.2 그래프 그리기12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485Gender_17 = ( df17['GenderSelect'] .replace(['A different identity', 'Prefer to self-describe', 'Non-binary, genderqueer, or gender non-conforming'], 'Others') .fillna('Others') .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'GenderSelect':'Gender'}) .groupby('type') .sum() .reset_index())Gender_18 = ( df18['Q1'] .replace(['Prefer not to say', 'Prefer to self-describe'], 'Others') .fillna('Others') .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'Q1':'Gender'}) .groupby('type') .sum() .reset_index())Gender_19 = ( df19['Q2'] .replace(['Prefer not to say','Prefer to self-describe'],'Others') .fillna('Others') .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'Q2':'Gender'}) .groupby('type') .sum() .reset_index())Gender_20 = ( df20['Q2'] .replace(['Prefer not to say', 'Prefer to self-describe', 'Nonbinary'], 'Others') .replace(['Man', 'Woman'], ['Male', 'Female']) .fillna('Others') .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'Q2':'Gender'}) .groupby('type') .sum() .reset_index())Gender_21 = ( df21['Q2'] .replace(['Prefer not to say', 'Prefer to self-describe', 'Nonbinary'], 'Others') .replace(['Man', 'Woman'], ['Male', 'Female']) .fillna('Others') .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'Q2':'Gender'}) .groupby('type') .sum() .reset_index())# Create subplots: use 'domain' type for Pie subplotfig = make_subplots(rows=1, cols=5, specs=[[{'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}]], subplot_titles=(&quot;2017&quot;, &quot;2018&quot;, &quot;2019&quot;, &quot;2020&quot;, &quot;2021&quot;))fig.add_trace(go.Pie(labels=Gender_21['type'], values=Gender_21['Gender'], name=&quot;2021&quot;, scalegroup='one'), 1, 5)fig.add_trace(go.Pie(labels=Gender_20['type'], values=Gender_20['Gender'], name=&quot;2020&quot;, scalegroup='one'), 1, 4)fig.add_trace(go.Pie(labels=Gender_19['type'], values=Gender_19['Gender'], name=&quot;2019&quot;, scalegroup='one'), 1, 3)fig.add_trace(go.Pie(labels=Gender_18['type'], values=Gender_18['Gender'], name=&quot;2018&quot;, scalegroup='one'), 1, 2)fig.add_trace(go.Pie(labels=Gender_17['type'], values=Gender_17['Gender'], name=&quot;2017&quot;, scalegroup='one'), 1, 1)# Use `hole` to create a donut-like pie chartfig.update_traces(hole=.2, hoverinfo=&quot;label+percent+name&quot;)fig.update_layout( title_text=&quot;&lt;b&gt;World_Gender&lt;/b&gt;&quot;, # Add annotations in the center of the donut pies. )fig.show() 연도별, 지역별로 보았을때, 여성의 비율이 20% 미만으로 적은 것을 알 수 있다. 2019년 이전보다 2020년 이후가 증가 했다. (16%-&gt; 19%) 0. Kaggle job (W/Ea)0.1 data 전처리0.2 그래프 그리기123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293#data 확인Data_Analyst =['Data Analyst','Data Miner,Information technology','Data Miner', 'Predictive Modeler','Information technology, networking, or system administration', 'A business discipline (accounting, economics, finance, etc.)', 'Business Analyst', 'Humanities', 'Statistician', 'Mathematics or statistics', 'Medical or life sciences (biology, chemistry, medicine, etc.)', 'Physics or astronomy', 'Research Scientist', 'Researcher', 'Social sciences (anthropology, psychology, sociology, etc.)', 'Humanities (history, literature, philosophy, etc.)']Data_Scientist =['Data Scientist', 'Environmental science or geology', 'Machine Learning Engineer', 'Scientist/Researcher']Developer=['Developer Relations/Advocacy','Data Engineer','Engineer','Engineering (non-computer focused)', 'Programmer','Software Engineer', 'Computer Scientist','Computer science (software engineering, etc.)', 'Fine arts or performing arts','Product Manager', 'Software Developer/Software Engineer', 'Product/Project Manager','Program/Project Manager','DBA/Database Engineer']Not_Employeed =['Currently not employed', 'Not employed', 'Student']Others = ['I never declared a major', 'Other']#연도별로 뽑은 나라별 직업.df21job_Ea = df21_Ea.loc[:,['Q3','Q5']].reset_index().rename(columns={'index':'job', 'Q5':'2021'}).fillna('Other')df20job_Ea = df20_Ea.loc[:,['Q3','Q5']].reset_index().rename(columns={'index':'job', 'Q5':'2020'}).fillna('Other')df19job_Ea = df19_Ea.loc[:,['Q3','Q5']].reset_index().rename(columns={'index':'job', 'Q5':'2019'}).fillna('Other')df18job_Ea = df18_Ea.loc[:,['Q3','Q5']].reset_index().rename(columns={'index':'job', 'Q5':'2018'}).fillna('Other')df17job_Ea = df17_Ea.loc[:,['Country','CurrentJobTitleSelect']].reset_index().rename(columns={'index':'job', 'CurrentJobTitleSelect':'2017'}).fillna('Other')# 연도별 job Grouping in east asiadf21job_Ea.value_counts('2021')df21job_Ea['JOB']=[&quot;Data Analyst&quot; if x in Data_Analyst else &quot;Data Scientist&quot; if x in Data_Scientist # Data Scientist else &quot;Data Engineer&quot; if x in Developer else &quot;NotEmployeed&quot; if x in Not_Employeed else &quot;Others&quot; for x in df21job_Ea['2021']]df21job_Ea.value_counts('JOB')df20job_Ea.value_counts('2020')df20job_Ea['JOB']=[&quot;Data Analyst&quot; if x in Data_Analyst else &quot;Data Scientist&quot; if x in Data_Scientist else &quot;Data Engineer&quot; if x in Developer else &quot;NotEmployeed&quot; if x in Not_Employeed else &quot;Other&quot; for x in df20job_Ea['2020']]df20job_Ea[['2020','JOB']]df19job_Ea.value_counts('2019')df19job_Ea['JOB']=[&quot;Data Analyst&quot; if x in Data_Analyst else &quot;Data Scientist&quot; if x in Data_Scientist else &quot;Data Engineer&quot; if x in Developer else &quot;NotEmployeed&quot; if x in Not_Employeed else &quot;Other&quot; for x in df19job_Ea['2019']]#2019 data에 &quot;Other&quot; grouping이 제대로 이루어 졌는지 확인 df19jobTest = df19job_Ea.loc[df19job_Ea.JOB == 'Other']df19jobTest['2019'].value_counts()df18job_Ea.value_counts('2018')df18job_Ea['JOB']=[&quot;Data Analyst&quot; if x in Data_Analyst else &quot;Data Scientist&quot; if x in Data_Scientist else &quot;Data Engineer&quot; if x in Developer else &quot;NotEmployeed&quot; if x in Not_Employeed else &quot;Other&quot; for x in df18job_Ea['2018']]df18jobTest = df18job_Ea.loc[df18job_Ea.JOB == 'Other']df18jobTest['2018'].value_counts()df17job_Ea.value_counts('2017')df17job_Ea['JOB']=[&quot;Data Analyst&quot; if x in Data_Analyst else &quot;Data Scientist&quot; if x in Data_Scientist else &quot;Data Engineer&quot; if x in Developer else &quot;NotEmployeed&quot; if x in Not_Employeed else &quot;Other&quot; for x in df17job_Ea['2017']]df17jobTest = df17job_Ea.loc[df17job_Ea.JOB == 'Other']df17jobTest['2017'].value_counts()df21jobTest = df21job_Ea.loc[df21job_Ea.JOB == 'Other']df21jobTest['2021'].head()df21job_Ea.value_counts('JOB')#data frame 정리dfjob21 =df21job_Ea.groupby(['Q3','JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;}).rename(columns={'Q3':'country', 'JOB':'2021'})dfjob20 =df20job_Ea.groupby(['Q3','JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;}).rename(columns={'Q3':'country', 'JOB':'2020'})dfjob19 =df19job_Ea.groupby(['Q3','JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;}).rename(columns={'Q3':'country', 'JOB':'2019'})dfjob18 =df18job_Ea.groupby(['Q3','JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;}).rename(columns={'Q3':'country', 'JOB':'2018'})dfjob17 =df17job_Ea.groupby(['Country','JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;}).rename(columns={'Country':'country', 'JOB':'2017'}) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475#21년도 Bar graph 그리기fig = go.Figure()for country, group in dfjob21.groupby('country'): fig.add_trace(go.Bar( x = group['2021'], y = group['Count'], name = country ))fig.update_layout(barmode=&quot;group&quot;, plot_bgcolor = &quot;white&quot;, title='2021_나라별 직업 수', width=700, height=450 )fig.show()#20년도 Bar graph 그리기fig = go.Figure()for country, group in dfjob20.groupby('country'): fig.add_trace(go.Bar( x = group['2020'], y = group['Count'], name = country ))fig.update_layout(barmode=&quot;group&quot;, plot_bgcolor = &quot;white&quot;, title='2020_나라별 직업 수', width=700, height=450)fig.show()#19년도 Bar graph 그리기fig = go.Figure()for country, group in dfjob19.groupby('country'): fig.add_trace(go.Bar( x = group['2019'], y = group['Count'], name = country ))fig.update_layout(barmode=&quot;group&quot;, plot_bgcolor = &quot;white&quot;, title='2019_나라별 직업 수', width=700, height=450 )fig.show()#18년도 Bar graph 그리기fig = go.Figure()for country, group in dfjob18.groupby('country'): fig.add_trace(go.Bar( x = group['2018'], y = group['Count'], name = country ))fig.update_layout(barmode=&quot;group&quot;, plot_bgcolor = &quot;white&quot;, title='2018_나라별 직업 수', width=700, height=450 )fig.show()#17년도 Bar graph 그리기fig = go.Figure()for country, group in dfjob17.groupby('country'): fig.add_trace(go.Bar( x = group['2017'], y = group['Count'], name = country ))fig.update_layout(barmode=&quot;group&quot;, plot_bgcolor = &quot;white&quot;, title='2017_나라별 직업 수', width=700, height=450 )fig.show() 0. Kaggle age &amp; Edu (W/Ea)0.1 data 전처리0.2 그래프 그리기123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990# 연도별 나이 df21Age_Ea = df21_Ea.loc[:,['Q3','Q1']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'2021'}).fillna('etc')df20Age_Ea = df20_Ea.loc[:,['Q3','Q1']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'2020'}).fillna('etc')df19Age_Ea = df19_Ea.loc[:,['Q3','Q1']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'2019'}).fillna('etc')df18Age_Ea = df18_Ea.loc[:,['Q3','Q2']].reset_index().rename(columns={'Q3':'East_Asia', 'Q2':'2018'}).fillna('etc')df17Age_Ea = df17_Ea.loc[:,['Country','Age']].reset_index().rename(columns={'Country':'East_Asia', 'Age':'2017'}).fillna('etc')#data frame 정리dfAge21 =df21Age_Ea.groupby(['East_Asia','2021']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfAge20 =df20Age_Ea.groupby(['East_Asia','2020']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfAge19 =df19Age_Ea.groupby(['East_Asia','2019']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfAge18 =df18Age_Ea.groupby(['East_Asia','2018']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfAge17 =(df17Age_Ea.groupby(['East_Asia','2017']) .size().reset_index().rename(columns = {0:&quot;Count&quot;}))#2017data# array([16.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0,# 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0,# 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 47.0, 50.0, 54.0, 100.0,# 'etc', 46.0, 48.0, 49.0, 51.0, 52.0, 53.0, 55.0, 57.0, 58.0, 59.0,# 62.0, 64.0, 65.0, 67.0, 68.0, 70.0, 17.0, 56.0, 60.0], dtype=object)#21년도 Bar graph 그리기fig = go.Figure()for country, group in dfAge21.groupby('East_Asia'): fig.add_trace(go.Bar( x = group['2021'], y = group['Count'], name = country ))fig.update_layout(barmode=&quot;group&quot;, plot_bgcolor = &quot;white&quot;, title='2021_나라별 연령')fig.show()#20년도 Bar graph 그리기fig = go.Figure()for country, group in dfAge20.groupby('East_Asia'): fig.add_trace(go.Bar( x = group['2020'], y = group['Count'], name = country ))fig.update_layout(barmode=&quot;group&quot;, plot_bgcolor = &quot;white&quot;, title='2020_나라별 연령')fig.show()#19년도 Bar graph 그리기fig = go.Figure()for country, group in dfAge19.groupby('East_Asia'): fig.add_trace(go.Bar( x = group['2019'], y = group['Count'], name = country ))fig.update_layout(barmode=&quot;group&quot;, plot_bgcolor = &quot;white&quot;, title='2019_나라별 연령')fig.show()#18년도 Bar graph 그리기fig = go.Figure()for country, group in dfAge18.groupby('East_Asia'): fig.add_trace(go.Bar( x = group['2018'], y = group['Count'], name = country ))fig.update_layout(barmode=&quot;group&quot;, plot_bgcolor = &quot;white&quot;, title='2018_나라별 연령')fig.show()#17년도 Bar graph 그리기_ Scatter 로 변경fig = go.Figure()for country, group in dfAge17.groupby('East_Asia'): fig.add_trace(go.Bar( x = group['2017'], y = group['Count'], name = country ))fig.update_layout(barmode=&quot;group&quot;, plot_bgcolor = &quot;white&quot;, title='2017_나라별 연령')fig.show() 0.2 Kaggle age (Ea)0.1.1 data 전처리0.2.1 그래프 그리기123456789101112131415161718192021222324252627282930313233#data frame 정리dfAge21_percent =df21Age_Ea.groupby(['East_Asia','2021']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfAge21_percent['percent'] =((dfAge21_percent['Count'] / len(df21Age_Ea))*100).round(2)dfAge21_percent['percent_str'] =((dfAge21_percent['Count'] / len(df21Age_Ea))*100).round(2).astype(str) + '%'# dfAge21['percent'] = ((media['Count'] / len(df))*100).round(2).astype(str) + '%'# dfAge_percent21=dfAge21.value_counts('East_Asia',normalize=True).mul(100).round(1).astype(str)dfAge21_percent# 나라별 연령대 비율 in East Asiafig = go.Figure()for country, group in dfAge21_percent.groupby('East_Asia'): fig.add_trace(go.Bar( x = group['2021'], y = group['percent'], name = country, text=group['percent_str'] ))fig.update_layout(barmode=&quot;stack&quot;, plot_bgcolor = &quot;white&quot;, title='2021_ 연령별 나라 비율 in East Asia')fig.show()# 연령별 나라 비율 in East Asia // 다중 pie 그래프로 바꾸기 fig = go.Figure()for country, group in dfAge21_percent.groupby('2021'): fig.add_trace(go.Bar( x = group['East_Asia'], y =group['percent'], text=dfAge21_percent['percent_str'] ))fig.update_layout(barmode=&quot;stack&quot;, plot_bgcolor = &quot;white&quot;, title='2021_ 나라별 연령 비율 in East Asia')fig.show() 연도별 연령1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 나라별로 연도가 나왓으면 좋겠어요 .z = df21Age_Ea.groupby(['East_Asia', '2021']).size().unstack().fillna(0).astype('int64')z_data = z.apply(lambda x:np.round(x/x.sum(), 2), axis = 1).to_numpy() # convert to correlation matrixx = z.columns.tolist()y = z.index.tolist()fig21 = ff.create_annotated_heatmap(z_data, x = x, y = y, colorscale = &quot;sunset&quot;)z = df20Age_Ea.groupby(['East_Asia', '2020']).size().unstack().fillna(0).astype('int64')z_data = z.apply(lambda x:np.round(x/x.sum(), 2), axis = 1).to_numpy() # convert to correlation matrixx = z.columns.tolist()y = z.index.tolist()fig20 = ff.create_annotated_heatmap(z_data, x = x, y = y, colorscale = &quot;sunset&quot;)z = df19Age_Ea.groupby(['East_Asia', '2019']).size().unstack().fillna(0).astype('int64')z_data = z.apply(lambda x:np.round(x/x.sum(), 2), axis = 1).to_numpy() # convert to correlation matrixx = z.columns.tolist()y = z.index.tolist()fig19 = ff.create_annotated_heatmap(z_data, x = x, y = y, colorscale = &quot;sunset&quot;)z = df18Age_Ea.groupby(['East_Asia', '2018']).size().unstack().fillna(0).astype('int64')z_data = z.apply(lambda x:np.round(x/x.sum(), 2), axis = 1).to_numpy() # convert to correlation matrixx = z.columns.tolist()y = z.index.tolist()fig18 = ff.create_annotated_heatmap(z_data, x = x, y = y, colorscale = &quot;sunset&quot;)z = df17Age_Ea.groupby(['East_Asia', '2017']).size().unstack().fillna(0).astype('int64')z_data = z.apply(lambda x:np.round(x/x.sum(), 2), axis = 1).to_numpy() # convert to correlation matrixx = z.columns.tolist()y = z.index.tolist()fig17 = ff.create_annotated_heatmap(z_data, x = x, y = y, colorscale = &quot;sunset&quot;)fig21.show()fig20.show()fig19.show()fig18.show() 연령별 지역123456789101112131415161718192021222324252627282930313233343536373839404142# 연령-지역 %dfKo_Age21= df21Age_Ea[df21Age_Ea['East_Asia']=='South Korea']dfKo_Age21_per=dfKo_Age21['2021'].value_counts().to_frame().reset_index()dfKo_Age21_per['South Korea']=((dfKo_Age21_per['2021'] / len(dfKo_Age21))*100).round(2)dfTw_Age21= df21Age_Ea[df21Age_Ea['East_Asia']=='Taiwan']dfTw_Age21_per=dfTw_Age21['2021'].value_counts().to_frame().reset_index()dfTw_Age21_per['Taiwan']=((dfTw_Age21_per['2021'] / len(dfTw_Age21))*100).round(2)dfTw_Age21_perdfCh_Age21= df21Age_Ea[df21Age_Ea['East_Asia']=='China']dfCh_Age21_per=dfCh_Age21['2021'].value_counts().to_frame().reset_index()dfCh_Age21_per['China']=((dfCh_Age21_per['2021'] / len(dfCh_Age21))*100).round(2)dfCh_Age21_perdf21Age_Ea.head()dfJp_Age21= df21Age_Ea[df21Age_Ea['East_Asia']=='Japan']dfJp_Age21_per=dfJp_Age21['2021'].value_counts().to_frame().reset_index()dfJp_Age21_per['Japan']=((dfJp_Age21_per['2021'] / len(dfJp_Age21))*100).round(2)dfJp_Age21_per#g 그리기(heatMap)merge1= pd.merge(dfKo_Age21_per,dfTw_Age21_per, on='index', how='outer')merge2= pd.merge(dfCh_Age21_per,dfJp_Age21_per, on='index', how='outer')merge= pd.merge(merge1,merge2, on='index', how='outer').fillna(0).sort_values(by=['index'],ascending=True)merge.iloc[:,[2,4,6,8]]merge.iloc[:,[2,4,6,8]].to_numpy()fig = go.Figure(data=go.Heatmap( z=merge.iloc[:,[2,4,6,8]].to_numpy(), x=['South Korea','Taiwan','China','Japan'], y=merge.sort_values(by=['index'],ascending=True)['index'].tolist(), hoverongaps = False, opacity=1.0, xgap=2.5, ygap=2.5, colorscale='orrd'), )fig.show() 0.2 Kaggle Edu (Ea)0.1.1 data 전처리0.2.1 그래프 그리기1234567891011121314151617181920212223242526272829303132333435363738394041424344# 연도별 학력df21Edu_Ea = df21_Ea.loc[:,['Q3','Q4']].reset_index().rename(columns={'Q3':'East_Asia', 'Q4':'2021'}).fillna('etc')df20Edu_Ea = df20_Ea.loc[:,['Q3','Q4']].reset_index().rename(columns={'Q3':'East_Asia', 'Q4':'2020'}).fillna('etc')df19Edu_Ea = df19_Ea.loc[:,['Q3','Q4']].reset_index().rename(columns={'Q3':'East_Asia', 'Q4':'2019'}).fillna('etc')df18Edu_Ea = df18_Ea.loc[:,['Q3','Q4']].reset_index().rename(columns={'Q3':'East_Asia', 'Q4':'2018'}).fillna('etc')df17Edu_Ea = df17_Ea.loc[:,['Country','FormalEducation']].reset_index().rename(columns={'Country':'East_Asia', 'FormalEducation':'2017'}).fillna('etc')df21Edu_Ea =df21Edu_Ea.replace({'I prefer not to answer':'etc'}).sort_values(by='2021', ascending=False)df20Edu_Ea =df20Edu_Ea.replace({'I prefer not to answer':'etc'}).sort_values(by='2020', ascending=False)df19Edu_Ea =df19Edu_Ea.replace({'I prefer not to answer':'etc'}).sort_values(by='2019', ascending=False)df18Edu_Ea =df18Edu_Ea.replace({'I prefer not to answer':'etc'}).sort_values(by='2018', ascending=False)df17Edu_Ea =df17Edu_Ea.replace({'I prefer not to answer':'etc'}).sort_values(by='2017', ascending=False)#data frame 정리dfEdu21 =df21Edu_Ea.groupby(['East_Asia','2021']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfEdu20 =df20Edu_Ea.groupby(['East_Asia','2020']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfEdu19 =df19Edu_Ea.groupby(['East_Asia','2019']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfEdu18 =df18Edu_Ea.groupby(['East_Asia','2018']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfEdu17 =(df17Edu_Ea.groupby(['East_Asia','2017']) .size().reset_index().rename(columns = {0:&quot;Count&quot;}))# 비율 data 추가##21dfEdu21_percent =df21Edu_Ea.groupby(['East_Asia','2021']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfEdu21_percent['percent'] =((dfEdu21_percent['Count'] / len(df21Edu_Ea))*100).round(2)dfEdu21_percent['percent_str'] =((dfEdu21_percent['Count'] / len(df21Edu_Ea))*100).round(2).astype(str) + '%'##20dfEdu20_percent =df20Edu_Ea.groupby(['East_Asia','2020']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfEdu20_percent['percent'] =((dfEdu20_percent['Count'] / len(df20Edu_Ea))*100).round(2)dfEdu20_percent['percent_str'] =((dfEdu20_percent['Count'] / len(df20Edu_Ea))*100).round(2).astype(str) + '%'##19dfEdu19_percent =df19Edu_Ea.groupby(['East_Asia','2019']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfEdu19_percent['percent'] =((dfEdu19_percent['Count'] / len(df19Edu_Ea))*100).round(2)dfEdu19_percent['percent_str'] =((dfEdu19_percent['Count'] / len(df19Edu_Ea))*100).round(2).astype(str) + '%'##18dfEdu18_percent =df18Edu_Ea.groupby(['East_Asia','2018']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfEdu18_percent['percent'] =((dfEdu18_percent['Count'] / len(df18Edu_Ea))*100).round(2)dfEdu18_percent['percent_str'] =((dfEdu18_percent['Count'] / len(df18Edu_Ea))*100).round(2).astype(str) + '%'##19dfEdu17_percent =df17Edu_Ea.groupby(['East_Asia','2017']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfEdu17_percent['percent'] =((dfEdu17_percent['Count'] / len(df17Edu_Ea))*100).round(2)dfEdu17_percent['percent_str'] =((dfEdu17_percent['Count'] / len(df17Edu_Ea))*100).round(2).astype(str) + '%' 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667#21년도 Bar graph 그리기fig = go.Figure()for country, group in dfEdu21_percent.groupby('East_Asia'): fig.add_trace(go.Bar( x = group['2021'], y = group['percent'], name = country ))fig.update_layout(barmode=&quot;group&quot;, plot_bgcolor = &quot;white&quot;, title='2021_나라별 학력')fig.show()#20년도 Bar graph 그리기fig = go.Figure()for country, group in dfEdu20_percent.groupby('East_Asia'): fig.add_trace(go.Bar( x = group['2020'], y = group['percent'], name = country ))fig.update_layout(barmode=&quot;group&quot;, plot_bgcolor = &quot;white&quot;, title='2020_나라별 학력')fig.show()#19년도 Bar graph 그리기fig = go.Figure()for country, group in dfEdu19_percent.groupby('East_Asia'): fig.add_trace(go.Bar( x = group['2019'], y = group['percent'], name = country ))fig.update_layout(barmode=&quot;group&quot;, plot_bgcolor = &quot;white&quot;, title='2019_나라별 학력')fig.show()#18년도 Bar graph 그리기fig = go.Figure()for country, group in dfEdu18_percent.groupby('East_Asia'): fig.add_trace(go.Bar( x = group['2018'], y = group['percent'], name = country ))fig.update_layout(barmode=&quot;group&quot;, plot_bgcolor = &quot;white&quot;, title='2018_나라별 학력')fig.show()#17년도 Bar graph 그리기fig = go.Figure()for country, group in dfEdu17_percent.groupby('East_Asia'): fig.add_trace(go.Bar( x = group['2017'], y = group['percent'], name = country ))fig.update_layout(barmode=&quot;group&quot;, plot_bgcolor = &quot;white&quot;, title='2017_나라별 학력')fig.show() 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667#21년도 Bar graph 그리기fig = go.Figure()for country, group in dfEdu21_percent.groupby('East_Asia'): fig.add_trace(go.Bar( x = group['2021'], y = group['Count'], name = country ))fig.update_layout(barmode=&quot;stack&quot;, plot_bgcolor = &quot;white&quot;, title='2021_나라별 학력 비율 ')fig.show()#20년도 Bar graph 그리기fig = go.Figure()for country, group in dfEdu20_percent.groupby('East_Asia'): fig.add_trace(go.Bar( x = group['2020'], y = group['Count'], name = country ))fig.update_layout(barmode=&quot;stack&quot;, plot_bgcolor = &quot;white&quot;, title='2020_나라별 학력 비율 ')fig.show()#19년도 Bar graph 그리기fig = go.Figure()for country, group in dfEdu19_percent.groupby('East_Asia'): fig.add_trace(go.Bar( x = group['2019'], y = group['Count'], name = country ))fig.update_layout(barmode=&quot;stack&quot;, plot_bgcolor = &quot;white&quot;, title='2019_나라별 학력 비율 ')fig.show()#18년도 Bar graph 그리기fig = go.Figure()for country, group in dfEdu18_percent.groupby('East_Asia'): fig.add_trace(go.Bar( x = group['2018'], y = group['Count'], name = country ))fig.update_layout(barmode=&quot;stack&quot;, plot_bgcolor = &quot;white&quot;, title='2018_나라별 학력 비율 ')fig.show()#17년도 Bar graph 그리기fig = go.Figure()for country, group in dfEdu17_percent.groupby('East_Asia'): fig.add_trace(go.Bar( x = group['2017'], y = group['Count'], name = country ))fig.update_layout(barmode=&quot;stack&quot;, plot_bgcolor = &quot;white&quot;, title='2017_나라별 학력 비율 ')fig.show() 12345678z = df21_Ea.groupby(['Q4', 'Q1']).size().unstack().fillna(0).astype('int64')z_data = z.apply(lambda x:np.round(x/x.sum(), 2), axis = 1).to_numpy() # convert to correlation matrixx = z.columns.tolist()y = z.index.tolist()fig = ff.create_annotated_heatmap(z_data, x = x, y = y, colorscale = &quot;sunset&quot;)fig.show() 경력123456789101112131415161718192021222324#전체 코드_3year = ['I have never written code', '&lt; 1 years', '1-3 years']_5year = ['3-5 years ','5-10 years']_10year = ['10-20 years','20+ years']df21_3year = df21['Q6'][df21['Q6'].isin(_3year)]df21_5year = df21['Q6'][df21['Q6'].isin(_5year)]df21_10year = df21['Q6'][df21['Q6'].isin(_10year)]df21_3year.count()df21_5year.count()df21_10year.count()years =['_3year','_5year', '_10year']values =[df21_3year.count(), df21_5year.count(), df21_10year.count()]fig = go.Figure(data=[ go.Bar(name='21년 World kaggler들의 경력', x=years, y=values ,orientation='v'),])fig.update_layout(title_text=&quot;&lt;b&gt;21년 World kaggler들의 경력&lt;/b&gt;&quot;,title_font_size=35)fig.show() 123456789101112131415161718192021222324252627282930#최종 합친 코드_3year = ['I have never written code', '&lt; 1 years', '1-3 years']_5year = ['3-5 years ','5-10 years']_10year = ['10-20 years','20+ years']df21_Ea_3year = df21_Ea['Q3'][df21_Ea['Q6'].isin(_3year)].value_counts().to_frame().rename(columns = {'Q3':'3year'})df21_Ea_5year = df21_Ea['Q3'][df21_Ea['Q6'].isin(_5year)].value_counts().to_frame().rename(columns = {'Q3':'5year'})df21_Ea_10year = df21_Ea['Q3'][df21_Ea['Q6'].isin(_10year)].value_counts().to_frame().rename(columns = {'Q3':'10year'})career=(df21_Ea_3year.join(df21_Ea_5year).join(df21_Ea_10year))careercareer.iloc[0,0:3] #Chinacareer.iloc[1,0:3] #Japancareer.iloc[2,0:3] #South Koreacareer.iloc[3,0:3] #Taiwanfig = go.Figure(data=[ go.Bar(name='China', x = years, y=career.iloc[0,0:3]), go.Bar(name='Japan', x = years, y=career.iloc[1,0:3]), go.Bar(name='South Korea', x= years, y=career.iloc[2,0:3]), go.Bar(name='Taiwan', x= years, y=career.iloc[3,0:3]) ])fig.update_layout(title_text=&quot;&lt;b&gt;21년 EastAisa kaggler들의 경력&lt;/b&gt;&quot;,title_font_size=35)fig.show() 연봉1234567891011121314#전체 코드#마지막 행 삭제해줌df21_=(df21['Q25'].value_counts().to_frame())#df21_=df21_.drop(df21_.index[26])#df21_compensation = df21_['Q25'].indexfig = go.Figure(data=[ go.Bar(name='21년 World kaggler들의 연봉', x=compensation, y=df21_['Q25'].to_numpy() ,orientation='v')])fig.update_layout(title_text=&quot;&lt;b&gt;21년 World kaggler들의 연봉&lt;/b&gt;&quot;,title_font_size=35)fig.show() 123456789101112131415161718compensation = df21_['Q25'].indexfig = go.Figure(data=[ go.Bar(name='China', x = compensation, y = df21_Ea['Q25'][df21_Ea['Q3'] =='Japan'].value_counts()), go.Bar(name='Japan', x = compensation, y=df21_Ea['Q25'][df21_Ea['Q3'] =='Taiwan'].value_counts()), go.Bar(name='South Korea', x = compensation, y=df21_Ea['Q25'][df21_Ea['Q3'] =='South Korea'].value_counts()), go.Bar(name='Taiwan', x = compensation, y=df21_Ea['Q25'][df21_Ea['Q3'] =='China'].value_counts()) ])fig.update_layout(title_text=&quot;&lt;b&gt;21년 EastAisa kaggler들의 연봉&lt;/b&gt;&quot;,title_font_size=35)fig.show() 언어1df21['Q7_Part_1'].value_counts() Python 21860 Name: Q7_Part_1, dtype: int64 12345678910111213141516171819202122232425262728293031323334353637#코드 전체df21_p = df21['Q7_Part_1'].value_counts().to_frame() #pythondf21_r = df21['Q7_Part_2'].value_counts().to_frame() #rdf21_s = df21['Q7_Part_3'].value_counts().to_frame() #sqldf21_c = df21['Q7_Part_4'].value_counts().to_frame() #cdf21_cc = df21['Q7_Part_5'].value_counts().to_frame() #c++df21_j = df21['Q7_Part_6'].value_counts().to_frame() #javadf21_js = df21['Q7_Part_7'].value_counts().to_frame() #javascriptdf21_ju = df21['Q7_Part_8'].value_counts().to_frame() #juliadf21_sw = df21['Q7_Part_9'].value_counts().to_frame() #swiftdf21_b = df21['Q7_Part_10'].value_counts().to_frame() #bashdf21_ma = df21['Q7_Part_11'].value_counts().to_frame() #matlabdf21_n = df21['Q7_Part_12'].value_counts().to_frame() #nonelanguages = ['Python','R','SQL','C','C++','Java','Javascript','Julia','Swift','Bash','MATLAB','None']fig = go.Figure(data=[ go.Bar(name='21년 World kaggler들이 사용하는 언어', x = languages, y = [df21_p.iloc[0,0], df21_r.iloc[0,0], df21_s.iloc[0,0], df21_c.iloc[0,0], df21_cc.iloc[0,0], df21_j.iloc[0,0], df21_js.iloc[0,0], df21_ju.iloc[0,0], df21_sw.iloc[0,0], df21_b.iloc[0,0], df21_ma.iloc[0,0], df21_n.iloc[0,0]],orientation='v') ])fig.update_layout(title_text=&quot;&lt;b&gt;21년 World kaggler들이 사용하는 언어&lt;/b&gt;&quot;,title_font_size=35)fig.show() 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980df21_lan_ch_p=df21_Ea['Q7_Part_1'][df21_Ea['Q3']=='China'].value_counts().to_frame().rename(columns = {'Q7_Part_1':'cnt'})df21_lan_ch_r=df21_Ea['Q7_Part_2'][df21_Ea['Q3']=='China'].value_counts().to_frame().rename(columns = {'Q7_Part_2':'cnt'})df21_lan_ch_s=df21_Ea['Q7_Part_3'][df21_Ea['Q3']=='China'].value_counts().to_frame().rename(columns = {'Q7_Part_3':'cnt'})df21_lan_ch_c=df21_Ea['Q7_Part_4'][df21_Ea['Q3']=='China'].value_counts().to_frame().rename(columns = {'Q7_Part_4':'cnt'})df21_lan_ch_cc=df21_Ea['Q7_Part_5'][df21_Ea['Q3']=='China'].value_counts().to_frame().rename(columns = {'Q7_Part_5':'cnt'})df21_lan_ch_j=df21_Ea['Q7_Part_6'][df21_Ea['Q3']=='China'].value_counts().to_frame().rename(columns = {'Q7_Part_6':'cnt'})df21_lan_ch_js=df21_Ea['Q7_Part_7'][df21_Ea['Q3']=='China'].value_counts().to_frame().rename(columns = {'Q7_Part_7':'cnt'})df21_lan_ch_ju=df21_Ea['Q7_Part_8'][df21_Ea['Q3']=='China'].value_counts().to_frame().rename(columns = {'Q7_Part_8':'cnt'})df21_lan_ch_sw=df21_Ea['Q7_Part_9'][df21_Ea['Q3']=='China'].value_counts().to_frame().rename(columns = {'Q7_Part_9':'cnt'})df21_lan_ch_b=df21_Ea['Q7_Part_10'][df21_Ea['Q3']=='China'].value_counts().to_frame().rename(columns = {'Q7_Part_10':'cnt'})df21_lan_ch_ma=df21_Ea['Q7_Part_11'][df21_Ea['Q3']=='China'].value_counts().to_frame().rename(columns = {'Q7_Part_11':'cnt'})df21_lan_ch_n=df21_Ea['Q7_Part_12'][df21_Ea['Q3']=='China'].value_counts().to_frame().rename(columns = {'Q7_Part_12':'cnt'})ch_lan = pd.concat([df21_lan_ch_p,df21_lan_ch_r,df21_lan_ch_s,df21_lan_ch_c,df21_lan_ch_cc,df21_lan_ch_j,df21_lan_ch_js,df21_lan_ch_ju,df21_lan_ch_sw,df21_lan_ch_b,df21_lan_ch_ma,df21_lan_ch_n])df21_lan_jp_p=df21_Ea['Q7_Part_1'][df21_Ea['Q3']=='Japan'].value_counts().to_frame().rename(columns = {'Q7_Part_1':'cnt'})df21_lan_jp_r=df21_Ea['Q7_Part_2'][df21_Ea['Q3']=='Japan'].value_counts().to_frame().rename(columns = {'Q7_Part_2':'cnt'})df21_lan_jp_s=df21_Ea['Q7_Part_3'][df21_Ea['Q3']=='Japan'].value_counts().to_frame().rename(columns = {'Q7_Part_3':'cnt'})df21_lan_jp_c=df21_Ea['Q7_Part_4'][df21_Ea['Q3']=='Japan'].value_counts().to_frame().rename(columns = {'Q7_Part_4':'cnt'})df21_lan_jp_cc=df21_Ea['Q7_Part_5'][df21_Ea['Q3']=='Japan'].value_counts().to_frame().rename(columns = {'Q7_Part_5':'cnt'})df21_lan_jp_j=df21_Ea['Q7_Part_6'][df21_Ea['Q3']=='Japan'].value_counts().to_frame().rename(columns = {'Q7_Part_6':'cnt'})df21_lan_jp_js=df21_Ea['Q7_Part_7'][df21_Ea['Q3']=='Japan'].value_counts().to_frame().rename(columns = {'Q7_Part_7':'cnt'})df21_lan_jp_ju=df21_Ea['Q7_Part_8'][df21_Ea['Q3']=='Japan'].value_counts().to_frame().rename(columns = {'Q7_Part_8':'cnt'})df21_lan_jp_sw=df21_Ea['Q7_Part_9'][df21_Ea['Q3']=='Japan'].value_counts().to_frame().rename(columns = {'Q7_Part_9':'cnt'})df21_lan_jp_b=df21_Ea['Q7_Part_10'][df21_Ea['Q3']=='Japan'].value_counts().to_frame().rename(columns = {'Q7_Part_10':'cnt'})df21_lan_jp_ma=df21_Ea['Q7_Part_11'][df21_Ea['Q3']=='Japan'].value_counts().to_frame().rename(columns = {'Q7_Part_11':'cnt'})df21_lan_jp_n=df21_Ea['Q7_Part_12'][df21_Ea['Q3']=='Japan'].value_counts().to_frame().rename(columns = {'Q7_Part_12':'cnt'})jp_lan = pd.concat([df21_lan_jp_p,df21_lan_jp_r,df21_lan_jp_s,df21_lan_jp_c,df21_lan_jp_cc,df21_lan_jp_j,df21_lan_jp_js,df21_lan_jp_ju,df21_lan_jp_sw,df21_lan_jp_b,df21_lan_jp_ma,df21_lan_jp_n])df21_lan_tw_p=df21_Ea['Q7_Part_1'][df21_Ea['Q3']=='Taiwan'].value_counts().to_frame().rename(columns = {'Q7_Part_1':'cnt'})df21_lan_tw_r=df21_Ea['Q7_Part_2'][df21_Ea['Q3']=='Taiwan'].value_counts().to_frame().rename(columns = {'Q7_Part_2':'cnt'})df21_lan_tw_s=df21_Ea['Q7_Part_3'][df21_Ea['Q3']=='Taiwan'].value_counts().to_frame().rename(columns = {'Q7_Part_3':'cnt'})df21_lan_tw_c=df21_Ea['Q7_Part_4'][df21_Ea['Q3']=='Taiwan'].value_counts().to_frame().rename(columns = {'Q7_Part_4':'cnt'})df21_lan_tw_cc=df21_Ea['Q7_Part_5'][df21_Ea['Q3']=='Taiwan'].value_counts().to_frame().rename(columns = {'Q7_Part_5':'cnt'})df21_lan_tw_j=df21_Ea['Q7_Part_6'][df21_Ea['Q3']=='Taiwan'].value_counts().to_frame().rename(columns = {'Q7_Part_6':'cnt'})df21_lan_tw_js=df21_Ea['Q7_Part_7'][df21_Ea['Q3']=='Taiwan'].value_counts().to_frame().rename(columns = {'Q7_Part_7':'cnt'})df21_lan_tw_ju=df21_Ea['Q7_Part_8'][df21_Ea['Q3']=='Taiwan'].value_counts().to_frame().rename(columns = {'Q7_Part_8':'cnt'})df21_lan_tw_sw=df21_Ea['Q7_Part_9'][df21_Ea['Q3']=='Taiwan'].value_counts().to_frame().rename(columns = {'Q7_Part_9':'cnt'})df21_lan_tw_b=df21_Ea['Q7_Part_10'][df21_Ea['Q3']=='Taiwan'].value_counts().to_frame().rename(columns = {'Q7_Part_10':'cnt'})df21_lan_tw_ma=df21_Ea['Q7_Part_11'][df21_Ea['Q3']=='Taiwan'].value_counts().to_frame().rename(columns = {'Q7_Part_11':'cnt'})df21_lan_tw_n=df21_Ea['Q7_Part_12'][df21_Ea['Q3']=='Taiwan'].value_counts().to_frame().rename(columns = {'Q7_Part_12':'cnt'})tw_lan = pd.concat([df21_lan_tw_p,df21_lan_tw_r,df21_lan_tw_s,df21_lan_tw_c,df21_lan_tw_cc,df21_lan_tw_j,df21_lan_tw_js,df21_lan_tw_ju,df21_lan_tw_sw,df21_lan_tw_b,df21_lan_tw_ma,df21_lan_tw_n])df21_lan_ko_p=df21_Ea['Q7_Part_1'][df21_Ea['Q3']=='South Korea'].value_counts().to_frame().rename(columns = {'Q7_Part_1':'cnt'})df21_lan_ko_r=df21_Ea['Q7_Part_2'][df21_Ea['Q3']=='South Korea'].value_counts().to_frame().rename(columns = {'Q7_Part_2':'cnt'})df21_lan_ko_s=df21_Ea['Q7_Part_3'][df21_Ea['Q3']=='South Korea'].value_counts().to_frame().rename(columns = {'Q7_Part_3':'cnt'})df21_lan_ko_c=df21_Ea['Q7_Part_4'][df21_Ea['Q3']=='South Korea'].value_counts().to_frame().rename(columns = {'Q7_Part_4':'cnt'})df21_lan_ko_cc=df21_Ea['Q7_Part_5'][df21_Ea['Q3']=='South Korea'].value_counts().to_frame().rename(columns = {'Q7_Part_5':'cnt'})df21_lan_ko_j=df21_Ea['Q7_Part_6'][df21_Ea['Q3']=='South Korea'].value_counts().to_frame().rename(columns = {'Q7_Part_6':'cnt'})df21_lan_ko_js=df21_Ea['Q7_Part_7'][df21_Ea['Q3']=='South Korea'].value_counts().to_frame().rename(columns = {'Q7_Part_7':'cnt'})df21_lan_ko_ju=df21_Ea['Q7_Part_8'][df21_Ea['Q3']=='South Korea'].value_counts().to_frame().rename(columns = {'Q7_Part_8':'cnt'})df21_lan_ko_sw=df21_Ea['Q7_Part_9'][df21_Ea['Q3']=='South Korea'].value_counts().to_frame().rename(columns = {'Q7_Part_9':'cnt'})df21_lan_ko_b=df21_Ea['Q7_Part_10'][df21_Ea['Q3']=='South Korea'].value_counts().to_frame().rename(columns = {'Q7_Part_10':'cnt'})df21_lan_ko_ma=df21_Ea['Q7_Part_11'][df21_Ea['Q3']=='South Korea'].value_counts().to_frame().rename(columns = {'Q7_Part_11':'cnt'})df21_lan_ko_n=df21_Ea['Q7_Part_12'][df21_Ea['Q3']=='South Korea'].value_counts().to_frame().rename(columns = {'Q7_Part_12':'cnt'})ko_lan = pd.concat([df21_lan_ko_p,df21_lan_ko_r,df21_lan_ko_s,df21_lan_ko_c,df21_lan_ko_cc,df21_lan_ko_j,df21_lan_ko_js,df21_lan_ko_ju,df21_lan_ko_sw,df21_lan_ko_b,df21_lan_ko_ma,df21_lan_ko_n])ch_lan['cnt'].to_list()languages = ['Python','R','SQL','C','C++','Java','Javascript','Julia','Swift','Bash','MATLAB','None']fig = go.Figure(data=[ go.Bar(name='China', x = languages, y = ch_lan['cnt'].tolist()), go.Bar(name='Japan', x = languages, y=jp_lan['cnt'].tolist()), go.Bar(name='South Korea', x = languages, y=ko_lan['cnt'].tolist()), go.Bar(name='Taiwan', x = languages, y=tw_lan['cnt'].tolist()) ])fig.update_layout(title_text=&quot;&lt;b&gt;21년 EastAisa kaggler들이 사용하는 언어&lt;/b&gt;&quot;,title_font_size=35)fig.show() Thank you for reading!","link":"/2021/11/18/kgg/Kgg_eastasia_Ver.1118/"},{"title":"kgg compete data 정리","text":"data Import1234567891011121314151617181920212223242526272829import numpy as npimport pandas as pdimport seaborn as snsimport matplotlib.pylab as pltimport plotly.io as pioimport plotly.express as pximport plotly.graph_objects as goimport plotly.figure_factory as fffrom plotly.subplots import make_subplotsfrom plotly.offline import init_notebook_mode, iplotinit_notebook_mode(connected=True)pio.templates.default = &quot;none&quot;# import plotly.offline as py# py.offline.init_notebook_mode()import osfor dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename))import warningswarnings.filterwarnings(&quot;ignore&quot;)df17= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2017/multipleChoiceResponses.csv&quot;, encoding=&quot;ISO-8859-1&quot;)df18= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2018/multipleChoiceResponses.csv&quot;, )df19= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2019/multiple_choice_responses.csv&quot;, )df20= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2020/kaggle_survey_2020_responses.csv&quot;, )df21= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2021/kaggle_survey_2021_responses.csv&quot;, ) 질문 뽑는 법18년도의 질문 frame을 확인 해 보자 . 12questions = df18.iloc[0, :].Tquestions 질문 제거하기이후 data에 질문이 들어가면 안되기 때문에 질문을 제거 해 준다. 12345df17= df17.iloc[1:, :].replace(&quot;People 's Republic of China&quot;,'China')df18= df18.iloc[1:, :].replace('Republic of Korea','South Korea')df19= df19.iloc[1:, :].replace('Republic of Korea','South Korea')df20= df20.iloc[1:, :].replace('Republic of Korea','South Korea')df21= df21.iloc[1:, :] 연도 추가하기이후 연도별 data를 뽑기 위해서 data set에 연도를 추가 해 준다. 12345df21['year'] = '2021'df20['year'] = '2020'df19['year'] = '2019'df18['year'] = '2018'df17['year'] = '2017' 동아시아와 세계 나누기1df21['Q1'].unique() [Q3]에 나라가 있다. 나라를 확인하여 East Asia만 region column을 새로 만들어 분류 해 준다. 1234567891011121314151617181920212223242526272829EastAsia17 = ['China',&quot;People 's Republic of China&quot;, 'Taiwan', 'South Korea', 'Japan']EastAsia18= ['China', 'South Korea', 'Japan', 'Republic of Korea'] EastAsia19 = ['China','Taiwan', 'South Korea', 'Japan', 'Republic of Korea']EastAsia20 = ['China','Taiwan', 'South Korea','Republic of Korea', 'Japan']EastAsia21 = ['China','Taiwan', 'South Korea', 'Japan']EastAsia = ['Republic of Korea','China','Taiwan', 'South Korea', 'Japan', &quot;People 's Republic of China&quot; ]df21_Ea = df21[df21['Q3'].isin(EastAsia)]df21_Wo = df21[~df21['Q3'].isin(EastAsia)]df21['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df21['Q3']]df20_Ea = df20[df20['Q3'].isin(EastAsia)]df20_Wo = df20[~df20['Q3'].isin(EastAsia)]df20['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df20['Q3']]df19_Ea = df19[df19['Q3'].isin(EastAsia)]df19_Wo = df19[~df19['Q3'].isin(EastAsia)]df19['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df19['Q3']]df18_Ea = df18[df18['Q3'].isin(EastAsia)]df18_Wo = df18[~df18['Q3'].isin(EastAsia)]df18['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df18['Q3']]df17_Ea = df17[df17['Country'].isin(EastAsia)]df17_Wo = df17[~df17['Country'].isin(EastAsia)]df17['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df17['Country']] lng()을 이용하여 % 그래프 그리기1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# 수치 bar g = 사용자 수 비교.Ea21 = len(df21_Ea)Wo21 = len(df21) - len(df21_Ea)Ea20 = len(df20_Ea)Wo20 = len(df20) - len(df20_Ea)Ea19 = len(df19_Ea)Wo19 = len(df19) - len(df19_Ea)Ea18 = len(df18_Ea)Wo18 = len(df18) - len(df18_Ea)Ea17 = len(df17_Ea)Wo17 = len(df17) - len(df17_Ea)years = ['2017','2018','2019','2020', '2021']def percent (a, b): result =a/(a+b)*100 result = np.round(result) return resultdef percentR (b, a): result =a/(a+b)*100 result = np.round(result) return resultpercent = [percent(Ea17, Wo17), percent(Ea18, Wo18), percent(Ea19, Wo19), percent(Ea20, Wo20), percent(Ea21, Wo21)]# percentR = [percentR(Ea17, Wo17), percentR(Ea18, Wo18), percentR(Ea19, Wo19), # percentR(Ea20, Wo20), percentR(Ea21, Wo21)]fig = go.Figure()fig.add_trace(go.Bar(x=years, y=[len(df17), len(df18), len(df19), len(df20), len(df21)], base=[-len(df17), -len(df18), -len(df19), -len(df20), -len(df21)], marker_color='#88BFBA', name='World' ))fig.add_trace(go.Bar(x=years, y=[Ea17, Ea18, Ea19, Ea20, Ea21], base=0, marker_color='#D9946C', name='East Asia', text= percent, texttemplate='%{text} %', textposition='outside', hovertemplate='&lt;b&gt;KaggleUser&lt;/b&gt;: %{x}&lt;br&gt;'+ '&lt;b&gt;Count&lt;/b&gt;: %{y}', textfont_size=14 ))fig.show() 다음과 같은 G가 그려 진다. East asia와 world비교 _ kgg 사용자수123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687#data 정제하기total17 = ( df17['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index())total18 = ( df18['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index())total19 = ( df19['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index())total20 = ( df20['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index())total21 = ( df21['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index())colors = ['#88BFBA','#F28705']# Create subplots: use 'domain' type for Pie subplotfig = make_subplots(rows=1, cols=5, specs=[[{'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}]], subplot_titles=(&quot;2017&quot;, &quot;2018&quot;, &quot;2019&quot;, &quot;2020&quot;, &quot;2021&quot;))fig.add_trace(go.Pie(marker=dict(colors=colors),labels=total21['type'], values=total21['respodents'], name=&quot;2021&quot;, scalegroup='one'), 1, 1)fig.add_trace(go.Pie(marker=dict(colors=colors),labels=total20['type'], values=total20['respodents'], name=&quot;2020&quot;, scalegroup='one'), 1, 2)fig.add_trace(go.Pie(marker=dict(colors=colors),labels=total19['type'], values=total19['respodents'], name=&quot;2019&quot;, scalegroup='one'), 1, 3)fig.add_trace(go.Pie(marker=dict(colors=colors),labels=total18['type'], values=total18['respodents'], name=&quot;2018&quot;, scalegroup='one'), 1, 4)fig.add_trace(go.Pie(marker=dict(colors=colors),labels=total17['type'], values=total17['respodents'], name=&quot;2017&quot;, scalegroup='one'), 1, 5)# Use `hole` to create a donut-like pie chartfig.update_traces(hole=.0, hoverinfo=&quot;label+percent+name&quot;, textfont_size=15,)fig.update_layout(showlegend=False, margin=dict(pad=20), height=100, yaxis_title=None, xaxis_title=None, title_text=&quot;&lt;b&gt;World vs EastAsia&lt;/b&gt;&quot;, title_font_size=22, font=dict(size=17, color='#000000'), autosize=True)fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.show() subplot을 이용하여 원그래프 5개를 한꺼번에 묶어서 출력 할 수 있다. World map그리기1234567891011121314151617181920212223242526272829303132333435def world_map(locations,counts,title): data = [ dict( type = 'choropleth', locations = locations, z = counts, colorscale = 'Blues', locationmode = 'country names', autocolorscale = False, reversescale = True, marker = dict( line = dict(color = '#F7F7F7', width = 1.5)), colorbar = dict(autotick = True, legth = 3, len=0.75, title = 'respodents', max = 1000, min = 0) ) ] layout = dict( title = title, titlefont={'size': 28, 'family': 'san serif'}, width=750, height=475, paper_bgcolor='#F7F7F7', geo = dict( showframe = True, showcoastlines = True, fitbounds=&quot;locations&quot;, ) ) fig = dict(data=data, layout=layout) iplot(fig, validate=False, filename='world-map') z = df21_Ea['Q3'].value_counts() ## 메서드 호출world_map(locations=z.index, counts=z.values, title= '&lt;b&gt; EastAsia Countries (2021 survey) &lt;b&gt;') bar mode를 stack으로 하여 G를 그릴 수 있다.12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061years = ['2017', '2018', '2019', '2020', '2021']df21_Ea = df21[df21['Q3'].isin(EastAsia21)]Ea21= ( df21_Ea['Q3'].value_counts().to_frame() .reset_index().rename(columns={'index':'Country', 'Q3':'21'}))df20_Ea=df20[df20['Q3'].isin(EastAsia)]Ea20= ( df20_Ea['Q3'].replace('Republic of Korea','South Korea') .value_counts().to_frame().reset_index() .rename(columns={'index':'Country', 'Q3':'20'}))df19_Ea=df19[df19['Q3'].isin(EastAsia)]Ea19= (df19_Ea['Q3'].replace('Republic of Korea','South Korea') .value_counts().to_frame().reset_index() .rename(columns={'index':'Country', 'Q3':'19'}))df18_Ea=df18[df18['Q3'].isin(EastAsia)]Ea18= (df18_Ea['Q3'].replace('Republic of Korea','South Korea') .value_counts().to_frame().reset_index() .rename(columns={'index':'Country', 'Q3':'18'}))Ea18.value_counts()#df18 열에 taiwan = 0을 추가 해야 합니다. df17_Ea = df17[df17['Country'].isin(EastAsia)]Ea17= (df17_Ea['Country'].replace(&quot;People 's Republic of China&quot;,'China') .value_counts().to_frame().reset_index() .rename(columns={'index':'Country', 'Country':'17'}))#data를 합쳐서 하나의 dataframe으로 만들어 줌.df5years = pd.merge(Ea17, Ea18, on='Country', how='outer')df5year =pd.merge(Ea19,Ea20, on='Country', how='outer')df5year=pd.merge(df5year, Ea21, on='Country', how='outer')df5years = pd.merge(df5years, df5year, on='Country', how='outer')fig = go.Figure(data=[ go.Bar(name='2017', x=df5years['Country'], y=df5years['17']), go.Bar(name='2018', x=df5years['Country'], y=df5years['18']), go.Bar(name='2019', x=df5years['Country'], y=df5years['19']), go.Bar(name='2020', x=df5years['Country'], y=df5years['20']), go.Bar(name='2021', x=df5years['Country'], y=df5years['21'])])fig.update_layout(barmode='stack', showlegend=True, margin=dict(pad=20), height=500, yaxis_title=None, xaxis_title=None, title_text=&quot;&lt;b&gt;연도별 동아시아 Kaggle 사용자수&lt;/b&gt;&quot;, title_x=0.5, font=dict(size=17, color='#000000'), title_font_size=35)fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.show()# Text : percent 오늘은 여기까지 정리","link":"/2021/11/19/kgg/Kgg_compt/"},{"title":"NLP_text_classification","text":"##Kaggle _ API !pip Install Kaggle : Kaggle 설치 google.colab에 kaggle.json files upload Saving kaggle.json to kaggle.json User uploaded file “kaggle.json” with length 66 bytes kaggle.json file에는 뭐가 들어있을까 너무 궁금하다. !kaggle competitions download -c nlp-getting-started 케글 대회 자료를 다운받기. (-c nlp-getting-started 이게 뭘까) data path 설정하기 ##data 둘러보기 data frame을 만들기 위해 Pandas와 numpy를 import후 각 file을 data set을 Load해 준다. data set 확인 .head()로 대략적인 data set 확인 .shape로 각 data set의 크기 확인 .info()로 각 data frame의 정보 확인 ##EDAEDA : 수집한 data를 다양한 각도에서 관찰하고 이해하는 과정 : 통계적 방법으로 자료를 직관적으로 바라보는 과정 data의 분포 및 값을 검토함으로써 데이터가 표현하는 현상을 더 잘 이해하고, 잠재적 문제를 발견 할 수 있다. 문제를 발견하여 기존의 가설을 수정하거나 새로운 가설을 세울 수 있다. data visualiztion을 위해 matplotlib.pyplot과 seaborn 설치 missing_colunms = [“keyword”, “location”] 각 data set에서 null인 columns를 가져온다. matplotlib.pyplot으로 bar plot 그리기 ##Feature Engineering##Mideling##algorithm logistic regression","link":"/2021/11/11/kgg/Kgg_nlp_Tex/"},{"title":"kaggle :HorizontalBar (Q9)","text":"kaggle dictation (04) plotly.graph_objects as go: 를 이용한 bar graphHorizontalBar plot /가로 막대 차트0. data sethttps://www.kaggle.com/miguelfzzz/the-typical-kaggle-data-scientist-in-2021 Subject : 어떤알고리즘을 선호 하는 지 알아보는 Horizontal bar1. data 읽어오기123456# Features that start with Q7ide_cols = [col for col in df if col.startswith('Q9')]ide = df[ide_cols]print(ide) list comprehensionfor문으로 돌린 data값을 전부 col로 받아와서그 값을 또 ide_cols에 싣는 한줄로 이루어진 코드. 2. data Frame 만들어 주기ide를 dataframe화 완료. 123ide.columns = ['JupyterLab', 'RStudio', 'Visual Studio', 'VSCode', 'PyCharm', 'Spyder', 'Notepad++', 'Sublime Text', 'Vim, Emacs, or similar', 'MATLAB', 'Jupyter Notebook', 'None', 'Other'] Q9의 column이름 재 설정. 3.표 설정.12345678ide = ( ide .count() .to_frame() .reset_index() .rename(columns={'index':'IDE', 0:'Count'}) .sort_values(by=['Count'], ascending=False) ) 3. percent 추가1ide['percent'] = ((ide['Count'] / len(df))*100).round(2).astype(str) + '%' 4. 색 지정123456789colors = ['#033351',] * 13colors[0] = '#5abbf9'colors[1] = '#5abbf9'colors[2] = '#0779c3'colors[3] = '#0779c3'colors[4] = '#0779c3'colors[5] = '#0779c3'colors[6] = '#0779c3'colors[7] = '#0779c3' 5. bar Graph 만들기1234567fig = go.Figure(go.Bar( x=ide['Count'], y=ide['IDE'], text=ide['percent'], orientation='h', marker_color=colors )) 6. update_traces()123456fig.update_traces(texttemplate='%{text}', textposition='outside', cliponaxis = False, hovertemplate='&lt;b&gt;IDE&lt;/b&gt;: %{y}&lt;br&gt;&lt;extra&gt;&lt;/extra&gt;'+ '&lt;b&gt;Count&lt;/b&gt;: %{x}', textfont_size=12) 7. Design12345678910111213141516fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False) fig.update_layout(showlegend=False, plot_bgcolor='#F7F7F7', margin=dict(pad=20), paper_bgcolor='#F7F7F7', xaxis={'showticklabels': False}, yaxis_title=None, height = 600, xaxis_title=None, yaxis={'categoryorder':'total ascending'}, title_text=&quot;Most Commonly Used &lt;b&gt;IDE's&lt;/b&gt;&quot;, title_x=0.5, font=dict(family=&quot;Hiragino Kaku Gothic Pro, sans-serif&quot;, size=15, color='#000000'), title_font_size=35) 8. Annotation1234567891011121314151617181920fig.add_annotation(dict(font=dict(size=14), x=0.98, y=-0.17, showarrow=False, text=&quot;@miguelfzzz&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.add_annotation(dict(font=dict(size=12), x=0, y=-0.17, showarrow=False, text=&quot;Source: 2021 Kaggle Machine Learning &amp; Data Science Survey&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show()","link":"/2021/11/09/kgg/Kgg_plotly_HZB(2)/"},{"title":"kaggle :HorizontalBar (Q17)","text":"kaggle dictation (03) plotly.graph_objects as go: 를 이용한 bar graphHorizontalBar plot /가로 막대 차트0. data sethttps://www.kaggle.com/miguelfzzz/the-typical-kaggle-data-scientist-in-2021 Subject : 어떤알고리즘을 선호 하는 지 알아보는 Horizontal bar1. data 읽어오기algorithms_cols에 data frame을 먼저 만들고 시작.잘 모르겠지만 아마도 Q17이 붙은 data를 선택 하기 위한 code algorithms_cols = [col for col in df if col.startswith(‘Q17’)] col 1부터 df 끝까지 Q17로 시작하는지 확인하여 true일때만 데이터 가져오기 ref. How to select dataframe columns that start with *** using pandas in python ? 12algorithms_cols = [col for col in df if col.startswith('Q17')] 2. data Frame 만들어 주기algorithms 에 data frame을 씌워서 표를 만들고, 이름을 다음과같이 바꿔줌. 12345678algorithms = df[algorithms_cols]algorithms.columns = ['Linear or Logistic Regression', 'Decision Trees or Random Forests', 'Gradient Boosting Machines', 'Bayesian Approaches', 'Evolutionary Approaches', 'Dense Neural Networks', 'Convolutional Neural Networks', 'Generative Adversarial Networks', 'Recurrent Neural Networks', 'Transformer Networks', 'None', 'Other'] 3.표 설정.12345678algorithms = ( algorithms .count() .to_frame() .reset_index() .rename(columns={'index':'Algorithms', 0:'Count'}) .sort_values(by=['Count'], ascending=False) ) .count() :coulumn 수 세기 .to_frame() : frame 생성 .reset_index() : 원본과 상관없는 Index 생성 .rename() columns의 이름을 지정 : ‘index’:’Algorithms’, 0:’Count’ .sort_values() by=[‘Count’], ascending=False Count 기준으로 내림차순으로 정렬 3. percent 추가12algorithms['percent'] = ((algorithms['Count'] / len(df))*100).round(2).astype(str) + '%' 표에 ‘percent’를 추가algorithms의 count에 df의 length로 나누고 *100을 하는 전형적인 % 나타내기 값자체에 %를 입력하여 나중에 %를 추가 입력하지 않아도 됨 소숫점 자리 2까지 반영(반올림).type 자체를 String으로 하여 추가 계산은 불가능. 4. 색 지정12345678colors = ['#033351',] * 12colors[0] = '#5abbf9'colors[1] = '#5abbf9'colors[2] = '#066eb0'colors[3] = '#066eb0'colors[4] = '#044a77'colors[5] = '#044a77'colors[6] = '#044a77' 색 깊이는 12단계, 0~6까지는 정해주고 나머지 5개는 #033351을 default로 한 것을 알 수 있다. 5. bar Graph 만들기1234567fig = go.Figure(go.Bar( x=algorithms['Count'], y=algorithms['Algorithms'], text=algorithms['percent'], orientation='h', marker_color=colors )) horizontal과 vertical Graph의 차이는 x, y axis를 바꾸어 주는 것과 orientation=’h’ 을 넣어 주는 것의 차이. &lt;orientation 없음&gt; &lt;orientation H 있음&gt; 6. update_traces()traces() 수정 : Trace에 대한 설정 123456fig.update_traces(texttemplate='%{text}', textposition='outside', cliponaxis = False, hovertemplate='&lt;b&gt;Algorithm&lt;/b&gt;: %{y}&lt;br&gt;&lt;extra&gt;&lt;/extra&gt;'+ '&lt;b&gt;Count&lt;/b&gt;: %{x}', textfont_size=12) texttemplate : text type textposition : ‘outside’ _ 설정 해 주지 않은 경우 칸에 따라 적당히 들어감. cliponaxis = False : text가 칸이 작아서 짤리는 경우를 막아주는 기능 (off) hovertemplate : 마우스 On하면 (커서를 위에 대면) 나오는 Hovert에대한 설정. textfont_size : 폰트 size 7. Design1234567891011121314151617fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.update_layout(showlegend=False, plot_bgcolor='#F7F7F7', margin=dict(pad=20), paper_bgcolor='#F7F7F7', xaxis={'showticklabels': False}, yaxis_title=None, height = 600, xaxis_title=None, yaxis={'categoryorder':'total ascending'}, title_text=&quot;Most Commonly Used &lt;b&gt;Algorithms&lt;/b&gt;&quot;, title_x=0.5, font=dict(family=&quot;Hiragino Kaku Gothic Pro, sans-serif&quot;, size=15, color='#000000'), title_font_size=35) Grid Delete update_layout showlegend=False, plot_bgcolor=’#F7F7F7’ margin=dict(pad=20), paper_bgcolor=’#F7F7F7’, xaxis={‘showticklabels’: False}, x 축 labels을 삭제. yaxis_title=None, xaxis_title=None, yaxis={‘categoryorder’:’total ascending’}, y 축 title을 categoryorder Automatically Sorting Categories by Name or Total Value layout-xaxis-categoryorder title_text=”Most Commonly Used Algorithms“, title_x=0.5, font=dict(family=”Hiragino Kaku Gothic Pro, sans-serif”, size=15, color=’#000000’), title_font_size=35) 이미 앞서서 충분히 설명 했기 때문에 본 posting에서 설명되지 않은 부분은 basic bar Graph 에서 확인 8. Annotation1234567891011121314151617181920fig.add_annotation(dict(font=dict(size=14), x=0.98, y=-0.17, showarrow=False, text=&quot;@miguelfzzz&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.add_annotation(dict(font=dict(size=12), x=0, y=-0.17, showarrow=False, text=&quot;Source: 2021 Kaggle Machine Learning &amp; Data Science Survey&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show()","link":"/2021/11/09/kgg/Kgg_plotly_Horizontal/"},{"title":"kaggle :HorizontalBar (Q7)","text":"kaggle dictation (06) plotly.graph_objects as go: 를 이용한 bar graphHorizontalBar plot /가로 막대 차트0. data sethttps://www.kaggle.com/miguelfzzz/the-typical-kaggle-data-scientist-in-2021 Subject : 가장 많이쓰는 programming 언어_Horizontal bar1. data 읽어오기 Q7에는 sub가 많기 때문에 python 구문을 이용하여‘Q7’ 이 붙어있는 컬럼 불러오기. languages_cols = [col for col in df if col.startswith(‘Q7’)] col 1부터 df 끝까지 Q7로 시작하는지 확인하여 true 일 때만 데이터 가져오기 1languages_cols = [col for col in df if col.startswith('Q7')] 2. data Frame 만들어 주기algorithms 에 data frame을 씌워서 표를 만들고, 이름을 다음과같이 바꿔줌. 12345languages = df[languages_cols]languages.columns = ['Python', 'R', 'SQL', 'C', 'C++', 'Java', 'Javascript', 'Julia', 'Swift', 'Bash', 'MATLAB', 'None', 'Other'] 3.표 설정.12345678languages = ( languages .count() .to_frame() .reset_index() .rename(columns={'index':'Languages', 0:'Count'}) .sort_values(by=['Count'], ascending=False) ) .count() :coulumn 수 세기 .to_frame() : frame 생성 .reset_index() : 원본과 상관없는 Index 생성 .rename() columns의 이름을 지정 : ‘index’:’Languages’, 0:’Count’ .sort_values() by=[‘Count’], ascending=False Count 기준으로 내림차순으로 정렬 3. percent 추가1languages['percent'] = ((languages['Count'] / len(df))*100).round(2).astype(str) + '%' 표에 ‘percent’를 추가algorithms의 count에 df의 length로 나누고 *100을 하는 전형적인 % 나타내기 값자체에 %를 입력하여 나중에 %를 추가 입력하지 않아도 됨 소숫점 자리 2까지 반영(반올림).type 자체를 String으로 하여 추가 계산은 불가능. 4. 색 지정1234567891011colors = ['#033351',] * 13colors[0] = '#5abbf9'colors[1] = '#5abbf9'colors[2] = '#0779c3'colors[3] = '#0779c3'colors[4] = '#0779c3'colors[5] = '#0779c3'colors[6] = '#0779c3'colors[7] = '#05568a'colors[8] = '#05568a'colors[9] = '#05568a' 5. bar Graph 만들기1234567fig = go.Figure(go.Bar( x=algorithms['Count'], y=algorithms['Algorithms'], text=algorithms['percent'], orientation='h', marker_color=colors )) horizontal과 vertical Graph의 차이는 x, y axis를 바꾸어 주는 것과 orientation=’h’ 을 넣어 주는 것의 차이. 6. update_traces()traces() 수정 : Trace에 대한 설정 123456fig.update_traces(texttemplate='%{text}', textposition='outside', cliponaxis = False, hovertemplate='&lt;b&gt;Lenguage&lt;/b&gt;: %{y}&lt;br&gt;&lt;extra&gt;&lt;/extra&gt;'+ '&lt;b&gt;Count&lt;/b&gt;: %{x}', textfont_size=12) texttemplate : text type textposition : ‘outside’ _ 설정 해 주지 않은 경우 칸에 따라 적당히 들어감. cliponaxis = False : text가 칸이 작아서 짤리는 경우를 막아주는 기능 (off) hovertemplate : 마우스 On하면 (커서를 위에 대면) 나오는 Hovert에대한 설정. textfont_size : 폰트 size 7. Design1234567891011121314151617fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.update_layout(showlegend=False, plot_bgcolor='#F7F7F7', margin=dict(pad=20), paper_bgcolor='#F7F7F7', height=700, xaxis={'showticklabels': False}, yaxis_title=None, xaxis_title=None, yaxis={'categoryorder':'total ascending'}, title_text=&quot;Most Commonly Used &lt;b&gt;Programming Languages&lt;/b&gt;&quot;, title_x=0.5, font=dict(family=&quot;Hiragino Kaku Gothic Pro, sans-serif&quot;, size=17, color='#000000'), title_font_size=35) Grid Delete update_layout showlegend=False, plot_bgcolor=’#F7F7F7’ margin=dict(pad=20), paper_bgcolor=’#F7F7F7’, xaxis={‘showticklabels’: False}, x 축 labels을 삭제. yaxis_title=None, xaxis_title=None, yaxis={‘categoryorder’:’total ascending’}, y 축 title을 categoryorder : 정렬 title_text=”Most Commonly Used Algorithms“, title_x=0.5, font=dict(family=”Hiragino Kaku Gothic Pro, sans-serif”, size=15, color=’#000000’), title_font_size=35) 8. Annotation12345678910111213141516171819fig.add_annotation(dict(font=dict(size=14), x=0.98, y=-0.13, showarrow=False, text=&quot;@miguelfzzz&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.add_annotation(dict(font=dict(size=12), x=0, y=-0.13, showarrow=False, text=&quot;Source: 2021 Kaggle Machine Learning &amp; Data Science Survey&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show()","link":"/2021/11/09/kgg/Kgg_plotly_Bar_H(3)/"},{"title":"kaggle :Bar Graph (Q6)","text":"kaggle dictation (02) plotly.graph_objects as go: 를 이용한 bar graph###bar plot /막대 차트 막대그래프는 가장 많이 쓰이는 플롯들중 하나로 숫자 변수와 범주 형 변수 간의 관계를 보여 줍니다. 막대 차트는 종종 히스토그램과 혼동 되기도 하는데(숫자의 분포를 보여줌) 이는 통계학의 부분 그룹당 여러 값이 있는 경우에는 박스플롯이나 바이올린 플롯을 추천 최소한 그룹당 관측치 수와 각 그룹의 신뢰구간은 표시되야함. 사용한 Library12345678import pandas as pdimport numpy as npimport seaborn as snsimport plotly.express as pximport plotly.graph_objects as goimport warningswarnings.filterwarnings('ignore') 사실 이 부분에서 seaborn을 사용 했는지 잘 모르겠음. github에서 plotly가 동적 Livrary라 자꾸 오류가남. data importdata 원문data import 방법 data: Kaggle의 the-typical-kaggle-data-scientist-in-2021 이 부분은 data import 방법 을 참고 하거나kaggle dictation (01) 을 참조하세요. data encoding (Feature Engineering)사실 이 부분이 feature Engineering에 해당하는 부분인지 잘 모르겠다. 이 부분은 data를 computer로 자동화하여 계산, 동적 그래프를 만들기 위한 부분. ###Feature Engineering Experience라는 Question 6에 해당하는 값을 전처리 해 준다. 123456789101112experience = ( df['Q6'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'Experience', 'Q6':'Count'}) .replace(['I have never written code','&lt; 1 years', '1-3 years', '3-5 years', '5-10 years', '10-20 years', '20+ years'], ['No experience', '&lt;1 years', '1-3 years', '3-5 years', '5-10 years', '10-20 years', '20+ years']) ) .value_counts() : 데이터의 분포를 확인하는데 매우 유용한 함수 column 값의 개수를 확인 하는것. 중복되는 값을 묶어줌. .to_frame() : frame을 설정 (표 생성) reset_index() 원본 data를 회손하지 않고 Index를 새로 만듦 .rename(columns={‘index’:’Experience’, ‘Q6’:’Count’}) column에 새로운 이름을 붙여줌 index는 Experience로 Q6은 count로 지정 replace() [왼쪽]에 있는 값대신 [오른쪽]에 있는 값을 넣으려고 함. 이 경우 ‘I have never written code’를 ‘No experience’로 바꾸려고 한듯. Ref. 판다스 함수 df.value_counts() 함수만 사용하면 아래와 같이 나온다. 1df['Q6'].value_counts() 1-3 years 7874&lt; 1 years 58813-5 years 40615-10 years 309910-20 years 216620+ years 1860I have never written code 1032Name: Q6, dtype: int64 이는 문자형 data를 분석하여 display하기 위한 방법 data categoircal로 List로 만들고, 함수정의, 정렬(#1) : Pandas lib의 categories function문자열 객체의 배열을 series로 변환하여 범주형으로 변환 1234567#1experience['Experience'] = pd.Categorical( experience['Experience'], ['No experience', '&lt;1 years', '1-3 years', '3-5 years', '5-10 years', '10-20 years', '20+ years'] ) pd.categories (#2) : experience[‘percent’]Experience에 하나의 tap을 추가 해준다.SQL 의 insert 에 percent tep을 만들어 줄 때 column에대해 계산하여 값을 보여주는 것과 같은 느낌. value에 들어갈 수식 지정_ Experience의 percent 계산 123 #2experience['percent'] = ((experience['Count'] / experience['Count'].sum())*100).round(2).astype(str) + '%' (#3) : experience.sort_values()데이터 정렬하기 : 컬럼의 data를 기준으로 정렬 - short_Index의 경우에는 Index를 기준으로 data를 정렬한다. - 이 경우에는 (‘Experience’)를 기준으로 default값인 오름차순으로 정렬 보통 percent나 count를 기준으로 정렬 되는데 이 경우 sort_values(‘Experience’) 를 하였기때문에기준인 Experience를 기준으로 오름차순으로 정렬 되었다. Short_value 123#3experience = experience.sort_values('Experience') (#4) : colors바chart의 color을 설정 *7 은 7개의 수준이 있다는 것. colors[N] = 뭘까 chart color 123456789#4colors = ['#033351',] * 7colors[1] = '#5abbf9'colors[2] = '#5abbf9'colors[3] = '#0779c3'colors[4] = '#0779c3' (#5) : fig = go.Figure(go.Bar())import plotly.graph_objects as go 이기때문에 fig는 plotly library 함수 사용 12345678#5fig = go.Figure(go.Bar( y=experience['Count'], x=experience['Experience'], cliponaxis = False, text=experience['percent'], marker_color=colors )) x축, y축 정해주기 y=experience[‘Count’], x=experience[‘Experience’], cliponaxis cliponaxis = False, cliponaxis – Text node를 아래 축에 고정 할지 아닐지 결정text node를 축 라인과 체크라벨 위에 보여주기위해서는 x축Layer와 y축 layer 설정을 해 주어야 한다. text=experience[‘percent’] marker_color=colors 지정 해 준 colors를 사용. (#6) : fig.update_traces()그래프 위에 캡션 다는 기능 Perform a property update operation on all traces that satisfy the specified selection criteria 지정된 선택 기준을 충족하는 모든 추적에 대해 속성 업데이트 작업 수행 (?? 전혀 모르겠군 !) 아래 본문과는 상관없는 data를 좀 보세요. 12345678910111213141516df1 = df['Q1'].value_counts()df2 = df['Q1'].value_counts(normalize = True)val_pnt_df = pd.DataFrame({&quot;count&quot;:df1,&quot;%&quot;:df2*100})fig = go.Figure()# [str(x) + ' %' for x in np.round(val_pnt_df[&quot;%&quot;].values, 1).tolist()]# Add Tracesfig.add_bar(x = val_pnt_df.index, y = val_pnt_df['count'].values, text = [str(x) + ' %' for x in np.round(val_pnt_df[&quot;%&quot;].values, 1).tolist()], textposition=&quot;auto&quot;)# layoutfig.update_layout(title_text = &quot;Q1. &quot; + questions[1])fig.show() python의 comprehension 오늘은 못보고 나중에 다시 보자 ! 123456#6fig.update_traces(texttemplate='%{text}', textposition='outside', hovertemplate='&lt;b&gt;Experience&lt;/b&gt;: %{x}&lt;br&gt;&lt;extra&gt;&lt;/extra&gt;'+ '&lt;b&gt;Count&lt;/b&gt;: %{y}', textfont_size=12) texttemplate=’%{text}’, : text type textposition=’outside’, : inside하면 그래프 안쪽, ouside 하면 그래프 위쪽에 생성 hovertemplate= 커서를 가까이 대면 나오는 창 x값과, y 값이 어떤 상태인지 알려 준다. hevertemplate Returns the Figure object that the method was called on 메서드가 호출된 그림객체를 반환. plotly.graph_objects.Figure.update_traces 123#7fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False) (#7) : fig.update_?axes()만들어진 fig를 수정.SQL의 update와 비슷한 기능인듯. fig.update_xaxes(showgrid=False) : x축의 grid 수정fig.update_yaxes(showgrid=False) : y축의 grid 수정 축을 보이지 않는 형태로 바꾸어 예쁘게 보이게 해줌. (#8) : update_layout()1234567891011121314#8fig.update_layout(showlegend=False, plot_bgcolor='#F7F7F7', margin=dict(pad=20), paper_bgcolor='#F7F7F7', height=500, yaxis={'showticklabels': False}, yaxis_title=None, xaxis_title=None, title_text=&quot;&lt;b&gt;Experience&lt;/b&gt; Distribution&quot;, title_x=0.5, font=dict(family=&quot;Hiragino Kaku Gothic Pro, sans-serif&quot;, size=14, color='#000000'), title_font_size=35) default로 되어있는 그래프의 Layout을 수정. showlegend = False 래전드를 보여줄지 : 안보여줌 plot_bgcolor=’#F7F7F7’ margin=dict(pad=20)-dic에는 여러가지가 올 수 있는데 여기서는 dict(pad)를 사용 padding을 설정, 축과 그래프 사이의 패딩을 px 단위로 설정 Sets the amount of padding (in px) between the plotting area and the axis lines layout-margin paper_bgcolor=’#F7F7F7’ 배경색 설정 height=500 plot size 설정 yaxis={‘showticklabels’: False} y축의 showticklabels 설정 : 안함 yaxis_title=None, xaxis_title=None y축 제목, x축 제목 설정 : 없음 title_text=”Most Recommended Programming Language“ 제목 달기 &lt;b&gt; code는 bolde tag인듯. title_x=0.5, title_y=0.95, 제목의 위치 (상단 고정) font=dict(family=”Hiragino Kaku Gothic Pro, sans-serif”, size=17, color=’#000000’), title_font_size=35) title의 font 설정 (Default: “”Open Sans”, verdana, arial, sans-serif”) font 설정 family, color, size 설정 가능, title_fond도 함께 설정 가능 해 보임. plotly.graph_objects.Figure.update_layout (#9) : add_annotation()annotation의 경우 plot 안에 글을 집어 넣는 것. 설명을 추가 해 준다고 생각하면 쉽다. 어렵지도 않고, 같은 내용이므로 이전 posting을 첨부 annotation 12345678910111213141516171819fig.add_annotation(dict(font=dict(size=14), x=0.98, y=-0.24, showarrow=False, text=&quot;@miguelfzzz&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.add_annotation(dict(font=dict(size=12), x=-0.03, y=-0.24, showarrow=False, text=&quot;Source: 2021 Kaggle Machine Learning &amp; Data Science Survey&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() ref. plotly.graph_object Parameter var chart in plotly/en","link":"/2021/11/08/kgg/Kgg_plotly_bar/"},{"title":"kaggle :ScatterLine (Q42)","text":"kaggle dictation (08) plotly.graph_objects as go_ Scatter + line plot산점도 bivariate”이변수” 값을 시각화 하는 기본적인 그래프. correlation: Positive, Negative, non 두 개의 변수 각각의 분포과 변수간의 관계를 확인 할 수 있다. ref. line-and-scatter/En. Q11_Scatter 0. data sethttps://www.kaggle.com/miguelfzzz/the-typical-kaggle-data-scientist-in-2021 Subject : 가장 많이 이용하는 Media source1. data 읽어오기Q42로 시작하는 col을 읽어오기.python의 for문을 이용. 1media_cols = [col for col in df if col.startswith('Q42')] 2. data Frame 만들어 주기123456media = df[media_cols]media.columns = ['Twitter', 'Email newsletters', 'Reddit', 'Kaggle', 'Course Forums', 'YouTube', 'Podcasts', 'Blogs', 'Journal Publications', 'Slack Communities', 'None', 'Other'] 3.표 설정.12345678910media = ( media .count() .to_frame() .reset_index() .rename(columns={'index':'Medias', 0:'Count'}) .sort_values(by=['Count'], ascending=False) )media 4. 색 지정12345678910colors = ['#033351',] * 12colors[11] = '#5abbf9'colors[10] = '#5abbf9'colors[9] = '#5abbf9'colors[8] = '#0779c3'colors[7] = '#0779c3'colors[6] = '#0779c3'colors[5] = '#0779c3'colors[4] = '#0779c3' 5. percent로 계산한 column 추가i. add percent column 123media['percent'] = ((media['Count'] / len(df))*100).round(2).astype(str) + '%'media ii. Count값 (column값으로 ) 정렬 123456media = (media .sort_values(by = ['Count']) .iloc[0:15] .reset_index())media 1. Default는 내림차순 2. iloc으로 0번부터 15까지 List로 긁어오기 3. reset index() 6.plotly.graph_objects.Scatter()Scatter G 그리기 i. 산점도 점 찍기 12345678fig = go.Figure(go.Scatter(x = media['Count'], y = media[&quot;Medias&quot;], text = media['percent'], mode = 'markers', marker_color =colors, marker_size = 12))fig ii. 산점도에 for문을 이용하여 line 연결하기 1234567for i in range(0, len(media)): fig.add_shape(type='line', x0 = 0, y0 = i, x1 = media[&quot;Count&quot;][i], y1 = i, line=dict(color=colors[i], width = 4)) fig for i in range(0~platform의 길이만큼) fig. add_shape() type = ‘line’ line모양의 grape shape add x0 = 0, y0 = i, 초기값 (0, i)에서 시작 (0, 0) = other Line Start x1 = platform[“Count”][i], x축 Index : count의 값만큼 x축방향으로 Line이 그어진다. y1 = i, y축 Index, 마지막 값 line=dict(color=colors[i], width = 4) line의 세부 설정, 색과 두께 7. update_traces(hovertemplate)123fig.update_traces(hovertemplate='&lt;b&gt;Media Source&lt;/b&gt;: %{y}&lt;br&gt;&lt;extra&gt;&lt;/extra&gt;'+ '&lt;b&gt;Count&lt;/b&gt;: %{x}&lt;br&gt;'+ '&lt;b&gt;Proportion&lt;/b&gt;: %{text}') 8. Designi. 축 grid 12fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='#9f9f9f', ticklabelmode='period')fig.update_yaxes(showgrid=False) x 축의 grid만 보여줌. tick labe lmode : period plotly의 axes/En ii. update_layout() 123456789101112fig.update_layout(showlegend=False, plot_bgcolor='#F7F7F7', margin=dict(pad=20), paper_bgcolor='#F7F7F7', yaxis_title=None, xaxis_title=None, title_text=&quot;Most Commonly Used &lt;b&gt;Media Sources&lt;/b&gt;&quot;, title_x=0.5, height=700, font=dict(family=&quot;Hiragino Kaku Gothic Pro, sans-serif&quot;, size=17, color='#000000'), title_font_size=35) 9. Annotation12345678910111213141516171819fig.add_annotation(dict(font=dict(size=14), x=0.98, y=-0.22, showarrow=False, text=&quot;@miguelfzzz&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.add_annotation(dict(font=dict(size=12), x=0.04, y=-0.22, showarrow=False, text=&quot;Source: 2021 Kaggle Machine Learning &amp; Data Science Survey&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show()","link":"/2021/11/09/kgg/Kgg_plotly_ScatterL(2)/"},{"title":"kaggle :Plotly_Treemap (Q8)","text":"kaggle dictation (01) plotly.graph_objects as go : 를 이용한 Treemap 많은 계층 구조 데이트를 표현할때 적합. Ben Shneiderman에 의해 1990년도부터 출발 Treemap은 크기(count) 순서로 %에따라 공간을 차지하는 사각형으로 표현됨. 계층에서 동일한 수준에 속하는 각 사각형 집합은 데이터 테이블의 표현식 또는 컬럼을 표현. 계층에서 동일한 수준의 개별 사각형은 컬럼의 Index 언제 사용하면 좋을까- 많은 범주간의 부분과 전체를 시각화 하고 싶을때 - 범주 간의 정확한 비교 대신 큰 특징을 나타내고 싶을때 - 데이터가 계층을 이루고 있을때 사용한 Library12345678import pandas as pdimport numpy as npimport seaborn as snsimport plotly.express as pximport plotly.graph_objects as goimport warningswarnings.filterwarnings('ignore') 사실 이 부분에서 seaborn을 사용 했는지 잘 모르겠음. github에서 plotly가 동적 Livrary라 자꾸 오류가남. data importdata 원문data import 방법data는 Kaggle의 the-typical-kaggle-data-scientist-in-2021 data를 이용하였음. data 불러오기/합치기data를 표현 해 주기 위해 컴퓨터가 읽을 수 있는 형태로 가공. 12df = pd.read_csv('../input/kaggle-survey-2021/kaggle_survey_2021_responses.csv')df = df.iloc[1:, :] df : data frame에 file 연동.df.iloc[1:, :] : 1행부터 끝까지 건너뛰기 없이 선택 행번호(row number)로 선택하는 방법 (.iloc) label이나 조건표현으로 선택하는 방법 (.loc) Ref. loc, iloc을 이용한 행 선택 data encoding (Feature Engineering)사실 이 부분이 feature Engineering에 해당하는 부분인지 잘 모르겠다. 이 부분은 data를 computer로 자동화하여 계산, 동적 그래프를 만들기 위한 부분. ###Feature Engineering####data frame 설정 123456789101112recommend_leng = ( df['Q8'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'Lenguage', 'Q8':'Count'}) .sort_values(by=['Count'], ascending=False) )df['Q8'].head()recommend_leng dataframe[Q8] : data를 가공하여 분석 할 예정 pd.value_counts() Q8의 data counting, 중복된 data 를 counting 하여 수를 나타냄. pd.frame() dataframe으로 표의 형태를 잡아준다. pd.reset_Index() Q8의 Index reset, 원본 data에는 영향을 주지 않으면서 새로운 Index 생성 Reset_parameter/Ko pd.rename() columns에 Index를 붙여 호출 하기 위해 이름을 바꿔줌 index는 Lenguage로, count는 Q8로 pd.sort_values() count 값으로 정렬 by=[‘Count’], ascending = false by에 option 기준, (오름차순 = F )= 내림차순으로 정렬 앞으로 사용 할 색상을 미리 지정 한 눈에 보기 편함. 123456colors = ['#033351',] * 13colors[0] = '#5abbf9'colors[1] = '#066eb0'colors[2] = '#044a77'colors[3] = '#043e64'colors[4] = '#043e64' 아직 잘 모르겠는 부분은 왜 colors = [‘#NNNNNN’, ] 이 부분과 *13 이부분 Treemap 생성123456fig = go.Figure(go.Treemap( labels = recommend_leng['Lenguage'], values = recommend_leng['Count'], parents = ['']*recommend_leng.shape[0], textinfo = &quot;percent root+label+value+text&quot;, )) fig 생성 import plotly.graph_objects as goplotly의 graph_objects 를 이용하여 객체 생성. Treemap의 parameter labels : values 이름 values : 값 parents : ??? textinfo = 표시형식 fig.update_traces(hovertemplate)12fig.update_traces(hovertemplate='&lt;b&gt;Lenguage&lt;/b&gt;: %{label}&lt;br&gt;&lt;extra&gt;&lt;/extra&gt;'+ '&lt;b&gt;Count&lt;/b&gt;: %{value}') fig.update_traces() 그래프 위에 캡션 다는 기능 Perform a property update operation on all traces that satisfy the specified selection criteria 지정된 선택 기준을 충족하는 모든 추적에 대해 속성 업데이트 작업 수행 (?? 전혀 모르겠군 !) hoverinfo : 마우스 오버시 나타나는 추적정보 hovertemplate : 커서를 가까이 대면 나오는 창을 렌더링하는데 사용되는 Temp 변수 : %{variable} (변수의 형식을 지정) 숫자 : %{d3-format} price : %{yL$.2f} hovertemplate/en hoverTemp.para/ko fig.update_layout()fig의 layout을 설정. hoverTemp까지 설정된 treemap. fig.update_layout()을 사용하여 layout을 변경 해 보자. 1234567891011121314fig.update_layout(showlegend=False, treemapcolorway = colors, margin=dict(pad=20), paper_bgcolor='#F7F7F7', plot_bgcolor='#F7F7F7', height=600, yaxis={'showticklabels': False}, yaxis_title=None, xaxis_title=None, title_text=&quot;Most Recommended &lt;b&gt;Programming Language&lt;/b&gt;&quot;, title_x=0.5, title_y=0.95, font=dict(family=&quot;Hiragino Kaku Gothic Pro, sans-serif&quot;, size=17, color='#000000'), title_font_size=35) showlegend = False 래전드를 보여줄지 : 안보여줌 treemapcolorway = colors 위에서 정의 해 준 colors가 13개였는데 여기 계층도 13개임 아마도 light 부터 deep으로 색이 정해지는듯. margin=dict(pad=20)-dic에는 여러가지가 올 수 있는데 여기서는 dict(pad)를 사용 padding을 설정, 축과 그래프 사이의 패딩을 px 단위로 설정 Sets the amount of padding (in px) between the plotting area and the axis lines layout-margin paper_bgcolor=’#F7F7F7’ 배경색 설정 plot_bgcolor=’#F7F7F7’ 설정 바꿔 보았으나 안보임 height=600 plot size 설정 yaxis={‘showticklabels’: False} y축의 showticklabels 설정 : 안함 yaxis_title=None, xaxis_title=None y축 제목, x축 제목 설정 : 없음 title_text=”Most Recommended Programming Language“ 제목 달기 &lt;b&gt; code는 bolde tag인듯. title_x=0.5, title_y=0.95, 제목의 위치 (상단 고정) font=dict(family=”Hiragino Kaku Gothic Pro, sans-serif”, size=17, color=’#000000’), title_font_size=35) title의 font 설정 (Default: “”Open Sans”, verdana, arial, sans-serif”) font 설정 family, color, size 설정 가능, title_fond도 함께 설정 가능 해 보임. fig Information 추가fig.add_annotation() _1플롯에 메모를 남길 수 있는것. 코멘트나 copy-Right 같은걸 남기는듯 -[plotly-annotation/ko.] (https://soohee410.github.io/plotly_annotation) 12345678fig.add_annotation(dict(font=dict(size=14), x=0.96, y=-0.14, showarrow=False, text=&quot;@miguelfzzz&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;)) dict = dictionary A list or tuple of dicts of string/value properties where:The ‘type’ property specifies the trace type 여기서 dict는 fig객체 즉 plotly.graph_objects.Figure 에서 상응하는 정보(font, size등) 변수를 가져와 대응 시켜 주는 역할을 한다. plot 아래에 보면 “@miguelfzzz”이라는 글자가 보이는데 이것을 설정 한 것. showarrow=False, 화살표등을 남길 수 있는데 이 Graph에선 false xref=”paper”, yref=”paper” 어느 부분 (plot or paper)에 표시 할 것인지 나머지는 말 안해도 이제는 알 수 있기 때문에 생략. fig.add_annotation() _212345678fig.add_annotation(dict(font=dict(size=12), x=0.01, y=-0.14, showarrow=False, text=&quot;Source: 2021 Kaggle Machine Learning &amp; Data Science Survey&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;)) fig.show() 내보내기1fig.show() fig.show()로 마무리 해 주면 된다. 이건 java의 return과 같은 느낌인듯. Plotly Treemap/en. layout/en.","link":"/2021/11/08/kgg/Kgg_plotly_treemap/"},{"title":"kaggle :Donut Chart Graph (Q2)","text":"","link":"/2021/11/09/kgg/Kgg_plotly_pie/"},{"title":"kaggle study","text":"Sumarry Demographics &amp; Geographics (Q1) Age-bar (Q2) Gender-pie (Q3)countries-scatter+line Education &amp; Occupation (Q4) Age-bar_h (Q5) Role-bar_h (Q20) Industry-bar_h knowledge &amp; skills (Q6) Experience : 52% 넘는 응답이 3년이상 코딩과 프로그래밍을 했다. -bar vertical (위) (Q17) Algorithms : Linear or Logistic Regression 55% 과 Decision tree or Random Forests, respectively 66% 사용. - bar horizon (옆) (Q7) Languages : python 84%, SQL 41% 사용 -bar horizon (Q8) Recommend_Leng: programming에 추천하는 언어는 81%가 python -Treemap (Q9) F_EG /w Q7 가장 많이 쓰는 IDE : jupyter Notebook26.2%, VSCODE 13.92% - bar horizon Platforms &amp; Media4. Platforms &amp; Media (Q11) Platform: 많이 쓰는 컴퓨터 플랫폼은 랩탑이 66% -Scatter + line (Q27_A)cloud_platform: 아마존 14%, 구글클라우드 12%, 마쏘 아줠 9% - bar horizon (Q40) courses: DS들이 많이 쓰는 course 플랫폼 Coursera 20, Kaggle 18%-Treemap (Q42) media : DS topic report를 위해 많이 쓰는 media는 kaggle 44%, youTube 40%, blog 31% -scatter + line Takeaways : typical Kaggle DS in 2021 :HTML picture n line n text 123456789101112131415161718# This Python 3 environment, PKG Loadimport numpy as np # linear algebraimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)# Input data files : &quot;../input/&quot; # Running : Shift+Enterimport osfor dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename))# ~ 20GB ,/kaggle/temp/ /kaggle/input/kaggle-survey-2021/kaggle_survey_2021_responses.csv /kaggle/input/kaggle-survey-2021/supplementary_data/kaggle_survey_2021_methodology.pdf /kaggle/input/kaggle-survey-2021/supplementary_data/kaggle_survey_2021_answer_choices.pdf The Typical Kaggle Data Scientist in 2021 Remade By @YoonHwa-P HTML code는 Markdown 형식으로 넣을 수 있게 해 준다. scr : 구글 팟케스트에서 바로 연결하여 사용. 1234567891011import pandas as pdimport numpy as npimport seaborn as snsimport plotly.express as pximport plotly.graph_objects as goimport warningswarnings.filterwarnings('ignore') 시각화, 계산을 위해 Pandas, Numpy, seaborn을 이용 할 것이고, 동적보드를 만들기 위해 plotly를 이용 하였다. plotly 중에서 Express와 Graph_objects를 가져와서 사용 할 예정인듯. plotly-express The Plotly Express API in general offers the following features: Every PX function returns a** plotly.graph_objects.Figure object**, so you can edit it using all the same methods like update_layout and add_trace. input으로 Express를 사용 한다면 Graph_objects가 동적 plotly 를 만드는 것 같다. : update 하거나 trace를 가능 하게 하는듯. 실제 필사할 data에서는 어떤 data가 있는지 확인 해 보지 않았지만,나는 배우는 입장이니 어떤 data가 있는지, 어떤 head가 있는지 확인 해 보도록 한다. 맨 위에 가보면 Note가 생성 될때 /kaggle/input/kaggle-survey-2021/kaggle_survey_2021_responses.csv /kaggle/input/kaggle-survey-2021/supplementary_data/kaggle_survey_2021_methodology.pdf /kaggle/input/kaggle-survey-2021/supplementary_data/kaggle_survey_2021_answer_choices.pdf 위와같은 file dir을 알려준다. 이제, pandas로 이 files를 로딩 시켜 주면된다. Ref. Kaggle활용.국문 12345678df = pd.read_csv(&quot;../input/kaggle-survey-2021/kaggle_survey_2021_responses.csv&quot;)df = df.iloc[1:, :] #이건 왜 선택 해 놓은 것일까요?df.head()df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 25973 entries, 1 to 25973 Columns: 369 entries, Time from Start to Finish (seconds) to Q38_B_OTHER dtypes: object(369) memory usage: 73.1+ MB df에 pd.read_csv로 csv file을 읽어 옵니다. 역시 pd인 df객체에 iloc을 이용하여 [ 1행부터 : , : ] iloc를 선택 해 놓았다. 행번호(row number)로 선택하는 방법 (.iloc) label이나 조건표현으로 선택하는 방법 (.loc) Ref. loc를 이용한 행 선택 0. Introduction This notebook will explore the fascinating results obtained from the survey conducted by Kaggle in September 2021. Over 25,000 data scientists and ML engineers participated, providing information on their backgrounds and experience in their occupations. To increase readability, this report is divided into four sections: Demographics & Geographics Education & Occupation Knowledge & Skills Platforms & Media Introduction이 노트북은 25000 data scientist들과 ML Engineer들의 kaggle에서 경험 한것을 조사한 data를 매력적인 결과로 탐험 하게 될 것이다.(대충) ; Introduction 에서 이 notebook의 성격, data의 간간한 정보, 목차 등을 설명. 모든 글은 Markdown을 이용한 css 로 작성 된 것 같다. 혹시,css에 대하여 더 알아보고 싶으면, 이 문서를 참조 하자.","link":"/2021/11/07/kgg/kaggle_CC/"},{"title":"Scatter with bar","text":"#Kgg 대회 준비 Scatter와 bar가 함께 있는 Graph를 그려 보았다.1234567891011121314151617181920212223242526272829fig = go.Figure()y=[len(df17_Ea),len(df18_Ea), len(df19_Ea),len(df20_Ea),len(df21_Ea)]fig.add_trace(go.Bar(x=years, y=y, base=0, marker_color='#D9946C', yaxis = &quot;y1&quot;, name='East Asia', text= percent, texttemplate='%{text} %', textposition='outside', hovertemplate='&lt;b&gt;KaggleUser&lt;/b&gt;: %{x}&lt;br&gt;'+ '&lt;b&gt;Count&lt;/b&gt;: %{y}', textfont_size=14 ))fig.add_trace( go.Scatter(name = &quot;World&quot;, x=years, y=[len(df17), len(df18), len(df19), len(df20), len(df21)], marker_color='#88BFBA', mode = 'lines+markers', # please check option here yaxis = &quot;y2&quot;))fig.update_layout(yaxis = dict(title = &quot;KaggleUser in World&quot;, showgrid = False, range=[0, len(df21_Ea)*1.2]), yaxis2 = dict(title = &quot;KaggleUser in East Asia&quot;, overlaying = &quot;y1&quot;, side = &quot;right&quot;, showgrid = False, zeroline = False, range=[0, len(df21)*1.2]), # This code solves the different zero set but with same zero values. template = &quot;plotly_white&quot;, height=500, width=700,)fig.show() age data , 연도별로12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394#age data 전처리# age 뽑아오기Age21_W = df21.loc[:,['Q3','Q1', 'year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'age'}).fillna('etc')Age20_W = df20.loc[:,['Q3','Q1','year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'age'}).fillna('etc')Age19_W = df19.loc[:,['Q3','Q1','year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'age'}).fillna('etc')Age18_W = df18.loc[:,['Q3','Q2','year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q2':'age'}).fillna('etc')#data 정제(한꺼번에 이름바꾸기)Age5y_W= pd.concat([Age21_W, Age20_W, Age19_W, Age18_W])Age5y_W= (Age5y_W.replace(['60-69', '70+', '70-79', '80+'], '60+') .replace(['22-24', '25-29'], '22-29') .replace(['30-34', '35-39'], '30-39') .replace(['40-44', '45-49'], '40-49') .replace(['50-54', '55-59'], '50-59') .groupby(['year', 'age']) .size() .reset_index() .rename(columns = {0:&quot;Count&quot;}))#percent data 넣기Age21_percent_W = Age5y_W[Age5y_W['year'] == &quot;2021&quot;].reset_index(drop = True)Age21_percent_W['percentage'] = Age21_percent_W[&quot;Count&quot;] / Age21_percent_W[&quot;Count&quot;].sum()Age21_percent_W['%'] = np.round(Age21_percent_W['percentage'] * 100, 1)Age20_percent_W = Age5y_W[Age5y_W['year'] == &quot;2020&quot;].reset_index(drop = True)Age20_percent_W['percentage'] = Age20_percent_W[&quot;Count&quot;] / Age20_percent_W[&quot;Count&quot;].sum()Age20_percent_W['%'] = np.round(Age20_percent_W['percentage'] * 100, 1)Age19_percent_W = Age5y_W[Age5y_W['year'] == &quot;2019&quot;].reset_index(drop = True)Age19_percent_W['percentage'] = Age19_percent_W[&quot;Count&quot;] / Age19_percent_W[&quot;Count&quot;].sum()Age19_percent_W['%'] = np.round(Age19_percent_W['percentage'] * 100, 1)Age18_percent_W = Age5y_W[Age5y_W['year'] == &quot;2018&quot;].reset_index(drop = True)Age18_percent_W['percentage'] = Age18_percent_W[&quot;Count&quot;] / Age18_percent_W[&quot;Count&quot;].sum()Age18_percent_W['%'] = np.round(Age18_percent_W['percentage'] * 100, 1)#data 완성Age5y_percent_W = pd.concat([Age18_percent_W, Age19_percent_W, Age20_percent_W, Age21_percent_W], ignore_index = True)Age5y_percent_W= pd.pivot(Age5y_percent_W, index = &quot;year&quot;, columns = 'age', values = &quot;%&quot;).reset_index()Age5y_percent_W#age data 전처리# age 뽑아오기Age21 = df21_Ea.loc[:,['Q3','Q1', 'year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'age'}).fillna('etc')Age20 = df20_Ea.loc[:,['Q3','Q1','year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'age'}).fillna('etc')Age19 = df19_Ea.loc[:,['Q3','Q1','year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'age'}).fillna('etc')Age18 = df18_Ea.loc[:,['Q3','Q2','year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q2':'age'}).fillna('etc')#data 정제(한꺼번에 이름바꾸기)Age5y= pd.concat([Age21, Age20, Age19, Age18])Age5y= (Age5y.replace(['60-69', '70+', '70-79', '80+'], '60+') .replace(['22-24', '25-29'], '22-29') .replace(['30-34', '35-39'], '30-39') .replace(['40-44', '45-49'], '40-49') .replace(['50-54', '55-59'], '50-59') .groupby(['year', 'age']) .size() .reset_index() .rename(columns = {0:&quot;Count&quot;}))#percent data 넣기Age21_percent = Age5y[Age5y['year'] == &quot;2021&quot;].reset_index(drop = True)Age21_percent['percentage'] = Age21_percent[&quot;Count&quot;] / Age21_percent[&quot;Count&quot;].sum()Age21_percent['%'] = np.round(Age21_percent['percentage'] * 100, 1)Age21_percentAge20_percent = Age5y[Age5y['year'] == &quot;2020&quot;].reset_index(drop = True)Age20_percent['percentage'] = Age20_percent[&quot;Count&quot;] / Age20_percent[&quot;Count&quot;].sum()Age20_percent['%'] = np.round(Age20_percent['percentage'] * 100, 1)Age20_percentAge19_percent = Age5y[Age5y['year'] == &quot;2019&quot;].reset_index(drop = True)Age19_percent['percentage'] = Age19_percent[&quot;Count&quot;] / Age19_percent[&quot;Count&quot;].sum()Age19_percent['%'] = np.round(Age19_percent['percentage'] * 100, 1)Age19_percentAge18_percent = Age5y[Age5y['year'] == &quot;2018&quot;].reset_index(drop = True)Age18_percent['percentage'] = Age18_percent[&quot;Count&quot;] / Age18_percent[&quot;Count&quot;].sum()Age18_percent['%'] = np.round(Age18_percent['percentage'] * 100, 1)Age18_percent#data 완성Age5y_percent = pd.concat([Age18_percent, Age19_percent, Age20_percent, Age21_percent], ignore_index = True)Age5y_percent= pd.pivot(Age5y_percent, index = &quot;year&quot;, columns = 'age', values = &quot;%&quot;).reset_index()Age5y_percent Graph 그리기 _1_World12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364Age5y_percent_order = Age5y_percent_W['year'].tolist()Age5y_order = Age5y_W['age'].unique().tolist()fig = go.Figure()fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent_W['18-21'].tolist(), mode = &quot;lines&quot;, name = '18-21', line = dict(width = 1), stackgroup = &quot;one&quot;))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent_W['22-29'].tolist(), mode = &quot;lines&quot;, name = &quot;20s&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent_W['30-39'].tolist(), mode = &quot;lines&quot;, name = &quot;30s&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent_W['40-49'].tolist(), mode = &quot;lines&quot;, name = &quot;40s&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent_W['50-59'].tolist(), mode = &quot;lines&quot;, name = &quot;50s&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent_W['60+'].tolist(), mode = &quot;lines&quot;, name = &quot;60s&lt;&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;))fig.update_layout(yaxis_range = (0, 100), height=500, width=600, title_text=&quot;&lt;b&gt;in world 나이의 변화&lt;/b&gt;&quot;, title_x=0.5)fig.show() Graph 그리기 _1_in East Asia12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364Age5y_percent_order = Age5y_percent['year'].tolist()Age5y_order = Age5y['age'].unique().tolist()fig = go.Figure()fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent['18-21'].tolist(), mode = &quot;lines&quot;, name = '18-21', line = dict(width = 1), stackgroup = &quot;one&quot;))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent['22-29'].tolist(), mode = &quot;lines&quot;, name = &quot;20s&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent['30-39'].tolist(), mode = &quot;lines&quot;, name = &quot;30s&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent['40-49'].tolist(), mode = &quot;lines&quot;, name = &quot;40s&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent['50-59'].tolist(), mode = &quot;lines&quot;, name = &quot;50s&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent['60+'].tolist(), mode = &quot;lines&quot;, name = &quot;60s&lt;&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;))fig.update_layout(yaxis_range = (0, 100), height=500, width=600, title_text=&quot;&lt;b&gt;East Asia 나이의 변화&lt;/b&gt;&quot;, title_x=0.5)fig.show() 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# 연도별 나이 df21Age_Ea = df21_Ea.loc[:,['Q3','Q1']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'2021'}).fillna('etc')# 연령-지역 %dfKo_Age21= df21Age_Ea[df21Age_Ea['East_Asia']=='South Korea']dfKo_Age21_per=dfKo_Age21['2021'].value_counts().to_frame().reset_index()dfKo_Age21_per['South Korea']=((dfKo_Age21_per['2021'] / len(dfKo_Age21))*100).round(2)dfTw_Age21= df21Age_Ea[df21Age_Ea['East_Asia']=='Taiwan']dfTw_Age21_per=dfTw_Age21['2021'].value_counts().to_frame().reset_index()dfTw_Age21_per['Taiwan']=((dfTw_Age21_per['2021'] / len(dfTw_Age21))*100).round(2)dfTw_Age21_perdfCh_Age21= df21Age_Ea[df21Age_Ea['East_Asia']=='China']dfCh_Age21_per=dfCh_Age21['2021'].value_counts().to_frame().reset_index()dfCh_Age21_per['China']=((dfCh_Age21_per['2021'] / len(dfCh_Age21))*100).round(2)dfCh_Age21_perdf21Age_Ea.head()dfJp_Age21= df21Age_Ea[df21Age_Ea['East_Asia']=='Japan']dfJp_Age21_per=dfJp_Age21['2021'].value_counts().to_frame().reset_index()dfJp_Age21_per['Japan']=((dfJp_Age21_per['2021'] / len(dfJp_Age21))*100).round(2)dfJp_Age21_per#g 그리기(heatMap)merge1= pd.merge(dfKo_Age21_per,dfTw_Age21_per, on='index', how='outer')merge2= pd.merge(dfCh_Age21_per,dfJp_Age21_per, on='index', how='outer')merge= pd.merge(merge1,merge2, on='index', how='outer').fillna(0).sort_values(by=['index'],ascending=True)merge.iloc[:,[2,4,6,8]]merge.iloc[:,[2,4,6,8]].to_numpy()fig = go.Figure(data=go.Heatmap( z=merge.iloc[:,[2,4,6,8]].to_numpy(), x=['South Korea','Taiwan','China','Japan'], y=merge.sort_values(by=['index'],ascending=True)['index'].tolist(), hoverongaps = False, opacity=1.0, xgap=2.5, ygap=2.5, colorscale='orrd'), )fig.update_layout( height=500, width=600, title_text=&quot;&lt;b&gt;East Asia 나이 2021&lt;/b&gt;&quot;, title_x=0.5)fig.show() 123456789101112131415161718192021222324252627282930313233343536#데이터 전처리df21_Ea_degree=(df21_Ea['Q4'].replace(['No formal education past high school', 'Some college/university study without earning a bachelor’s degree'],'~college') .replace(['Doctoral degree', 'Professional doctorate'],'Doctoral degree~').value_counts().to_frame().rename(columns={'Q4':'2021'}))df20_Ea_degree=(df20_Ea['Q4'].replace(['No formal education past high school', 'Some college/university study without earning a bachelor’s degree'],'~college') .replace(['Doctoral degree', 'Professional doctorate'],'Doctoral degree~').value_counts().to_frame().rename(columns={'Q4':'2020'}))df19_Ea_degree=(df19_Ea['Q4'].replace(['No formal education past high school', 'Some college/university study without earning a bachelor’s degree'],'~college') .replace(['Doctoral degree', 'Professional doctorate'],'Doctoral degree~').value_counts().to_frame().rename(columns={'Q4':'2019'}))df18_Ea_degree=(df18_Ea['Q4'].replace(['No formal education past high school', 'Some college/university study without earning a bachelor’s degree'],'~college') .replace(['Doctoral degree', 'Professional doctorate'],'Doctoral degree~').value_counts().to_frame().rename(columns={'Q4':'2018'}))df17_Ea_degree=(df17_Ea['FormalEducation'].replace(['No formal education past high school', 'Some college/university study without earning a bachelor’s degree'],'~college') .replace(['Doctoral degree', 'Professional doctorate'],'Doctoral degree~') .value_counts().to_frame() .rename(columns={'FormalEducation':'2017'}) .rename(index = {'I did not complete any formal education past high school':'No formal education past high school'}))concat1 = pd.concat([df21_Ea_degree,df20_Ea_degree],axis=1, join='outer') concat2 = pd.concat([df19_Ea_degree,df18_Ea_degree],axis=1, join='outer') concat3 = pd.concat([concat1,concat2],axis=1, join='outer') df21_Ea_degree_yearly_=concat3.join(df17_Ea_degree).fillna(0).transpose() #.transpose() 행 열 바꾸기df21_Ea_degree_yearly=df21_Ea_degree_yearly_.stack().to_frame().reset_index().rename(columns={'level_0':'year','level_1':'degree',0:'value'})df21_Ea_degree_yearly#그래프 그리기fig = px.sunburst(df21_Ea_degree_yearly, path=['year','degree'], values=df21_Ea_degree_yearly['value'].tolist())fig.show()","link":"/2021/11/22/kgg/kgg_EastAsia_varH/"},{"title":"kaggle :ScatterLine (Q11)","text":"kaggle dictation (05) plotly.graph_objects as go: 를 이용한 Scatter + line G산점도 bivariate”이변수” 값을 시각화 하는 기본적인 그래프. correlation: Positive, Negative, non 두 개의 변수 각각의 분포과 변수간의 관계를 확인 할 수 있다. ref. box and scatter plot/Ko.통계 그리는 방법 0. data sethttps://www.kaggle.com/miguelfzzz/the-typical-kaggle-data-scientist-in-2021 Subject : 가장 많이 이용하는 computer platform(hardware)1. data 읽어오기 + data Frame 만들어 주기1234567891011platform = ( df['Q11'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'Platform', 'Q11':'Count'}) .sort_values(by=['Count'], ascending=False) .replace(['A deep learning workstation (NVIDIA GTX, LambdaLabs, etc)', 'A cloud computing platform (AWS, Azure, GCP, hosted notebooks, etc)'], ['A deep learning workstation', 'A cloud computing platform']) ) ide를 dataframe화 완료. Q11의 column이름 까지 재설정 완료. 2.표 설정.1234567891011ide = ( ide .count() .to_frame() .reset_index() .rename(columns={'index':'IDE', 0:'Count'}) .sort_values(by=['Count'], ascending=False) )ide['percent'] = ((ide['Count'] / len(df))*100).round(2).astype(str) + '%' 3. percent 추가1platform['percent'] = ((platform['Count'] / platform['Count'].sum())*100).round(2).astype(str) + '%' 3. 색 지정1234colors = ['#033351',] * 6colors[5] = '#5abbf9'colors[4] = '#0779c3'colors[3] = '#0779c3' 4. 표 재 설정12345platform = (platform .sort_values(by = ['Count']) .iloc[0:15] .reset_index())platform .sort_values(by = [‘Count’]) : [Count]로 정렬, .iloc[0:15] platform의 column 선택: 0~15까지 data 가져오기 .reset_index() : data와 상관 없는 새 index 가져오기 5.plotly.graph_objects.Scatter() 본격적으로 Scatter G 만들기. ## 산점도 점 찍기 12345678fig = go.Figure(go.Scatter(x = platform['Count'], y = platform[&quot;Platform&quot;], text = platform['percent'], mode = 'markers', marker_color =colors, marker_size = 12))fig x = platform[‘Count’], y = platform[“Platform”], x축, y축 설정 text = platform[‘percent’], text를 넣는다고 하는데 안보이네 mode = ‘markers’, Text, lines+markers, makers, line 이 가능 한거 같다. Scatter.mod marker_color =colors, marker_size = 12) 산점도 안에 있는 점의 색과 크기 ## 산점도에 for문을 이용하여 line 연결하기 123456for i in range(0, len(platform)): fig.add_shape(type='line', x0 = 0, y0 = i, x1 = platform[&quot;Count&quot;][i], y1 = i, line=dict(color=colors[i], width = 4)) for i in range(0~platform의 길이만큼) fig. add_shape() type = ‘line’ - line모양의 grape shape add x0 = 0, y0 = i, - 초기값 x1 = platform[“Count”][i], x축 Indexy1 = i, y축 Index, 마지막 값 line=dict(color=colors[i], width = 4) line의 세부 설정, 색과 두께 flatform은 .iloc[0:15] 로 뽑아진 list 형식따라서 platform[“Count”][i]값을 뽑아 낼 수 있다. 6. update_traces()123fig.update_traces(hovertemplate='&lt;b&gt;Platform&lt;/b&gt;: %{y}&lt;br&gt;&lt;extra&gt;&lt;/extra&gt;'+ '&lt;b&gt;Count&lt;/b&gt;: %{x}&lt;br&gt;'+ '&lt;b&gt;Proportion&lt;/b&gt;: %{text}') 7. Design12345678910111213fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='#9f9f9f', ticklabelmode='period')fig.update_yaxes(showgrid=False) fig.update_layout(showlegend=False, plot_bgcolor='#F7F7F7', margin=dict(pad=20), paper_bgcolor='#F7F7F7', yaxis_title=None, xaxis_title=None, title_text=&quot;Most Commonly Used &lt;b&gt;Computing Platforms&lt;/b&gt;&quot;, title_x=0.5, font=dict(family=&quot;Hiragino Kaku Gothic Pro, sans-serif&quot;, size=17, color='#000000'), title_font_size=35) 8. Annotation12345678910111213141516171819fig.add_annotation(dict(font=dict(size=14), x=0.98, y=-0.22, showarrow=False, text=&quot;@miguelfzzz&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.add_annotation(dict(font=dict(size=12), x=0.04, y=-0.22, showarrow=False, text=&quot;Source: 2021 Kaggle Machine Learning &amp; Data Science Survey&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show()","link":"/2021/11/09/kgg/Kgg_plotly_ScatterLine/"},{"title":"decisionTree Classifier()","text":"DecisionTreeClassifier##Classifier function Decision Tree Classifier는 데이터 집합에서 다중 클래스 분류를 수행할 수 있는 클래스이다. 다른 분류자와 마찬가지로 Decision Tre Classifier는 훈련 샘플을 고정하는 배열 X(n_샘플, n_특징)와 훈련 샘플에 대한 클래스 레이블을 고정하는 정수 값, 형상(n_샘플, n_특징)의 배열 Y의 두 배열을 입력으로 사용합니다. 12345from sklearn import treeX = [[0, 0], [1, 1]]Y = [0, 1]clf = tree.DecisionTreeClassifier()clf = clf.fit(X, Y) 이제 예측 모델을 만들어 봅시다. 12clf.predict([[2., 2.]])clf.predict_proba([[2., 2.]]) array([[0., 1.]]) 의사결정트리의 분류는 classification과 multiclass 양쪽으로 모두 분류 할 수 있다. iris dataset을 하용하면, 우리는 아래와 같은 plot_Tree를 만들 수 잇다. 1234567from sklearn.datasets import load_irisfrom sklearn import treeiris = load_iris()x, y = iris.data, iris.targetclf = tree.DecisionTreeClassifier()clf = clf.fit(x, y)tree.plot_tree(clf) [Text(167.4, 199.32, 'X[2] &lt;= 2.45\\ngini = 0.667\\nsamples = 150\\nvalue = [50, 50, 50]'), Text(141.64615384615385, 163.07999999999998, 'gini = 0.0\\nsamples = 50\\nvalue = [50, 0, 0]'), Text(193.15384615384616, 163.07999999999998, 'X[3] &lt;= 1.75\\ngini = 0.5\\nsamples = 100\\nvalue = [0, 50, 50]'), Text(103.01538461538462, 126.83999999999999, 'X[2] &lt;= 4.95\\ngini = 0.168\\nsamples = 54\\nvalue = [0, 49, 5]'), Text(51.50769230769231, 90.6, 'X[3] &lt;= 1.65\\ngini = 0.041\\nsamples = 48\\nvalue = [0, 47, 1]'), Text(25.753846153846155, 54.359999999999985, 'gini = 0.0\\nsamples = 47\\nvalue = [0, 47, 0]'), Text(77.26153846153846, 54.359999999999985, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 1]'), Text(154.52307692307693, 90.6, 'X[3] &lt;= 1.55\\ngini = 0.444\\nsamples = 6\\nvalue = [0, 2, 4]'), Text(128.76923076923077, 54.359999999999985, 'gini = 0.0\\nsamples = 3\\nvalue = [0, 0, 3]'), Text(180.27692307692308, 54.359999999999985, 'X[2] &lt;= 5.45\\ngini = 0.444\\nsamples = 3\\nvalue = [0, 2, 1]'), Text(154.52307692307693, 18.119999999999976, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 2, 0]'), Text(206.03076923076924, 18.119999999999976, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 1]'), Text(283.2923076923077, 126.83999999999999, 'X[2] &lt;= 4.85\\ngini = 0.043\\nsamples = 46\\nvalue = [0, 1, 45]'), Text(257.53846153846155, 90.6, 'X[1] &lt;= 3.1\\ngini = 0.444\\nsamples = 3\\nvalue = [0, 1, 2]'), Text(231.7846153846154, 54.359999999999985, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 0, 2]'), Text(283.2923076923077, 54.359999999999985, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1, 0]'), Text(309.04615384615386, 90.6, 'gini = 0.0\\nsamples = 43\\nvalue = [0, 0, 43]')] decision tree는 약 20여종의 parameter가 있다. Parameter criterion : 분할 품질을 측정하는 기능 (default : gini) splitter : 각 노드에서 분할을 선택하는 데 사용되는 전략 (default : best) max_depth : 트리의 최대 깊이 (값이 클수록 모델의 복잡도가 올라간다.) min_samples_split : 자식 노드를 분할하는데 필요한 최소 샘플 수 (default : 2) min_samples_leaf : 리프 노드에 있어야 할 최소 샘플 수 (default : 1) min_weight_fraction_leaf : min_sample_leaf와 같지만 가중치가 부여된 샘플 수에서의 비율 max_features : 각 노드에서 분할에 사용할 특징의 최대 수 random_state : 난수 seed 설정 max_leaf_nodes : 리프 노드의 최대수 min_impurity_decrease : 최소 불순도 min_impurity_split : 나무 성장을 멈추기 위한 임계치 class_weight : 클래스 가중치 presort : 데이터 정렬 필요 여부 12sklearn.tree.plot_tree(decision_tree, *, max_depth=None, feature_names=None, class_names=None, label='all', filled=False, impurity=True, node_ids=False, proportion=False, rounded=False, precision=3, ax=None, fontsize=None)tree.plot_tree() 가장 원시적이면서 기본적인 parameter가 있는 decision tree의 parameter를 외우기 보다 지금 어떤 형태로 코드가 들어 가는지에 집중하자. 왜냐하면, 최신의 버전은 decision tree가 아니기 때문. 123456from sklearn.datasets import load_irisfrom sklearn import treeclf = tree.DecisionTreeClassifier(random_state=0)iris = load_iris()","link":"/2021/11/04/blog/DecisionTreeClassifier/"},{"title":"become a BDS","text":"##Big data 를 이용 할 수 있는 BioData Scientist가 되고 싶다. 생물학은 점점 디지털화되고 있으며 이제 양적인 과학 분야의 빛을 발하고 있다. 핵심 추진 요인은 생물학적 연구에서 처리량이 높은 기술 플랫폼의 확산이 증가하는 것으로,체계적인 연구를 위해 수천 개의 조직과 유기체에 걸친유전자, 단백질 및 기타 생물학적 부분에 대한 수백만 개의 데이터 포인트가수집, 세척, 저장 및 통합될 수 있도록 한다. 이처럼 데이터가 풍부한 환경에서 생물학적(그리고 임상 샘플에 배치된 경우 생물의학)연구의 미래는 데이터의 전략적 극대화 에 있다고 해도 과언이 아니다. 오늘날의 기술 환경에서 데이터 과학 및 인공지능(AI)은이미 비즈니스 및 금융과 같은 영역에서 혁신 동력으로 작용하고 있다.여기서 데이터 과학자는 막후에서 작업하는 대신 데이터를 실질적인통찰력으로 변환하는 역할을 담당하고 있다. 예를 들어, 인공지능 기반 알고리즘 거래와 금융 기술(FinTech)의 주식 추천 시스템,엔지니어링의 자동화 엔진 설계, 시스템 유지보수 및 로봇공학 등이 있다.최근의 데이터 폭발과 이에 따른 비즈니스, 금융 및 컴퓨팅과 같은다른 분야의 데이터 과학의 발전을 감안할 때, 우리는 특히 생물학과 관련된 영역별문제를 다루는 새로운 변형인 빠르고 방대한 양의 데이터 생성과 함께 데이터 과학이등장할 것으로 예상한다. 이를 “바이오 데이터 과학” 이라고 한다. ###BDS(BioData Science)에는 세 가지 핵심 분야가 있다. 생물학 영역: Biology 수학 및 통계 : mathematics (statistics) 컴퓨터 공학 : computer science 생물학 영역은 질병의 원인이나 유추된 바이오 마커의 진단 효용 이해와 같은 생물학적 기원에 대한 질문과 관련이 있다. 컴퓨터 과학 코어는 특히 분석할 데이터가 큰 경우 문제 해결을 위한 적절한알고리즘 고안, 반복 처리 (예: 데이터의 큰 부분 집합에서동일한 알고리즘 여러 번 실행) 및 데이터 저장 문제 해결과 관련이 있다. 수학 및 통계 핵심 영역은 데이터 요약, 정규화 및 모델링을 포함한 문제와 관련이 있다.기술 및 탐색적 통계 데이터 분석이 BDS에만 국한된 것은 아니지만(생물 통계의 필수 구성 요소이기도 하며, 더 낮은 정도로 생물 정보학이기도 함),BDS는 빅데이터에 AI/ML을 적용하는 것에 기초한 신흥 기술을 이용한 예측에 초점을 맞추고 있다. ###바이오 데이터 과학은 다른 과학 분야와 다르지 않은 탐구 과학이다.BDS는 단순히 기술(technology), 기계 학습(machine learning) 인공지능(AI)이 아니다. 인공지능이 여러분을 위해 그렇게 해주기를 바라는 대신, 사람의 강한 논리적 사고에 기초해야 한다.BDS가 궁극적으로 연구의 과학이라는 이런 점에서, 전형적인 과학적 조사와 다르지 않다. 우리는 유전자 발현 변화가 정신 상태와 의미있게 상관되는지에 대한 질문에 답하도록 돕기 위해다음의 7가지 단계를 사용할 수 있다. 가장 큰 차이점은 BDS는 낮은 처리량 또는 저전력 물리적 실험에 대한 강조를 줄이면서의미 있는 데이터 조작 및 분석에 강력한 능력을 필요로 한다는 것이다. 데이터 과학은 다른 과학적 추구와 같은 과정으로 진행 된다. - 조사할 질문을 먼저 선택 - 테스트 가능한 관련 가설을 확인함으로써 이 질문의 범위를 넓힘 - 가설에 답하기 위한 데이터를 얻기 위한 적절한 실험을 설계하고 현장 적용할 - 결과를 결정하고 그 타당성, 즉 데이터가 연구 질문에 답하는 데 적합한지 여부를 평가 - 마지막으로, 모델을 배치하고 연구 결과가 반복 가능한지 확인 ###데이터 분석은 복잡한 다단계 프로세스이다.**BDS(BioData Science)**는 도전적인 분야이지만, 생물정보학이나 전산생물학과 비슷하게 어렵다. 생물학적 시스템을 측정하기 위한 기술적 플랫폼은 매우 정교하지만, 생물학적 시스템은 매우 복잡하다.게다가 생물학적 실체를 측정하기 위해 개발된 기술적 도구는 생물학적 시스템의구성요소가 변화하고 시간이 지남에 따라 자연스럽게 변화하는 동안 기술적 불확실성에 영향을 받는다.바이오 빅 데이터는 이러한 문제에 대한 자연스러운 솔루션이 아니며 새로운 문제를 야기한다. BDS 데이터는 매우 많은 수의 관측에서 보존된 패턴을 식별하는 과정과 같은 데이터 과학노력을 촉진할 수 있지만, 적절한 분석 파이프라인이 개발될 경우에만 그렇게 할 수 있다.이 작업은 하찮지 않다. 이러한 분석 파이프라인은 데이터 수집에서 시작하여더 높은 수준의 생물학적 해석과 통찰력을 향한 계산 및 통계 평가를 통해 계속 이어지는다양한 접근 방식의 엔드 투 엔드 통합으로 상상할 수 있다. omics 데이터의 바이오 마커 분석을 위한 단순화된 파이프라인과 관련 주요 고려 사항은 다음과 같을 수 있다. 분석 파이프라인은 매우 유연해야 하며 연구 질문의 필요에 따라 변화해야 한다.완벽한 지식이 부족하기 때문에 최적화와 재현성을 어느 정도 달성하기 위해 몇 단계를 왔다 갔다반복하고 다듬는 것도 일반적이다. 예를 들어, 정규화 단계에서 두 개의 서로 다른 정규화 절차를사용하여 매우 다르고 겹치지 않는 차등 유전자 세트를 발견했다고 가정 해 보자.정규화 절차는 데이터에 대해 잘못된 가정을 하거나 잘못 구현되었을 수 있다.표시된 주요 고려사항은 엄청나게 많다. 고려사항의 예와 함께 단계를 보여주는 목적은각 단계마다 완벽한 시스템이나 파이프라인이 없지만, 각 의사결정 지점마다 이후 단계에 대한결과를 갖는 많은 고려사항이 있음을 입증하는 것이다. 우리는 또한 특정 정규화 접근법이 다운스트림 통계 절차와 잘 작동하는지와 같은호환성 문제나 특정 절차가 배치 효과 보정 알고리즘 및 일부 다중 시험 보정 방법과 관련된과다 탈락 및 과다 첨가로 이어질 수 있는지 여부와 같은 문제를 걱정해야 한다. 일반적으로 좋은 결과를 보장하는 노선도나 표준 운영 절차는 없다는 점에서BDS는 예술에 가까운 과학인 것이다. Ref.https://gohwils.github.io/biodatascience/biodatascience.html","link":"/2021/11/04/blog/BDS/"},{"title":"Google Colaboratory file을 GitHube에 Upload하기","text":"Google Colaboratory file을 GitHub에 바로 올리기 ###google Colaboratory란? google Colaboratory (G-CLB, 구글 코랩)로 작업 했다면 이 file을 바로 gitHub blog에 올리고 싶을 것이다.우리는 다음과 같은 과정을 통해 G-CLB file을 다른 변환 과정 없이 file 째로 올릴 수 있다. 우리에게 필요한 app은 ‘아나콘다’ 이다. Anaconda의 JupyterLab을 이용하여 변환 해 봅시다. 아나콘다의 주피터랩&gt; file Tab에서 Export Note로 간다 MarkDown file으로 다운 받아서 _post 경로에 넣어주면 완성 !!! 다른 방법도 있다. !!","link":"/2021/11/01/blog/GC_Upload_GH/"},{"title":"Index","text":"GitHub Index made By @YoonHwa-P Python python, Tuple * python, List python, Matplotlib python, Pandas pandas, series pandas, dataframe * python, Numpy python, basic 머신러닝 (의사결정트리) 의사결정트리, Classification 의사결정트리, 이론 python, plotly Kaggle_typical Kaggle_Africa DBS DBS, 이론 논문리뷰, *github girhub, nodjs github, 카테고리 github, Multi jupyterExport(.MD만들기) github, blog 만들기 github, Repository 만들기 올려야 하는 Posting List 의사결정트리, Regression seaborn matplotlib 수정 해야 할것도 넘넘 많다.일단 이번 주 주말에는 Kaggle 먼저 (Team project 닌까.) * : 수정중 123&lt;hr style=&quot;dashed 10px pink;&quot;&gt;&lt;hr align=&quot;center&quot; style=&quot;border: solid 10px #304A84; width: 50%;&quot;&gt;&lt;span style=&quot;font-size:150%; color: Red;&quot;&gt; * &lt;/span&gt; ※ 참고 할 Github ㅁ https://github.com/dschloe/R_edu ㅁ https://github.com/JunghoGIT ㅁ https://github.com/kimgoden/JAVA01 ㅁ https://github.com/WDWDWWDff/Red ㅁ https://github.com/OliverKang ㅁ https://github.com/hanbeen1992 ㅁ https://github.com/kjw1390","link":"/2021/11/05/blog/Index/"},{"title":"Make a gitHub Draft","text":"Github blog에 초안(draft) 작성하기 ref.","link":"/2021/11/12/blog/Make_gitHub_draft/"},{"title":"Make a gitHub","text":"학원에서 배운 코드들을 올려봅시다. 깃허브 디자인 더 하고 싶은데 안되넹 흐흐 오늘 한 내용 정리 + slack join (green_702) + anaconda install + system Path (환경변수 설정_자동) + pycham install + gitHub blog 만들기 (회원가입 & repository 생성) + gitHub에 repository 만들고, file UPloading 하기_01 1. gitHub에서 repository 만들기 클릭 2. 이름 red로 설정후 하단에 add a README.file 체크 3. 바탕화면에서 우측클릭후 git bash here 선택 4. 링크로 연결 : git clone https://github.com/각자계정/red.git + gitHub에 repository 만들고, file UPloading 하기_02 1. file 만들기(Blue) 2. GitHub에 repository 만들기(Blue); 이름이 같지 않으면 안됨 3. 원하는 file 넣기 (folder 가능) + git 명령어 + terminal Tap에 Git Bash를 활성화하여 명령어 입력 $ git init >$ git init_ git을 initiation 해 준다. (초기화) ![img.png](..\\imeges\\Make_gitHub\\img.png) 맨 처음에만 해 주면 된다. $ git add . > $ git add : 지금 Update한 data를 서버에 올려준다. ![img_1.png](..\\imeges\\Make_gitHub\\img_1.png) $ git add . : 경로에 있는 모든 file Upload $ git add [filename.Ex] : [file 이름과 확장자] 특정 file 을 저장 $ git commit - m \"Comment(History log)\" > $ git commit - m \"Comment(History log)\" ![img_2.png](..\\imeges\\Make_gitHub\\img_2.png) commit하여 확정 해 준다. 5개의 sql files가 올라간 것을 볼 수 있다. ![img_3.png](..\\imeges\\Make_gitHub\\img_3.png) 확정 해 주고 마지막으로 push 해 주면 다음과같은 History Log를 github main에서 볼 수 있다. $ git status $git status 깃허브 commit 하기 전에 올릴 파일이 있는지 등의 상태를 알아 볼 수 있다. commit 이후에는 사라지나부다 사진이 없다. 올리기 전에 급하게 한컷 찍어 보앗쥐 커밋해야하는 상태를 보여준다. $ git push > $ git push 최종 브라우저에 저장. ![img_4.png](..\\imeges\\Make_gitHub\\img_4.png) $ git push 후 main -> main 이 나오면 성공 ! > Ref. *파이참, 아나콘다 DownLoad ㅁ https://www.anaconda.com/products/individual#Downloads ㅁ https://www.jetbrains.com/pycharm/download/ ㅁ https://git-scm.com/ Ref..https://80000coding.oopy.io/865f4b2a-5198-49e8-a173-0f893a4fed45 ㄴ&gt; 깃허브 꾸미기 ++앞으로 참고 하고 싶은 github++ㅁ https://github.com/dschloe/R_edu ㅁ https://github.com/JunghoGIT ㅁ https://github.com/kimgoden/JAVA01 ㅁ https://github.com/WDWDWWDff/Red ㅁ https://github.com/OliverKang ㅁ https://github.com/hanbeen1992 ㅁ https://github.com/kjw1390","link":"/2021/10/29/blog/Make_gitHub/"},{"title":"Study Numpy","text":"Numpy Numpy 정의NumPy는 행렬이나 일반적으로 대규모 다차원 배열을 쉽게 처리 할 수 있도록 지원하는 파이썬의 라이브러리Numpy는 데이터 구조 외에도 수치 계산을 위해 효율적으로 구현된 기능을 제시 한다. Ref. Wiki 라이브러리에서 numpy 불러오기우리는 numpy를 import 하여 numpy에 내장되어 있는 함수를 가져와 쓸 수 있다. 일반적으로 np에 저장하여 많이 사용 하는 듯 하다. 123456789import numpy as npprint(np.__version__)print (&quot;Numpy의 version을 확인 해 볼 수 있다. &quot;)temp = np.array([1, 2, 3])print(type(temp))print (&quot;Numpy의 type은 nd array인 것을 볼 수 있다. &quot;) 1.19.5 Numpy의 version을 확인 해 볼 수 있다. &lt;class 'numpy.ndarray'&gt; Numpy의 Type을 보면 nd array 인 것을 볼 수 있는데 ND : N dimension 을 의미한다.즉 한국어로 번역 해 보면 N차 행렬 정도로 볼 수 있다. Numpy 에서 배열을 생성 해 보자.1차원 배열 생성1차원의 배열을 생성해서 array와 List의 다른 점을 알아보자. 차이점은 shpae를 찍어 보면 알 수 있다.내장 함수 : (fuction or method or attribute) 123456789101112131415161718data1 = [0, 1, 2, 3, 4]data2 = [10, 9, 8, 7, 6]My_array = np.array(data1)print(&quot;data1은 List이다. &quot;)print(data1)print(type(data1))#print(data1.shape) #List의 경우 shape 함수가 내장 되어 있지 않아 shape를 알 수 없다. print(&quot;My_array은 numpy형식의 tuple인 것을 알 수 있다. &quot;)print(My_array)print(My_array.shape)print(type(My_array))print(&quot;.dtype() 는 data의 type을 확인 할 수 있는 function 이다.&quot;)print(My_array.dtype) data1은 List이다. [0, 1, 2, 3, 4] &lt;class 'list'&gt; My_array은 numpy형식의 tuple인 것을 알 수 있다. [0 1 2 3 4] (5,) &lt;class 'numpy.ndarray'&gt; .dtype() 는 data의 type을 확인 할 수 있는 function 이다. int64 List 형식의 경우 shape 함수가 내장 되어 있지 않은 반면,numpy 형식의 np.array 의 경우 tuple shape 함수가 내장 되어 에러가 나지 않과 (5, )의 형태로 result가 나오는 것 을 볼 수 있다. .dtype() 는 data의 type을 확인 할 수 있는 function 이다.이때 나타나는 int 64는 64byte의 타입 이라는 것을 알려 준다. https://rfriend.tistory.com/285 2차원 배열 생성4 X 3 배열을 만들어 보자. 1234my_array4 = np.array([[2,4,6],[8,10,12],[14,16,18],[20,22,24]])print(my_array4)my_array4.shape [[ 2 4 6] [ 8 10 12] [14 16 18] [20 22 24]] (4, 3) 3차원 배열 생성123my_array5 = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])print(my_array5)my_array5.shape [[[1 2] [3 4]] [[5 6] [7 8]]] (2, 2, 2) Numpy 기본 함수들 arange zeroes, ones reshape sort argsort arangenp.arange(5)를 넣으면 array안에 5개의 숫자가 순서대로 나오는 배열이 자동으로 만들어진다. 12Array = np.arange(5)print(Array) [0 1 2 3 4] np.arange(a, b, c) : a의 숫자부터 b 숫자까지 C씩 띄워서 배열생성 12aArray = np.arange(1, 9, 2)print(aArray) [1 3 5 7] zeroes, oneszeroes와 ones 함수를 살펴 보자각 함수들은 0과 1을 채워 넣어 배열을 생성하는 함수 들 이다. 1234567891011print(&quot;Zeros_Array&quot;)zeros_array = np.zeros((2,3))print(zeros_array)print(&quot;Data Type is:&quot;, zeros_array.dtype)print(&quot;Data Shape is:&quot;, zeros_array.shape)print(&quot;Ones_Array&quot;)ones_array = np.ones((3,4), dtype='int32')print(ones_array)print(&quot;Data Type is:&quot;, ones_array.dtype)print(&quot;Data Shape is:&quot;, ones_array.shape) Zeros_Array [[0. 0. 0.] [0. 0. 0.]] Data Type is: float64 Data Shape is: (2, 3) Ones_Array [[1 1 1 1] [1 1 1 1] [1 1 1 1]] Data Type is: int32 Data Shape is: (3, 4) 8행에 보면 Array를 행성 하면서 dtype을 int32로 지정 해 준 것을 볼 수 있다. Zeros_Array의 경우 채워진 0들이 모두 float type의 실수 이기 때문에 0. 이라고 나타는 것을 볼 수 있지만, Ones_Array의 경우 1 만 나타난 Int 형태의 type인 것을 볼 수 있다. reshapereshape는 행렬의 모양을 바꿔주는 함수이다.행렬의 모양을 바꿀 때에는 약간의 제약이 있는데예를 들어 설명 해 보자면, 3X4 = 12, 6X2 =12로 형태 변환을 할 수 있지만,3X5 = 15이기 때문에 변환이 불가능 하다.이것이 이해가 가지 않는다면 중학교로 돌아 가야 할 지도 모른다. 123456print(ones_array)print(&quot;Data Shape is:&quot;, ones_array.shape)print(&quot;Ones_array의 형태를 reshpe로 바꿔보자 \\n&quot;)after_reshape = ones_array.reshape(6,2)print(after_reshape)print(&quot;Data Shape is:&quot;, after_reshape.shape) [[1 1 1 1] [1 1 1 1] [1 1 1 1]] Data Shape is: (3, 4) Ones_array의 형태를 reshpe로 바꿔보자 [[1 1] [1 1] [1 1] [1 1] [1 1] [1 1]] Data Shape is: (6, 2) 2차원 배열은 3차원으로도 reshape 할 수 있다. 제약 조건은 3 X 4 = 12 였기 때문에 2 X 3 X 2 = 12가 되기 때문에 reshape 가 가능 하다. 123456789101112after_reshape = ones_array.reshape(2,3,2)print(after_reshape)print(&quot;Data Shape is:&quot;, after_reshape.shape, &quot;\\n&quot;)after_reshape2= ones_array.reshape(-1,6)print(&quot;reshape(-1,6)? \\n&quot;)print(after_reshape2)after_reshape3= ones_array.reshape(3,-1)print(&quot;reshape(3, -1)? \\n&quot;)print(after_reshape3)print(&quot;Data Shape is:&quot;, after_reshape3.shape) [[[1 1] [1 1] [1 1]] [[1 1] [1 1] [1 1]]] Data Shape is: (2, 3, 2) reshape(-1,6)? [[1 1 1 1 1 1] [1 1 1 1 1 1]] reshape(3, -1)? [[1 1 1 1] [1 1 1 1] [1 1 1 1]] Data Shape is: (3, 4) 만일 2차 행렬 reshape에서 한개의 변수만 정해 졌다면, 나머지 는 -1을 써주면 자동으로 알맞은 변수를 정해 줍니다. 3차 행렬 역시 남은 1개만 -1을 써서 reshape 함수로 행렬을 변환 할 수 있습니다. 21.11.02(이 아래 부분은 다음에 UPdate하기로 한다. ) short1 argsortNumpy 인덱싱과 슬라이딩Numpy 정렬","link":"/2021/11/01/blog/Numpy/"},{"title":"Study Pandas","text":"Pandas 판다스란pandas는 데이터를 시각화 하기 좋은 python base의 한 라이브러리이다. python을 이용한 data분석과 같은 작업에서 필수적으로 쓰이고 있다. 아나콘다와 같은 IDE를 이용하여 작업 할 수도 있지만, 기본적으로 Import하여 편하게 사용 할 수 있다. 판다스의 이름은 계량 경제학에서 사용되는 용어인 **’PANel DAta’**의 앞 글자를 따서 지어졌다.판다스는 R에서 사용되던 data.frame 구조를 본뜬 DataFrame이라는 구조를 사용하기 때문에, R의 data.frame에서 사용하던 기능 상당수를 무리없이 사용할 수 있도록 만들었다.더욱이 파이썬이라는 접근성이 좋은 언어 기반으로 동작하기 때문에 데이터 분석을 파이썬으로 입문하는사람들이 필수적으로 사용하는 라이브러리가 되었다.’ 판다스 Import 하기쥬피터 노트로 설치가 가능 하다고 하지만, 구글코랩이나 케글 노트에서는 Import하여 쉽게 사용한다. 12import pandas as pdpd.__version__ pandas는 오픈소스로 누구나 무료로 이용 할 수 있고,Numpy, matplotlib등 다른 라이브러리들과 함께 쓰인다. 일반적으로 pandas는 pd로 import되기 때문에 pd.[function] 으로 써있으면pandas 라이브러리를 이용한다고 생각 하면 된다. Livraly의 경우 version 오류가 많이 있으므로,Import 해 주는 라이브러리는 version을 꼭 확인하여 오류에 대비 하도록 한다. Pandas에는 3가지 자료 구조가 있는데series 와dataFrame,panel 이 그것이다. pandas에대해 더 알아보고 싶다면, 링크를 타고 가보자. ref. 판다스/위키 판다스를 이용에는 유용한 Tutorial들이 있다.아래 있으니 시간이 날 때마다 보면서 Update하자. 영문판이 있지만 지금은 일단 한글 번역한것을 보자화이팅 ! 판다스/doc10 minutes to Pandas/한글","link":"/2021/11/02/blog/Pandas/"},{"title":"python_List","text":"google Colaboratory 를 이용한 실습 Intro_ print와 주석처리 print Out 1print(&quot;Hello, World !!&quot;) Hello, World !! 이제 우리는 모든 것을 나타 낼 수 있습니다. 주석처리 1234567# 한 줄로 주석 처리 하는 방법 &quot;&quot;&quot;여러줄은 이렇게 print(&quot;Hello, World??&quot;)&quot;&quot;&quot;print(&quot;Hello, World!!&quot;) #주석 처리한 부분은 안나오지 Hello, World!! 변수의 종류 int : 정수 float : 실수 bool : 참/거짓 none : null String : 문자 이전에 배웠던 JAVA와는 다르게 변수형을 지정 해 주지 않아도 된다 Type 함수는 그 변수의 type이 어떤 것인지 알려준다. 123456789101112N_int = 1print (type(N_int))#Type 함수 : type 을 print 해 준다 N1_float = 2.0print (type(N1_float))bool_N = Trueprint (type(bool_N))X= Noneprint (type(X)) &lt;class 'int'&gt; &lt;class 'float'&gt; &lt;class 'bool'&gt; &lt;class 'NoneType'&gt; 연산 사칙연산 Number type 123456789101112131415161718192021a = 1b = 2c = 3.14d = 1.414print (&quot;정수_int_의 사칙연산 &quot;)print ('a + b = ' , a+b)print('a - b = ', a-b)print('a * b = ', a*b)print('a / b = ', a/b)print('a // b = ', a//b)print('a % b = ', a%b)print('a ** b = ', a**b)print (&quot;실수_float_의 사칙연산 &quot;)print('c + d =', c+d)print('c - d =', c-d)print('c * d =', c*d)print('c / d =', c/d)print('c // d =', c//d)print('c % d =', c%d)print('c ** d =', c**d) 정수_int_의 사칙연산 a + b = 3 a - b = -1 a * b = 2 a / b = 0.5 a // b = 0 a % b = 1 a ** b = 1 실수_float_의 사칙연산 c + d = 4.554 c - d = 1.7260000000000002 c * d = 4.43996 c / d = 2.2206506364922207 c // d = 2.0 c % d = 0.3120000000000003 c ** d = 5.042646477882085 String 연산 1234str1 = &quot;hi&quot;str2 = &quot;bye&quot;print('str1 + str2 = ', str1 + str2 )print ('str 1 * 3 + str * 2 = ', str1 * 3 + str2 * 2 ) str1 + str2 = hibye str 1 * 3 + str * 2 = hihihibyebye indexing1234567891011121314151617181920greeting = &quot;Hi!Hellow? AnNyung!&quot;print(greeting)#greeting, 이하니처럼 인사를 해 봅시다. print(greeting[6:])print(greeting [:6])G = &quot;12345 6789@&quot;print (G[2:])print (G[:3])print (G[1:9])print (G[0:7:3])&quot;&quot;&quot; [a : b : c] a : 시작 index b : 끝 Index c : 띄워서&quot;&quot;&quot; Hi!Hellow? AnNyung! low? AnNyung! Hi!Hel 345 6789@ 123 2345 678 146 ' \\n[a : b : c]\\n a : 시작 index\\n b : 끝 Index\\n c : 띄워서\\n' List List 함수 종류 .append() : 추가 .extend() : 연장 .remove() : 제거 .del() : 제거 .pop() :Index설정 .clear() : 전체 삭제 Internet에 더 많은 함수를 찾을 수 있는 documents 가 있을 것이다.^^ List Function List란, java로 치면 배열 List create 부터 123456789101112131415161718192021222324252627282930313233343536373839404142print(&quot;List 를 생성 해 봅시다.&quot;)A = [] #변수선언으로 빈 리스트 만들기a= list() #List 함수로 만들기 a = ['I', 'Have', ['a', 'DreAm']]print (&quot;a함수에 직접적 Index로 넣어주기&quot;)print (a)print (&quot;a[0]= 'YOU'의 결과 &quot;)a[0]= 'YOU'print (a)#List에 값 추가 해 주기print(A)A.append('U')A.append('song')A.append(['to', 'sing'])print(&quot;A[0:] : A의 모든 원소 추출&quot;)print(A[0:])print(&quot; &quot;)#Extend print(&quot;a : a에 Extend를 이용하여 원소 추가하기&quot;)print(a)a.extend('U')print(a)a.extend(['song', 'to', 'sing'])print(a)#Extend 와 같은 느낌으로 &quot;+=&quot; 연산자를 써 줄수 있다. print(&quot;a : a에 연산자를 이용하여 원소 추가하기&quot;) a += ['Like U']print(a)#Insert는 중간에 인덱스 값을 넣어 주어서 List에 값을 추가 해 줄 수 있다. print(a)print(&quot;a : a에 insert를 이용하여 원하는 Index번호에 원소 추가하기&quot;)a.insert(1,'do not')print(a)del a[1:1]#다음 코드를 위해 a의 Index 1에 넣어준 원소를 제거 했음 List 를 생성 해 봅시다. a함수에 직접적 Index로 넣어주기 ['I', 'Have', ['a', 'DreAm']] a[0]= 'YOU'의 결과 ['YOU', 'Have', ['a', 'DreAm']] [] A[0:] : A의 모든 원소 추출 ['U', 'song', ['to', 'sing']] a : a에 Extend를 이용하여 원소 추가하기 ['YOU', 'Have', ['a', 'DreAm']] ['YOU', 'Have', ['a', 'DreAm'], 'U'] ['YOU', 'Have', ['a', 'DreAm'], 'U', 'song', 'to', 'sing'] a : a에 연산자를 이용하여 원소 추가하기 ['YOU', 'Have', ['a', 'DreAm'], 'U', 'song', 'to', 'sing', 'Like U'] ['YOU', 'Have', ['a', 'DreAm'], 'U', 'song', 'to', 'sing', 'Like U'] a : a에 insert를 이용하여 원하는 Index번호에 원소 추가하기 ['YOU', 'do not', 'Have', ['a', 'DreAm'], 'U', 'song', 'to', 'sing', 'Like U'] 리스트 연산 12C = a+Aprint(&quot;a+A의 연산결과 : &quot;, C) ['U', 'do not', 'song', 'to', 'sing', 'U', 'song', ['to', 'sing']] 리스트에 값 삭제 해 주기 12345678910111213141516171819202122232425262728293031323334353637383940414243444546b=[1, 2, 3, 4, 5, 6 ]B=[7, 8, 9, 10, 11, 12, 13, 1, 4]print(&quot;리스트 값 삭제하기 &quot;)print(b)print(B)print(&quot;b의 1번째 삭제&quot;)#1개씩 삭제 해 주기 del b[1]print(b)print(&quot;B의 1~2 번째 삭제 : [1:2]&quot;)#범위로 삭제 해 주기 del B[1:2]print(B)#중복 값을 삭제 해 주는 functionK=[0, 1, 2, 3, 4, 5, 8, 6, 7, 9, 10, 0, 12]print(&quot;K : &quot;, K)K.remove(0)print(&quot;K.remove(0) : &quot;, K)K.insert(0, 1)print(&quot;K.insert(0,1) : &quot;, K)#pop은 Index를 정해서 특정 문자에 정해 줄 수 있다. S = b.pop()SS= B.pop(1)print(&quot;b.pop() : &quot;)print (S, &quot; : ()안에 아무것도 쓰지 않으면 맨 마지막 &quot;)print(&quot;B.pop(1) : &quot;)print(SS, &quot; : ()안에 숫자를 쓰면 (1) 1번Index 뽑아내기&quot;)#pop과 같은 느낌으로 특정 문자를 뽑아 내기 print(&quot;Index 숫자로 pop과 같은 기능을 실행 할 수 있다. &quot;)print (a[0:])print (a[2][1])print (a[2][1][3])#List 전체 삭제 리스트 값 삭제하기 [1, 2, 3, 4, 5, 6] [7, 8, 9, 10, 11, 12, 13, 1, 4] b의 1번째 삭제 [1, 3, 4, 5, 6] B의 1~2 번째 삭제 : [1:2] [7, 9, 10, 11, 12, 13, 1, 4] K : [0, 1, 2, 3, 4, 5, 8, 6, 7, 9, 10, 0, 12] K.remove(0) : [1, 2, 3, 4, 5, 8, 6, 7, 9, 10, 0, 12] K.insert(0,1) : [1, 1, 2, 3, 4, 5, 8, 6, 7, 9, 10, 0, 12] b.pop() : 6 : ()안에 아무것도 쓰지 않으면 맨 마지막 B.pop(1) : 9 : ()안에 숫자를 쓰면 (1) 1번Index 뽑아내기 Index 숫자로 pop과 같은 기능을 실행 할 수 있다. ['hi', 'do not', 'Have', ['a', 'DreAm'], 'U', 'song', 'to', 'sing', 'd', 'i', 'd', ' ', 'n', 'o', 't'] a --------------------------------------------------------------------------- IndexError Traceback (most recent call last) &lt;ipython-input-112-d37801788a81&gt; in &lt;module&gt;() 40 print (a[0:]) 41 print (a[2][1]) ---&gt; 42 print (a[2][1][3]) 43 44 #List 전체 삭제 IndexError: string index out of range List 값을 덮어 쓰는 기능 12345678print (&quot;A[] = &quot;, A) print (&quot;A[1:2]= ['i', 'do', 'NOT', 'know']의 결과 &quot;)A[1:2]= ['i', 'do', 'NOT', 'know']print (A) # A[]에 1번 Index에 범위보다 큰 수가 엎어 씌어 진 것을 볼 수 있다. del A [1:4]print(&quot;del A [1:4] : &quot;, A) A[] = ['U', 'i', 'do', 'NOT', 'know', 'do', 'NOT', 'know', 'do', 'NOT', 'know', ['to', 'sing']] A[1:2]= ['i', 'do', 'NOT', 'know']의 결과 ['U', 'i', 'do', 'NOT', 'know', 'do', 'NOT', 'know', 'do', 'NOT', 'know', 'do', 'NOT', 'know', ['to', 'sing']] del A [1:4] : ['U', 'know', 'do', 'NOT', 'know', 'do', 'NOT', 'know', 'do', 'NOT', 'know', ['to', 'sing']] Ref.https://dojang.io/course/view.php?id=7 보고 공부 할 수 있어요","link":"/2021/11/05/blog/Python_List/"},{"title":"python_Tuple","text":"TupleTuple은 List에비해 메모리 사용 효율이 좋지만, 정보 수정이 어렵다. splicing 기능, +, * 의 연산은 가능하다. 1 Dictionary1 If 조건문1 for 반복문1","link":"/2021/11/05/blog/Python_tuple/"},{"title":"Kaggle Note 사용하기","text":"google colab에서 실행되지 않는google colab에서 version 오류로 실행 되지 않는 것들이 있다고 한다.따라서 이때 Kaggle data를 이용 할 예정이면 Kaggle Note를 사용해 보는 것은 어떨까? ‘+ Create’ 표시를 누른다. - New NoteBook - New dataSet 탭이 나온다. New NoteBook을 선택하여 이용해 보자. 아래와 같은 코드가 나오게 된다. 123456789101112131415161718192021# This Python 3 environment comes with many helpful analytics libraries installed# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python# For example, here's several helpful packages to loadimport numpy as np # linear algebraimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)# Input data files are available in the read-only &quot;../input/&quot; directory# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directoryimport osfor dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename))# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using &quot;Save &amp; Run All&quot; # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session 이것을 유지 한채 아래에 code 연습을 하면 되는 것 같다. 21.11.05_","link":"/2021/11/05/blog/Using_Kaggle_Note/"},{"title":"visualization by python","text":"Visualization _python으로[1103 학습]시각화 연습 및 작성 코드 깃헙 블로그 및 깃헙에 올리기, 개인별로 공유 시각화 올릴 때, 전체 코드 올리지 마시고,나눠서 올리세요. 설명 글 추가하시고여~ (예시 하단) 산점도: 산점도란 무엇인가? 언제 쓰는가? 코드 작성 및 간단 설명박스플롯: 박스플롯이란 무엇인가? 언제 쓰는가? 코드 작성 및 간단 설명 오후 1시까지 1차로 한번 올려서 개인별로 공유해주세요. 홧팅요 Intropython에서 visualiztion 하기 위해서는 많은 방법이 있다. data Analist들은 시각화를 위해 많은 tool을 사용 하는데 우리는 Seaborn과 Matplotlib을 이용하여 시각화를 할 예정이다. 코드 기반(python)의 data visualiztion의 장점 여러 그래프 동시 작성 가능 기존의 코드 재활용성 데이터 그기의 제한이 없음 (RAM등의 제약조건 없을때) Matplotlib 는 이미지 데이터와 정형 데이터(정적 그래프)를 시각화 할 수 있는데 나와 같은 비전공자들에게 시각화 문법이 조금 어렵다고 한다. 하지만 pandas data frame에서 쉽게 시각화 구현 하며, 통계(회귀선) 그래프 등을 쉽게 구현 할 수 있기 때문에 이를 배워야 한다. seaborn의 경우 그래프가 예쁘게 나오지 않지만 비교적 간단한 코드로 시각화를 할 수 있다. 하지만, 세부 옵션을 수정 하고 싶다면 Matplotlib를 알아야 한다. 이는 내부 원리를 파악 할 수 없기 때문에 내가 감당하기 힘들다. 때문에 지금 단계에서는 Matplotlib과 seaborn의 장점을 적절하게 섞어서 시각화를 진행 해 보자. 아래와 같은 Tutorial을 진행 하면 세부 옵션을 조정 하기 쉽다고 한다.https://matplotlib.org/stable/tutorials/index.html matplotlibmatplotlib를 이용하여 visualiztion을 해 보자. 제일 먼저 해 주어야 할 일은 Import하여 matplotlib를 불러오고 이를 plt등의 객체에 저장 해 주는 것이다. 123456789import matplotlib.pyplot as pltdates = [ '2021-01-01', '2021-01-02', '2021-01-03', '2021-01-04', '2021-01-05', '2021-01-06', '2021-01-07', '2021-01-08', '2021-01-09', '2021-01-10']min_temperature = [20.7, 17.9, 18.8, 14.6, 15.8, 15.8, 15.8, 17.4, 21.8, 20.0]max_temperature = [34.7, 28.9, 31.8, 25.6, 28.8, 21.8, 22.8, 28.4, 30.8, 32.0] 위의 data는 강사님의 data를 가져 온 것이다. data를 시각화 자료로 만들어 보자, 12345fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10,6))axes.plot(dates, min_temperature, label = 'Min Temperature')axes.plot(dates, max_temperature, label = 'Max Temperature')axes.legend()plt.show() 위의 표는 legend가 있고, 날짜별로 Min Temp와 Max Temp가 있는 그래프 이다. 먼저 fig, ax = plt.subplots()를 이용하여 표를 생성 한다. 이때 figure에대한 정보를 함께 생성 할 수 있는데, (nrows=1, ncols=1, figsize=(10,6)) 가 의미하는 것은 (행의 갯수 =1, 열의 갯수=1, fig size는 10X6)이다. ax를 통하여 plot 형태의 그래프를 그리는데, (x축, y축 , label = “[name]”)의 형태로 plot을 추가 해 준다.이때 Legend 를 생성 하고 싶다면, ax.legend()함수로 추가 해 줄 수 있다. 마지막으로 plt.show() 를 사용하여 마무리 해 준다. 물론 마지막 코드를 넣지 않아도 보여 주지만, 넣어주는 것이 인지상정이라고 한다. 123print(fig)print(axes) Figure(720x432) AxesSubplot(0.125,0.125;0.775x0.755) 위의 표인 fig의 정보를 알 수 있게 print 함수로 뽑아 봤는데 이건 무슨 말 인지 잘 모르겠다. matplotlib로 선 그래프 그리기아직 data를 어디에서 받을 수 있는지 잘 모르기 때문에 강사님이 다운받은 표를 그대로 가져와 보자 참조: https://pypi.org/project/fix-yahoo-finance/ 1!pip install yfinance --upgrade --no-cache-dir Collecting yfinance Downloading yfinance-0.1.64.tar.gz (26 kB) Requirement already satisfied: pandas&gt;=0.24 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.1.5) Requirement already satisfied: numpy&gt;=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.19.5) Requirement already satisfied: requests&gt;=2.20 in /usr/local/lib/python3.7/dist-packages (from yfinance) (2.23.0) Requirement already satisfied: multitasking&gt;=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.9) Collecting lxml&gt;=4.5.1 Downloading lxml-4.6.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.3 MB) \u001b[K |████████████████████████████████| 6.3 MB 7.7 MB/s \u001b[?25hRequirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24-&gt;yfinance) (2.8.2) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24-&gt;yfinance) (2018.9) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas&gt;=0.24-&gt;yfinance) (1.15.0) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.20-&gt;yfinance) (1.24.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.20-&gt;yfinance) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.20-&gt;yfinance) (2.10) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.20-&gt;yfinance) (2021.5.30) Building wheels for collected packages: yfinance Building wheel for yfinance (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for yfinance: filename=yfinance-0.1.64-py2.py3-none-any.whl size=24109 sha256=25a6e16cba240e66cb4999d0947a231b790b2b96766767434407e09149ec9302 Stored in directory: /tmp/pip-ephem-wheel-cache-a8wml444/wheels/86/fe/9b/a4d3d78796b699e37065e5b6c27b75cff448ddb8b24943c288 Successfully built yfinance Installing collected packages: lxml, yfinance Attempting uninstall: lxml Found existing installation: lxml 4.2.6 Uninstalling lxml-4.2.6: Successfully uninstalled lxml-4.2.6 Successfully installed lxml-4.6.4 yfinance-0.1.64 123456import yfinance as yfdata= yf.download('AAPL', '2019-08-01', '2021-08-01')data.info()print(yf) [*********************100%***********************] 1 of 1 completed &lt;class 'pandas.core.frame.DataFrame'&gt; DatetimeIndex: 504 entries, 2019-08-01 to 2021-07-30 Data columns (total 6 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Open 504 non-null float64 1 High 504 non-null float64 2 Low 504 non-null float64 3 Close 504 non-null float64 4 Adj Close 504 non-null float64 5 Volume 504 non-null int64 dtypes: float64(5), int64(1) memory usage: 27.6 KB &lt;module 'yfinance' from '/usr/local/lib/python3.7/dist-packages/yfinance/__init__.py'&gt; info() 함수를 통해 column정보와 data의 갯수, data의 type까지 알 수 있다.date time index에 구간이 내가 정한데로 나와 있는 것도 볼 수 있다. 12ts = data['Open']print(ts.head()) Date 2019-08-01 53.474998 2019-08-02 51.382500 2019-08-05 49.497501 2019-08-06 49.077499 2019-08-07 48.852501 Name: Open, dtype: float64 ts 객체에 Open data를 담고 .head() 함수로 상위 5개의 항목을 가져와 본다. 혹시,Pandas 에대하여 더 정리된 것을 알고 싶다면, 링크를 통해 확인 할 수 있다. Pyplot API아래의 예는 Pyplot API 방법을 이용하여, 한개씩 data를 넣어 준 형태 이다. 이는 객체지향으로 만들었다고 하기 어렵지만, 가능은 하다. 12345678910111213# import fix_yahoo_finance as yfimport yfinance as yfimport matplotlib.pyplot as pltdata = yf.download('AAPL', '2019-08-01', '2020-08-01')ts = data['Open']plt.figure(figsize=(10,6))plt.plot(ts)plt.legend(labels=['Price'], loc='best')plt.title('Stock Market fluctuation of AAPL') plt.xlabel('Date') plt.ylabel('Stock Market Open Price') plt.show() [*********************100%***********************] 1 of 1 completed Pyplot API 객체지향12345from matplotlib.backends.backend_aggimport FigureCanvasAgg as FigureCanvasfrom matplotlib.figure import Figureimport matplotlib.pyplot as plt 객체지향을 위해 import를 해 준다. 1234567891011fig = Figure()import numpy as npnp.random.seed(6)x = np.random.randn(20000)ax = fig.add_subplot(111)ax.hist(x, 100)ax.set_title('Artist Layer Histogram')# fig.savefig('Matplotlib_histogram.png')plt.show()","link":"/2021/11/03/blog/Visualiztion__python/"},{"title":"Test page","text":"Hello world https://velog.io/@kwonhl0211/Hello-Kaggle-%EC%BA%90%EA%B8%80%EC%9D%B4-%EC%B2%98%EC%9D%8C%EC%9D%B8-%EB%B6%84%EB%93%A4%EC%9D%84-%EC%9C%84%ED%95%9C-%EC%BA%90%EA%B8%80-%EA%B0%80%EC%9D%B4%EB%93%9C kaggle guide ko!! https://medium.com/@kaggleteam/how-to-get-started-with-data-science-in-containers-6ed48cb08266 Kaggle note랑 연동 하는 방법이 나와 있는듯나중에 Posting 해 봐야지 ^^ https://www.chartjs.org/docs/latest/samples/bar/horizontal.html chart에대해 많은 List가 있는데 완전 유용할듯 neo4j 를 이용하여 graph 만들수 있다.neo4j git blog: Hexo로 multi, push1234567git config --global user.email &quot;ssiasoda@gmil.com&quot;git config --global user.name &quot;YoonHwa-P&quot;git push origin HEAD:main 내가 push 할때 쓰려고 저장 Kaggle/competitionskaggle competition에 참가 할 수 있다. 1!pip install kaggle Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10) 12345678910from google.colab import filesuploaded = files.upload()for fn in uploaded.keys(): print('User uploaded file &quot;{name}&quot; with length {length} bytes'.format( name=fn, length=len(uploaded[fn]))) # Then move kaggle.json into the folder where the API expects to find it.!mkdir -p ~/.kaggle/ &amp;&amp; mv kaggle.json ~/.kaggle/ &amp;&amp; chmod 600 ~/.kaggle/kaggle.json Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) &lt;ipython-input-57-5e2b5f92ba05&gt; in &lt;module&gt;() 1 from google.colab import files 2 ----&gt; 3 uploaded = files.upload() 4 5 for fn in uploaded.keys(): /usr/local/lib/python3.7/dist-packages/google/colab/files.py in upload() 62 result = _output.eval_js( 63 'google.colab._files._uploadFiles(&quot;{input_id}&quot;, &quot;{output_id}&quot;)'.format( ---&gt; 64 input_id=input_id, output_id=output_id)) 65 files = _collections.defaultdict(_six.binary_type) 66 # Mapping from original filename to filename as saved locally. /usr/local/lib/python3.7/dist-packages/google/colab/output/_js.py in eval_js(script, ignore_result, timeout_sec) 38 if ignore_result: 39 return ---&gt; 40 return _message.read_reply_from_input(request_id, timeout_sec) 41 42 /usr/local/lib/python3.7/dist-packages/google/colab/_message.py in read_reply_from_input(message_id, timeout_sec) 99 reply = _read_next_input_message() 100 if reply == _NOT_READY or not isinstance(reply, dict): --&gt; 101 time.sleep(0.025) 102 continue 103 if (reply.get('type') == 'colab_reply' and KeyboardInterrupt: 데이터 다운로드 및 불러오기1!kaggle competitions list Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4) ref deadline category reward teamCount userHasEntered --------------------------------------------- ------------------- --------------- --------- --------- -------------- contradictory-my-dear-watson 2030-07-01 23:59:00 Getting Started Prizes 63 False gan-getting-started 2030-07-01 23:59:00 Getting Started Prizes 81 False store-sales-time-series-forecasting 2030-06-30 23:59:00 Getting Started Knowledge 487 False tpu-getting-started 2030-06-03 23:59:00 Getting Started Knowledge 157 False digit-recognizer 2030-01-01 00:00:00 Getting Started Knowledge 1459 False titanic 2030-01-01 00:00:00 Getting Started Knowledge 14879 False house-prices-advanced-regression-techniques 2030-01-01 00:00:00 Getting Started Knowledge 4418 True connectx 2030-01-01 00:00:00 Getting Started Knowledge 263 False nlp-getting-started 2030-01-01 00:00:00 Getting Started Knowledge 1321 False competitive-data-science-predict-future-sales 2022-12-31 23:59:00 Playground Kudos 12891 False g-research-crypto-forecasting 2022-02-01 23:59:00 Featured $125,000 148 False petfinder-pawpularity-score 2022-01-13 23:59:00 Research $25,000 1631 False optiver-realized-volatility-prediction 2022-01-10 23:59:00 Featured $100,000 3852 False nfl-big-data-bowl-2022 2022-01-06 23:59:00 Analytics $100,000 0 False sartorius-cell-instance-segmentation 2021-12-30 23:59:00 Featured $75,000 495 False wikipedia-image-caption 2021-12-09 11:59:00 Playground Swag 71 False lux-ai-2021 2021-12-06 23:59:00 Featured $10,000 928 False tabular-playground-series-nov-2021 2021-11-30 23:59:00 Playground Swag 352 False kaggle-survey-2021 2021-11-28 23:59:00 Analytics $30,000 0 False chaii-hindi-and-tamil-question-answering 2021-11-15 23:59:00 Research $10,000 807 False 1!kaggle competitions download -c house-prices-advanced-regression-techniques User cancelled operation 1234import pandas as pd train = pd.read_csv('train.csv')test = pd.read_csv('test.csv')print('Data Loading is done!') Data Loading is done! 데이터 둘러보기12print(&quot;The shape of Train Data is:&quot;, train.shape)print(&quot;The shape of Test Data is:&quot;, test.shape) The shape of Train Data is: (1460, 81) The shape of Test Data is: (1459, 80) 1print(train.info()) &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 1460 entries, 0 to 1459 Data columns (total 81 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Id 1460 non-null int64 1 MSSubClass 1460 non-null int64 2 MSZoning 1460 non-null object 3 LotFrontage 1201 non-null float64 4 LotArea 1460 non-null int64 5 Street 1460 non-null object 6 Alley 91 non-null object 7 LotShape 1460 non-null object 8 LandContour 1460 non-null object 9 Utilities 1460 non-null object 10 LotConfig 1460 non-null object 11 LandSlope 1460 non-null object 12 Neighborhood 1460 non-null object 13 Condition1 1460 non-null object 14 Condition2 1460 non-null object 15 BldgType 1460 non-null object 16 HouseStyle 1460 non-null object 17 OverallQual 1460 non-null int64 18 OverallCond 1460 non-null int64 19 YearBuilt 1460 non-null int64 20 YearRemodAdd 1460 non-null int64 21 RoofStyle 1460 non-null object 22 RoofMatl 1460 non-null object 23 Exterior1st 1460 non-null object 24 Exterior2nd 1460 non-null object 25 MasVnrType 1452 non-null object 26 MasVnrArea 1452 non-null float64 27 ExterQual 1460 non-null object 28 ExterCond 1460 non-null object 29 Foundation 1460 non-null object 30 BsmtQual 1423 non-null object 31 BsmtCond 1423 non-null object 32 BsmtExposure 1422 non-null object 33 BsmtFinType1 1423 non-null object 34 BsmtFinSF1 1460 non-null int64 35 BsmtFinType2 1422 non-null object 36 BsmtFinSF2 1460 non-null int64 37 BsmtUnfSF 1460 non-null int64 38 TotalBsmtSF 1460 non-null int64 39 Heating 1460 non-null object 40 HeatingQC 1460 non-null object 41 CentralAir 1460 non-null object 42 Electrical 1459 non-null object 43 1stFlrSF 1460 non-null int64 44 2ndFlrSF 1460 non-null int64 45 LowQualFinSF 1460 non-null int64 46 GrLivArea 1460 non-null int64 47 BsmtFullBath 1460 non-null int64 48 BsmtHalfBath 1460 non-null int64 49 FullBath 1460 non-null int64 50 HalfBath 1460 non-null int64 51 BedroomAbvGr 1460 non-null int64 52 KitchenAbvGr 1460 non-null int64 53 KitchenQual 1460 non-null object 54 TotRmsAbvGrd 1460 non-null int64 55 Functional 1460 non-null object 56 Fireplaces 1460 non-null int64 57 FireplaceQu 770 non-null object 58 GarageType 1379 non-null object 59 GarageYrBlt 1379 non-null float64 60 GarageFinish 1379 non-null object 61 GarageCars 1460 non-null int64 62 GarageArea 1460 non-null int64 63 GarageQual 1379 non-null object 64 GarageCond 1379 non-null object 65 PavedDrive 1460 non-null object 66 WoodDeckSF 1460 non-null int64 67 OpenPorchSF 1460 non-null int64 68 EnclosedPorch 1460 non-null int64 69 3SsnPorch 1460 non-null int64 70 ScreenPorch 1460 non-null int64 71 PoolArea 1460 non-null int64 72 PoolQC 7 non-null object 73 Fence 281 non-null object 74 MiscFeature 54 non-null object 75 MiscVal 1460 non-null int64 76 MoSold 1460 non-null int64 77 YrSold 1460 non-null int64 78 SaleType 1460 non-null object 79 SaleCondition 1460 non-null object 80 SalePrice 1460 non-null int64 dtypes: float64(3), int64(35), object(43) memory usage: 924.0+ KB None 1print(test.info()) &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 1459 entries, 0 to 1458 Data columns (total 80 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Id 1459 non-null int64 1 MSSubClass 1459 non-null int64 2 MSZoning 1455 non-null object 3 LotFrontage 1232 non-null float64 4 LotArea 1459 non-null int64 5 Street 1459 non-null object 6 Alley 107 non-null object 7 LotShape 1459 non-null object 8 LandContour 1459 non-null object 9 Utilities 1457 non-null object 10 LotConfig 1459 non-null object 11 LandSlope 1459 non-null object 12 Neighborhood 1459 non-null object 13 Condition1 1459 non-null object 14 Condition2 1459 non-null object 15 BldgType 1459 non-null object 16 HouseStyle 1459 non-null object 17 OverallQual 1459 non-null int64 18 OverallCond 1459 non-null int64 19 YearBuilt 1459 non-null int64 20 YearRemodAdd 1459 non-null int64 21 RoofStyle 1459 non-null object 22 RoofMatl 1459 non-null object 23 Exterior1st 1458 non-null object 24 Exterior2nd 1458 non-null object 25 MasVnrType 1443 non-null object 26 MasVnrArea 1444 non-null float64 27 ExterQual 1459 non-null object 28 ExterCond 1459 non-null object 29 Foundation 1459 non-null object 30 BsmtQual 1415 non-null object 31 BsmtCond 1414 non-null object 32 BsmtExposure 1415 non-null object 33 BsmtFinType1 1417 non-null object 34 BsmtFinSF1 1458 non-null float64 35 BsmtFinType2 1417 non-null object 36 BsmtFinSF2 1458 non-null float64 37 BsmtUnfSF 1458 non-null float64 38 TotalBsmtSF 1458 non-null float64 39 Heating 1459 non-null object 40 HeatingQC 1459 non-null object 41 CentralAir 1459 non-null object 42 Electrical 1459 non-null object 43 1stFlrSF 1459 non-null int64 44 2ndFlrSF 1459 non-null int64 45 LowQualFinSF 1459 non-null int64 46 GrLivArea 1459 non-null int64 47 BsmtFullBath 1457 non-null float64 48 BsmtHalfBath 1457 non-null float64 49 FullBath 1459 non-null int64 50 HalfBath 1459 non-null int64 51 BedroomAbvGr 1459 non-null int64 52 KitchenAbvGr 1459 non-null int64 53 KitchenQual 1458 non-null object 54 TotRmsAbvGrd 1459 non-null int64 55 Functional 1457 non-null object 56 Fireplaces 1459 non-null int64 57 FireplaceQu 729 non-null object 58 GarageType 1383 non-null object 59 GarageYrBlt 1381 non-null float64 60 GarageFinish 1381 non-null object 61 GarageCars 1458 non-null float64 62 GarageArea 1458 non-null float64 63 GarageQual 1381 non-null object 64 GarageCond 1381 non-null object 65 PavedDrive 1459 non-null object 66 WoodDeckSF 1459 non-null int64 67 OpenPorchSF 1459 non-null int64 68 EnclosedPorch 1459 non-null int64 69 3SsnPorch 1459 non-null int64 70 ScreenPorch 1459 non-null int64 71 PoolArea 1459 non-null int64 72 PoolQC 3 non-null object 73 Fence 290 non-null object 74 MiscFeature 51 non-null object 75 MiscVal 1459 non-null int64 76 MoSold 1459 non-null int64 77 YrSold 1459 non-null int64 78 SaleType 1458 non-null object 79 SaleCondition 1459 non-null object dtypes: float64(11), int64(26), object(43) memory usage: 912.0+ KB None","link":"/2021/10/28/blog/1028/"},{"title":"BearSoup_Review","text":"&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD 논문 리뷰======= c5589a672e850f5edca309f81ca50915cbf17ada","link":"/2021/11/03/blog/bearSoup_Review/"},{"title":"Making Category","text":"--- title: &quot;Making Category&quot; excerpt :&quot;Credit card&quot; classes: wide categories: -init tags: -python -title -coding last_modified_at: 2021-11-03 --- color 바꾸고 싶은에 안되네 https://www.color-hex.com/color/f4dcdc","link":"/2021/11/04/blog/categories/"},{"title":"Column 뽑아오기","text":"-python: for문으로 뽑아오기 12345for column in df17_Ea:... print(column)df.coulumns#이건 column 이름 나옴 경력은 Tenure 인거 같다. 1df18.loc[:0] Question을 다 삭제 해 버렸는데 질문 정히 해 놓은 것이 사라지면 어쩔 수 없이 다시 불러와야지 판다스 옵션 pandas 옵션을 조정하여 생략없이 긴 data를 볼 수 있다. 1pd.describe_option() 1pd.set_option('display.max_seq_items', None) 1234# row 생략 없이 출력pd.set_option('display.max_rows', None)# col 생략 없이 출력pd.set_option('display.max_columns', None) 경력 : [Tenure] in df17, [Q8] in df18, [Q15] in 19, [Q6] in 20, [Q6]in 21연봉 : [Q9] in df18, [Q10] in df19, [Q24] in 20, [Q25]in 21*2017연봉의 경우 ‘CompensationAmount’ 컬럼에 있지만, 통화가 다르므로 하지 말자.","link":"/2021/11/19/blog/columnPOP/"},{"title":"Pandas_DataFrame","text":"#pandas에서 dataFrame 자료구조. dataFrame은 표와 같은 스프레드 시트 형식의 자료 구조이다. 2차원 배열 또는 리스트, data table 전체를 포함하는 object라고 볼수 있음. 여러개의 column이 있고, 각 컬럼은 숫자, 문자열, boolean type을 담을 수 있다. dataFrame은 Rew, column에대한 Index 이렇게 2가지 변수를 담고 있는데 matrix라고 할 수 있다. pd.dataframe()1234567import pandas as pdvalues = [['rose', 'tulip', 'Liry'], [4, 5, 6], ['red', 'blue', 'green']]index = ['flower', 'Number', 'color']columns = [1, 2, 3]df = pd.DataFrame(values, index=index, columns=columns)print(df) df 는 data frame의 준말. 1 2 3 flower rose tulip Liry Number 4 5 6 color red blue green Index와 column 의 dtype은 object이다. 데이터프레임은 리스트(List), 시리즈(Series), 딕셔너리(dict), Numpy의 ndarrays,또 다른 데이터프레임으로 생성할 수 있습니다. Ref.DataFrame","link":"/2021/11/06/blog/dataFrame(pandas)/"},{"title":"to use gitHub in Multi place","text":"깃허브 노트북과 데스크탑 두군데서깃허브를 동시에 사용하고 싶은 분들은아래 코드 확인해서 해보세요. 이 때, 깃헙 blog 저장소 삭제할 필요가 없어요~ 123456789101112$ hexo init myblog # 여기는 각자 소스 레포 확인$ cd myblog$ git init $ git remote add origin https://github.com/rain0430/myblog.git # 각자 소스 레포 주소$ git pull --set-upstream origin main # 에러 발생$ git clean -d -f$ git pull --set-upstream origin main # 에러 발생 안함 / 소스 확인$ npm install $ hexo clean$ hexo generate$ hexo server 저장 해 놓고 나중에 해 봐야지 .","link":"/2021/11/02/blog/gitHub_multi/"},{"title":"Hello World","text":"Welcome to Hexo!This is your very first post. Check documentationfor more info. If you get any problems when using Hexo,you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2021/10/01/blog/hello-world/"},{"title":"machine Learning_basic","text":"machine learningDecision tree learing Decision tree 란 관측값과 목표값을 연결 시켜주는 예측 모델 통계학과 데이터 마이닝, 기계학습에서 예측모델링으로 사용하는 방법 데이터 기능에서 유추된 결정규칙을 학습하여 대상 변수값을 예측 하는 모델을 만들기 위해 사용 ####종류 분류 트리 변수가 유한한 수의 값을 가지는 것, 클래스 출력 Leaf node는 클래스 라벨을 나타내고 가지는 클래스 라벨과 관련있는 특징들의 논리 곱을 나타낸다. 회귀트리 목표변수가 연속하는 값(일반적으로 실수 )을 가지는 트리 특정 의미를 가지는 실수값을 출력 의사결정 분석에서 결정트리는 시각적으로 명시적인 방법으로 과정을 보여준다. 결정트리의 학습 결정 트리의 학습 : 자료 집합을 적절한 분할기준 또는 분할 테스트에 따라 부분집합들로 나누는 과정 하향식 결정 트리 귀납법 (TDIDT_top-down induction of decision trees) 순환분할 방식으로 나눠진 자료의 부분집합에 재귀적으로 반복됨. 분할로 인해 더이상 새로운 예측값이 추가 되지 않거나 부분집합의 노드가 목표변수와 같은 값을 지닐대 까지 계속됨. 데이터 마이닝에서 결정트리 수학 적으로 표현됨 예를 들어 아래와 같은 데이터 마이닝에서의 결정 트리가 있다고 가정 해 보자. ( f{x},Y ) = (x_1, x_2, x_3, ..., x_k, Y) 종속 변수 Y는 분류를 통해 학습하고자 하는 목표 변수이며, 벡터 x는 { x_{1}, x_{2}, x_{3} } x_1, x_2, x_3 등의 입력 변수로 구성된다. 장점1. 이해하기 쉬우며 시각화 가능 2. data 준비가 거의 필요하지 않음 3. 수치형, 범주형 data를 모두 처리 가능 4. multi-output problems를 다룰 수 있다. 5. a white box model을 사용 할 수 있다. (bool 가능) 6. 통계 검정을 사용하여 모형을 검증하기 때문에 모델의 신뢰성을 설명할 수 있다. 7. 생성된 데이터가 실제 모형에 의해 가정이 다소 위반되더라도 잘 수행된다. 단점 data 일반화가 잘 되지 못하면 복잡한 트리가 만들어짐 (과적합) - 가지치기, 리프노드에 필요한 샘플 최소화, 트리 최대깊이 설정 으로 해결 가능 variation이 작은 경우 의사결정트리가 불안정 할 수 있다. 앙상블(ensemble) 내에서 의사결정 트리 사용으로 해결 가능. 실용적인 의사 결정 트리 학습 알고리즘은 각 노드에서 국소적으로 최적의 의사결정이 이루어지는 그리디 알고리즘과 같은 경험적 알고리즘을 기반으로 한다. 이러한 알고리즘은 전역 최적 의사 결정 트리를 반환한다고 보장할 수 없다. 이는 특징과 샘플이 교체와 함께 무작위로 샘플링되는 앙상블 학습기에서 여러 트리를 훈련시킴으로써 완화될 수 있다. 의사 결정 트리의 예측은 근사치이기 때문에 좋은 추정은 아닐 수 있다. 의사결정 트리의 최적화의 문제는 NP-complete로 잘 알려진 문제이다.잘 모르겠다. NP-complete XOR, parity or multiplexer problems와 같이 배우기 쉽지 않은 컨셉들 때문에 표현하기 쉽지않다. 학습자가 편향된 트리르르 만들 수 있으므로, 데이터 세트의 균형을 맞춰줘야 한다. 이어지는 posting classification Regression Ref. 결정트리학습법/wiki 결정트리 국문 Decusuib Trees 1.10.1 1.10.2","link":"/2021/11/04/blog/machineLearning_basic/"},{"title":"Method Making New Repository","text":"Hello World I’m in the Mars Github에서###새로운 저장소 만들기 git hub 로그인 후 새로운 저장소를 만들어 보도록 합시다. new 를 누르면 새로운 repository를 생성하기 위한 정보를 입렬 할 수있다. Repository name 입력 후 Public(전체 공개)으로 할 것인지, Private(비공개)로 할 것인지 선택한다. 마지막으로 create repository를 누르면 생성됨. 생성된 repository에 README.md file을 생성해 봅시다. git bash를 열어봅시다. (원하는 경로에서) git bash 에서 github에서 만든 file을 내려받아 봅니다.1$ git clone + 나의 경로 혹시 나의 경로를 어디서 찾는지 잘 모르는 나를 위해 남긴다. git bash네… bush인줄알았는데… 원하는 경로에 file이 생긴 것을 볼 수 있다. 이제 README.md file 을 만들어 보자. 위의 My path image를 보면 아랫쪽에 코드가 나와있는 것을 볼 수 있다. New file을 파이참으로 열어준 후 아래 코드를 한줄씩 입력 하면 된다. (위 사진에 나와 있는 코드!! )1234567echo &quot;# New&quot; &gt;&gt; README.mdgit initgit add README.mdgit commit -m &quot;first commit&quot;git branch -M maingit remote add origin https://github.com/ 이 부분이 사람마다 다르니 웹페이지 보고 하기!git push -u origin main 이제 저장소와 Desktop file이 페어링 되었다. 앞으로는 file Update후 git 명령어로 올리면 된다. 예를 들자면, folder에 file을 넣고, pycham으로 foler를 열어서 다음과 같은 명령어를 넣으면 된다. 123git add . -- 모든 file을 업로드 하기 위해 저장git commit -m &quot;history Log로 확인 할수 있는 message&quot; -- 확인git push -- 최종적으로 file을 git hub에 올림 이제 전세계 어디에 있던 대용량 파일 저장소를 직접 손에 들고 다닐 일이 없어진거다. (물론 인터넷이 잘 된다는 가정 하에서….)하지만, 나는 오늘도 노파심에 외장하드를 들고 나왔다 하하 앞으로 github에 다 정리해서 넣고 외장하드 안가지고 다녀야지 ^^ 화이팅 !!","link":"/2021/10/28/blog/make_NewRepository/"},{"title":"Making Blog method","text":"Hello World I’m in the Mars Hi my name is YoonHwa Park ! just call me Yoon ^^. Nice to meet you.Today’s Topic is Making Blog !!! it is not easy to make. (ㅜ_ㅜ) but We Can DO IT ^0^!! If you try to do….(…may be…)Let’s Start!! ###깃허브 블로그 만드는 방법####들어가기 전에! Install Applications gitHub : https://github.com/ pycham : https://www.jetbrains.com/pycharm/ nodejs : https://nodejs.org/en/ ※ 주의 : 설치 중에 SYSTEM PATH 관련 텝이 나오면 Check 하고 넘어가기 회원가입 github : mail 로 회원가입 하기. 나머지 회원가입 권유는 무시해도됨. (하고 싶으면 하시오.) 1$ hexo new &quot;My New Post&quot; Ref.this file is written by MARKDOWN마크다운 기초 확인 하고 작성https://gist.github.com/ihoneymon/652be052a0727ad59601","link":"/2021/10/28/blog/make_blog/"},{"title":"Deploy gitHub blog by hexo","text":"##github blog를 Hexo를 이용하여 deploy 해 보자. Ref. https://dschloe.github.io/settings/hexo_blog/ ###Hexo를 이용하여 블로그를 만들어 보자 Node js를 download 해 준다. Node js 설치 GitBash 에서 node의 version을 확인 1$ node -v Hexo를 설치 해 준다. Hexo의 경우 npm을 이용하여 설치 한다. 1$ npm install -g hexo-cli git bash를 적당한 경로에서 들어간다.makeBlog folder를 만들어준다. 12345$ mkdir makeBlog$ cd makeBlog$ hexo init myblog hexo init 을 myblog에서 해 준다.hexo server와 deployer를 설치 해 준다. 이를 설치 하지 않으면 에러가 날 수 있다. 1234$ cd myblog$ npm install$ npm install hexo-server --save$ npm install hexo-deployer-git --save","link":"/2021/11/04/blog/nodjs/"},{"title":"Pandas_panel","text":"#pandas에서 panel 자료구조.","link":"/2021/11/06/blog/panel(pandas)/"},{"title":"Lecture_python_basic","text":"Hello World12print(&quot;Hello, world!&quot;)print(&quot;hi ^0^&quot;) Hello, world! 주석처리12345# 한 줄 주석 처리&quot;&quot;&quot;여러 줄 주석 예제 동일한 따옴표(큰따옴표 혹은 작은 따옴표) 세 개와 세 개 사이에는어떠한 내용, 몇 줄이 들어가더라도 모두 주석으로 처리된다.&quot;&quot;&quot;print(&quot;Hello, world!&quot;) Hello, world! 변수의 종류12num_int = 1print(type(num_int)) &lt;class 'int'&gt; 12num_float = 0.2print(type(num_float)) &lt;class 'float'&gt; 12bool_true = Trueprint(type(bool_true)) &lt;class 'bool'&gt; 12none_x = Noneprint(type(none_x)) &lt;class 'NoneType'&gt; 사칙 연산123456789a = 3b = 2print('a + b = ', a+b)print('a - b = ', a-b)print('a * b = ', a*b)print('a / b = ', a/b)print('a // b = ', a//b)print('a % b = ', a%b)print('a ** b = ', a**b) a + b = 5 a - b = 1 a * b = 6 a / b = 1.5 a // b = 1 a % b = 1 a ** b = 9 123456789c = 3.0d = 2.0print('c + d =', c+d)print('c - d =', c-d)print('c * d =', c*d)print('c / d =', c/d)print('c // d =', c//d)print('c % d =', c%d)print('c ** d =', c**d) c + d = 5.0 c - d = 1.0 c * d = 6.0 c / d = 1.5 c // d = 1.0 c % d = 1.0 c ** d = 9.0 논리형 연산자1234print(True and True)print(True and False)print(False and True)print(False and False) True False False False 1234print(True or True)print(True or False)print(False or True)print(False or False) True True True False 비교 연산자123456print(4 &gt; 3)print(4 &lt; 3)print(4 &gt;= 3)print(4 &lt;= 3)print(4 &gt; 4)print(4 &gt;= 4) True False True False False True 논리형 &amp; 비교 연산자 응용123456#input(&quot;숫자를 입력하세요&quot;)_연습하기data = input (&quot;숫자를 입력하세요.&quot;)data2 =int(data)print(type(data2))# class 가 달라진다. data = string , data 2 = int -&gt; 형변환 12345678num1 = int(input(&quot;첫번째 숫자를 입력하세요: &quot;))num2 = int(input(&quot;두번째 숫자를 입력하세요: &quot;))num3 = int(input(&quot;세번째 숫자를 입력하세요: &quot;))num4 = int(input(&quot;네번째 숫자를 입력하세요: &quot;))var1 = num1 &gt;= num2var2 = num3 &lt; num4print(var1 and var2) 첫번째 숫자를 입력하세요: 10 두번째 숫자를 입력하세요: 20 세번째 숫자를 입력하세요: 30 네번째 숫자를 입력하세요: 11 False String12print(&quot;'Hello, world!'&quot;)print('&quot;Hello, world!&quot;') 'Hello, world!' &quot;Hello, world!&quot; String Operators123456str1 = &quot;Hello &quot;str2 = &quot;World &quot;print('str1 + str2 = ', str1 + str2)greet = str1 + str2print('greet * 3 = ', greet * 3) str1 + str2 = Hello World greet * 3 = Hello World Hello World Hello World Indexing12greeting = &quot;Hello Kaggle&quot;print(greeting[6]) K Slicing123456greeting = &quot;Hello Kaggle&quot;print(greeting[:])print(greeting[6:])print(greeting[:6])print(greeting[3:8])print(greeting[0:9:2]) Hello Kaggle Kaggle Hello lo Ka HloKg 1greeting[13] 리스트1234567891011a = [] # 빈 리스트a_func = list() #list()함수로도 빈 리스트를 만들 수 있다.b = [1] # 숫자도 요소가 될 수 있다.c = ['apple'] # 문자열도 요소가 될 수 있다d = [1, 2, ['apple']] # 리스트 안에 리스트를 요소로 넣을 수 있다.print(a)print(a_func)print(b)print(c)print(d) [] [] [1] ['apple'] [1, 2, ['apple']] 123456a = [1, 2, 3]# index [[0], [1], [2]]print(a[0]) # 첫번째 요소print(a[1]) # 두번째 요소print(a[2]) # 세번째 요소print(a[-1]) 1 2 3 3 1234567a = [['apple','banana','cherry'], 1]print(a[0]) # 리스트 내의 리스트print(a[0][0]) # 리스트 내의 리스트의 첫번째 문자열print(a[0][0][3]) # 리스트 내의 리스트의 첫번째 문자열 'apple' 중 첫번째 인덱스print(a[0][1]) # 리스트 내의 리스트의 두번째 문자열print (a[0][2]) ['apple', 'banana', 'cherry'] apple l banana cherry 12345678910111213a = [1,2,3,4,5,6,7,8,9,10]b = a[:4] # 인덱스 0부터 3까지c = a[1:4] # 인덱스 1부터 3까지d = a[0:7:2] # 인덱스 0부터 6까지 인덱스 2씩 건너 띄우기e = a[::-1] # 리스트 a의 역순f = a[::2] # 리스트 전체구간에서 인덱스 2씩 건너띄우기print(&quot;a[:4]&quot;, b)print(&quot;a[1:4]&quot;, c)print(&quot;a[0:7:2]&quot;, d)print(&quot;a[::-1]&quot;, e)print(&quot;a[::2]&quot;, f) a[:4] [1, 2, 3, 4] a[1:4] [2, 3, 4] a[0:7:2] [1, 3, 5, 7] a[::-1] [10, 9, 8, 7, 6, 5, 4, 3, 2, 1] a[::2] [1, 3, 5, 7, 9] 12345a = ['alice', 'bob', 'cat']b = ['apple', 'banana', 'cherry']c = a+bprint(c) ['alice', 'bob', 'cat', 'apple', 'banana', 'cherry'] 12345a = ['a','b','c']b = a*3c = a*0print(&quot;a * 3:&quot;, b)print(&quot;a * 0:&quot;, c) a * 3: ['a', 'b', 'c', 'a', 'b', 'c', 'a', 'b', 'c'] a * 0: [] 리스트 값 수정하기1234a = [0,1,2]a[1] = &quot;b&quot;print(a) [0, 'b', 2] 리스트 값 추가하기123456a = [100, 200, 300]a.append(400)print(a)a.append([500,600])print(a) [100, 200, 300, 400] [100, 200, 300, 400, [500, 600]] 1234a = [1,2,3]a.extend([40,500])print('a.extend([40,500]) result')print(a) a.extend([40,500]) result [1, 2, 3, 40, 500] 1234a = [0,1,2]a.insert(1,100)print(a) [0, 100, 1, 2] 12345678910111213a = [0,1,2,3]a[2:2] = [100,200]print(a)# 시작과 끝의 범위보다 큰 수를 덮어쓰는 예시b = [0,1,2,3]b[1:2] = [100,200,300,400] print(b)# 시작과 끝의 범위가 작을때의 예시c = [0,1,2,3]c[1:3] = [100]print(c) [0, 1, 100, 200, 2, 3] [0, 100, 200, 300, 400, 2, 3] [0, 100, 3] 리스트 값 삭제하기123456789a =[1,2,1,2]#리스트의 첫번째 1이 삭제a.remove(1)print(a)#리스트의 두번째 1이 삭제a.remove(1)print(a) [2, 1, 2] [2, 2] 12345678910a = [0,1,2,3,4,5,6,7,8,9]# 1 삭제del a[1]print(a)b = [0,1,2,3,4,5,6,7,8,9]# 범위로 삭제del b[1:3] #list는 항상 시작하는 index부터, 종료하는 n의 n-1까지의 범위를 잡아줍니다.print(b) [0, 2, 3, 4, 5, 6, 7, 8, 9] [0, 3, 4, 5, 6, 7, 8, 9] 123456#인덱스를 지정한 pop()a = [0,1,2,3,4]r = a.pop(1)print(a)print(r) [0, 2, 3, 4] 1 123456#인덱스를 지정하지 않은 pop()b = ['a','b','c','d']x = b.pop()print(b)print(x) ['a', 'b', 'c'] d 그 외 유용한 메서드12345a = [0,1,2,3]print(a)a.clear()print(a) [0, 1, 2, 3] [] 12a = [&quot;Gold&quot;, &quot;Gold&quot;, &quot;Silver&quot;, &quot;Silver&quot;]print(&quot;Silver가 처음 등장하는 인덱스 번호&quot;, a.index(&quot;Silver&quot;)) Silver가 처음 등장하는 인덱스 번호 2 12345678a = [1, 4, 5, 2, 3]b = [1, 4, 5, 2, 3]a.sort()print(&quot;sort():&quot;,a)b.sort(reverse=True)print(&quot;sort(reverse=True):&quot;, b) sort(): [1, 2, 3, 4, 5] sort(reverse=True): [5, 4, 3, 2, 1] 1234b = [4,3,2,'a']b.sort()print(b) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-38-1624da3f09a9&gt; in &lt;module&gt;() 1 b = [4,3,2,'a'] 2 ----&gt; 3 b.sort() 4 print(b) TypeError: '&lt;' not supported between instances of 'str' and 'int' 튜플1234567891011tuple1 = (0) # 끝에 콤마(,)를 붙이지 않았을 때tuple2 = (0,) # 끝에 콤마(,)를 붙여줬을 때tuple3 = 0,1,2print(tuple1)print(tuple2)print(tuple3)print(type(tuple1)) # 콤마(,)를 붙여주지 않으면 튜플이 아닙니다.print(type(tuple2)) # 콤마(,)를 붙여주어야 튜플 자료형 입니다.print(type(tuple3)) # 여러개의 값 일경우 괄호를 없애주어도 튜플 자료형 입니다. 0 (0,) (0, 1, 2) &lt;class 'int'&gt; &lt;class 'tuple'&gt; &lt;class 'tuple'&gt; 12a = (0,1,2,3,'a')del a['a'] --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-41-c41b8ecfc68f&gt; in &lt;module&gt;() 1 a = (0,1,2,3,'a') ----&gt; 2 del a['a'] TypeError: 'tuple' object does not support item deletion 12a = (0,1,2,3,'a')a[1]='t' --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-42-04fb068f82e0&gt; in &lt;module&gt;() 1 a = (0,1,2,3,'a') ----&gt; 2 a[1]='t' TypeError: 'tuple' object does not support item assignment 튜플 인덱싱 및 슬라이싱 하기1234t = (0,1,2,'b',4)print(t[1])print(t[3]) 123t = (0,1,2,3,4)print(t[2:])print(t[0:2]) (2, 3, 4) (0, 1) 더하기 및 곱셈 연산자 사용12345t1 = (0,1,2,3,4)t2 = ('a','b','c')t3 = t1+t2print(t1+t2)print(t3) 123t1 = ('a','b')print(t1*0)print(t1*3) 딕셔너리12345dic = {'teacher':'alice', 'class': 5, 'studentid': '15', 'list':[1,2,3]}print(dic['teacher'])print(dic['class'])print(dic['list']) alice 5 [1, 2, 3] 12dic = {'teacher':'alice', 'class': 5, 'studentid': '15', 'list':[1,2,3]}print(dic['real']) --------------------------------------------------------------------------- KeyError Traceback (most recent call last) &lt;ipython-input-44-fd82dcc94904&gt; in &lt;module&gt;() 1 dic = {'teacher':'alice', 'class': 5, 'studentid': '15', 'list':[1,2,3]} ----&gt; 2 print(dic['real']) KeyError: 'real' 12a = {'name': 'bob', 'job': 'farmer', 'age': 35}a.keys() dict_keys(['name', 'job', 'age']) 12a = {'name': 'bob', 'job': 'farmer', 'age': 35}a.values() dict_values(['bob', 'farmer', 35]) 1234a = {'name': 'chris', 'job': 'painter', 'age': 30}print(a.get('name'))print(a.get('dinner'))print(a.get('dinner', 'empty')) chris None empty 집합 연산자12345678s = {}print(type(s))s = set()print(type(s))s = {1,2,3}print(type(s)) &lt;class 'dict'&gt; &lt;class 'set'&gt; &lt;class 'set'&gt; 1234567a = {1,3,5}b = {2,4,6}c = a|bd = a.union(b)print(&quot;a|b:&quot;, c)print(&quot;a.union(b)&quot;, d) a|b: {1, 2, 3, 4, 5, 6} a.union(b) {1, 2, 3, 4, 5, 6} 1234567891011a = {1,3,5}b = {2,4,6}c = a&amp;bprint(c)e = {1,2,5}f = {2,3,5}g1 = e&amp;fg2 = e.intersection(f)print(&quot;e&amp;f:&quot;, g1)print(&quot;e.intersection(f):&quot;, g2) set() e&amp;f: {2, 5} e.intersection(f): {2, 5} 1234567a = {1,3,5}b = {2,4,5}c1 = a-bc2 = a.difference(b)print(&quot;a-b:&quot;, c1)print(&quot;a.difference(b)&quot;, c2) a-b: {1, 3} a.difference(b) {1, 3} 1234567a = {1,2,3,4,5}b = {3,4,5,6,7}c1 = a^bc2 = a.symmetric_difference(b)print(&quot;a^b&quot;, c1)print(&quot;a.symmetric_difference(b)&quot;, c2) a^b {1, 2, 6, 7} a.symmetric_difference(b) {1, 2, 6, 7} if 조건문12345678910a = -5if a&gt;5: print('a is bigger than 5')elif a &gt; 0: print(&quot;a is bigger than 0 but a is smaller than 5 &quot;)else: print(&quot;a is negative&quot;) a is negative 반복문1print(&quot;Hello World&quot;) Hello World 123print(&quot;Hello World&quot;)print(&quot;Hello World&quot;)print(&quot;Hello World&quot;) Hello World Hello World Hello World 12for i in range(10000): print(&quot;Hello World&quot;) \u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m Hello World Hello World Hello World Hello World (손으로 자름.) Hello World Hello World 1234567a = &quot;Kaggle&quot;for x in a: print(x) if x == 'g': break K a g 123alphabets = ['A', 'B', 'C']for index, value in enumerate(alphabets): print(index, value) 0 A 1 B 2 C","link":"/2021/11/02/blog/python_basic/"},{"title":"Pandas_Series","text":"#Pandas에서 series 자료구조 series는 1차원 배열같은 자료 구조를 말한다. 아래 code는 python pandas의 parameter 값이다. 123def __init__(data=None, index=None, dtype=None, name=None, copy=False, fastpath=False) series의 parameter는 data, index, dtype, name, copy, fastpath로 나뉘어져 있는데name의 경우는 이름 인 것 같고 기본적으로 Index와 value라는 parameter를 많이 이용 하는 듯 하다. Index : 배열의 이름 value : 값 python의 dictionalry와 거의 유사 한 것 같다. (다음에 찾아보자 오늘은 벅참.) series의 dtype에는 str, numpy.dtype, or ExtensionDtype, optional Data type 을담을 수 있는데 이는 자동으로 값이 입력 되는 것같다. series 객체를 생성 할 때 value와 Index를 직접 지정 해 줄 수 있다. 1234import pandas as pdsr = pd.Series([24000, 20000, 1000, 5000], index=[&quot;피자&quot;, &quot;치킨&quot;, &quot;콜라&quot;, &quot;생맥&quot;])print(sr) 구글 코랩에서 작업 하고 있는데,아래에 보면 series 객체의 parameter에대한 팝업이 나와 공부 하기 참 편하게 해 준다. def init(data=None, index=None, dtype=None, name=None, copy=False, fastpath=False)One-dimensional ndarray with axis labels (including time series). Labels need not be unique but must be a hashable type. The object supports both integer- and label-based indexing and provides a host of methods for performing operations involving the index. Statistical methods from ndarray have been overridden to automatically exclude missing data(currently represented as NaN).Operations between Series (+, -, /, *, **) align values based on their associated index values– they need not be the same length. The result index will be the sorted union of the two indexes. Parameters data : array-like, Iterable, dict, or scalar value Contains data stored in Series. index : array-like or Index (1d) Values must be hashable and have the same length as data. Non-unique index values are allowed. Will default to RangeIndex (0, 1, 2, …, n)if not provided. If both a dict and index sequence are used, the index willoverride the keys found in the dict. dtype : str, numpy.dtype, or ExtensionDtype, optional Data type for the output Series. If not specified, this will be inferred from data. See the user guide &lt;basics.dtypes&gt; for more usages. name : str, optional The name to give to the Series. copy : bool, default False Copy input data. type list [] tuple () set {} dict {Key:value} series pandas","link":"/2021/11/06/blog/series(pandas)/"},{"title":"East Asia, Data Scientist (kaggle in East-Asia)","text":"Data scientist in East Asia Data scientist로써 East Asia 에서 살아 남아보자 !Data ImportEast asia data에서 DS(Data scientist)뽑아내기12345678910111213141516171819202122232425262728# 21년 EastAsia 의 Data Scientist# 21년 EastAsia의 Data Scientist는 'Data Scientist', 'Machine Learning Engineer', 'Scientist/Researcher'# Data_Scientist =['Data Scientist', 'Research Scientist', 'Researcher','Machine Learning Engineer', 'Scientist/Researcher']df21_Ea_DS = df21_Ea[df21_Ea['Q5'].isin(Data_Scientist)]#21년 EastAsia의 Data Scientist 설문조사 응답자 리스트 dgree_order=[ '~college','Bachelor’s degree', 'Master’s degree', 'Doctoral degree~', 'etc']sal_order=['&lt; 999', '1,000-7,499', '7,500-24,999', '25,000-59,999', '60,000-99,999','100,000-199,999', '200,000~']df21_Ea_DS=(df21_Ea_DS.replace(['0-999','$0-999','0'], '&lt; 999') .replace({'&gt;$1,000,000':'200,000~'}) .replace(['1,000-1,999','2,000-2,999','3,000-3,999', '4,000-4,999','5,000-7,499'],'1,000-7,499') .replace(['7,500-9,999','10,000-14,999', '15,000-19,999', '20,000-24,999'],'7,500-24,999') .replace(['25,000-29,999','30,000-39,999', '40,000-49,999', '50,000-59,999'],'25,000-59,999') .replace(['60,000-69,999', '70,000-79,999', '80,000-89,999', '90,000-99,999'], '60,000-99,999') .replace(['100,000-124,999', '300,000-499,999', '125,000-149,999', '125,000-149,999', '150,000-199,999'],'100,000-199,999') .replace(['200,000-249,999', '250,000-299,999', '1,000,000','$500,000-999,999'], '200,000~') .replace({'I prefer not to answer':'etc'}).replace(['No formal education past high school', 'Some college/university study without earning a bachelor’s degree'],'~college') .replace(['Doctoral degree', 'Professional doctorate'],'Doctoral degree~')) replace.() replace.()를 data를 뽑아 내면서 사용 하면, 편하다. 다음 project 부터는 그렇게 사용하자! 순차적으로 적용 하더라도 replace.()는 맨 앞에 사용하자. data를 정제할 때 구획을 어디서 나누느냐는 presentation에 중요한 구성 요소이다.(강조할 부분이 바뀐다.) Ds의 연봉 뽑아내기123456789101112131415161718192021222324252627df21_Ea_DS_= df21_Ea_DS.loc[:,['Q5','Q25']].reset_index().rename(columns={'Q5':'Data_Scientist', 'Q25':'Salary'}).fillna('etc')df21_Ea_DS_= (df21_Ea_DS_.groupby(['Data_Scientist', 'Salary']).size() .reset_index() .rename(columns = {0:&quot;Count&quot;}) )#Data Scientistdf21_Ea_DS_Ds = df21_Ea_DS_[df21_Ea_DS_['Data_Scientist'] == &quot;Data Scientist&quot;].reset_index(drop = True)df21_Ea_DS_Ds['%']=((df21_Ea_DS_Ds['Count'] / df21_Ea_DS_Ds['Count'].sum())*100).round(2)# Salary21_Ea=Salary21_Ea.sort_values(by='%', ascending=False)#Machine Learning Engineerdf21_Ea_DS_Mle = df21_Ea_DS_[df21_Ea_DS_['Data_Scientist'] == &quot;Machine Learning Engineer&quot;].reset_index(drop = True)df21_Ea_DS_Mle['%']=((df21_Ea_DS_Mle['Count'] / df21_Ea_DS_Mle['Count'].sum())*100).round(2)#Research Scientistdf21_Ea_DS_Rs = df21_Ea_DS_[df21_Ea_DS_['Data_Scientist'] == &quot;Research Scientist&quot;].reset_index(drop = True)df21_Ea_DS_Rs['%']=((df21_Ea_DS_Rs['Count'] / df21_Ea_DS_Rs['Count'].sum())*100).round(2)df21_Ea_DS_Rsdf21_Ea_DS_salary = pd.concat([df21_Ea_DS_Ds, df21_Ea_DS_Mle, df21_Ea_DS_Rs], ignore_index = True)df21_Ea_DS_salary= pd.pivot(df21_Ea_DS_salary, index = &quot;Salary&quot;, columns = 'Data_Scientist', values = &quot;%&quot;).reset_index().fillna('0')df21_Ea_DS_salary= df21_Ea_DS_salary.set_index(&quot;Salary&quot;).reindex(sal_order) 뽑아낸 data를 합쳐서 하나의 표로 만든다. Excel에서 하는게 더 편하고 익숙하지만,python을 능숙 하게 다룰 수 잇는 언젠가가 오지 않을까 싶다. Ds의 연봉 bar graph 만들기123456789101112131415161718192021222324252627282930313233343536fig = go.Figure()fig.add_trace(go.Bar(x = df21_Ea_DS_salary.index, y = df21_Ea_DS_salary['Data Scientist'], name = &quot;Data Scientist&quot;, text = df21_Ea_DS_salary['Data Scientist'].astype(str) + &quot;%&quot;, textposition='auto'))fig.add_trace(go.Bar(x = df21_Ea_DS_salary.index, y = df21_Ea_DS_salary['Machine Learning Engineer'], name = &quot;Machine Learning Engineer&quot;, text = df21_Ea_DS_salary['Machine Learning Engineer'].astype(str) + &quot;%&quot;, textposition='auto'))fig.add_trace(go.Bar(x = df21_Ea_DS_salary.index, y = df21_Ea_DS_salary['Research Scientist'], name = &quot;Research Scientist&quot;, text = df21_Ea_DS_salary['Research Scientist'].astype(str) + &quot;%&quot;, textposition='auto'))fig.update_layout(barmode='stack', showlegend=True, margin=dict(pad=20), height=500, yaxis_title=None, xaxis_title=None, title_text=&quot;&lt;b&gt;21년 East Asia의 Data Scientist의 연봉&lt;/b&gt;&quot;, title_x=0.5, font=dict(size=17, color='#000000'), title_font_size=35)fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.show() [25,000-59,999] 이 구간이 East asia의 data scientist 들의 빈도가 가장 높은 연봉 구간이다. [7,500-24,999] 이 구간을 없애버리고 싶지만 (편입), 우선은 그냥 두기로 한다. HeatMap을 그려보자East asia의 DS들의 연봉과 경력간의 관계를 알아보고자 한다.12345678910111213141516171819202122df21Ea_DS_ExSal = df21_Ea_DS.loc[:,['Q6','Q25']].reset_index().rename(columns={'Q25':'Salary', 'Q6':'Exp'}).fillna('etc')df21Ea_DS_ExSal= (df21Ea_DS_ExSal.groupby(['Exp', 'Salary']).size().unstack().fillna(0).astype('int64'))# df21Ea_DS_ExSal['Exp'].unique()Exp_order=['&lt; 1 years','1-3 years','3-5 years', '5-10 years', '10-20 years', '20+ years', 'I have never written code']df21Ea_DS_ExSalz = df21Ea_DS_ExSalz = z[sal_order]z = z.reindex(Exp_order)z_data = z.apply(lambda x:np.round(x/x.sum()*100, 2), axis = 1).to_numpy() # convert to correlation matrixx = sal_ordery = Exp_orderfig = ff.create_annotated_heatmap(z_data, x = x, y = y, colorscale = &quot;sunset&quot;)fig.update_layout( title_text=&quot;&lt;b&gt;Data Scientist의 경력과 연봉 &lt;/b&gt;&quot;, height=700, width=800, title_x=0.5, margin=dict(l=200, r=100, t=200, b=100))fig.show() data Scientist의 경력과 연봉 상관관계를 Heatmap으로 그렸다. 정말 재미있는 사실은 [7,500-24,999], [60,000-99,999] 등의 구간이 비어 보인다. 혹시 연봉이 반올림되는 구간이 아닐까 생각한다. 다음에 연봉 구획을 다시 나눈다면 이런 부분을 신경쓰면서 나누어야 할 듯. [&lt;999] 구간은 생각보다 비율이 높은 걸을 알 수 있는데 이는 survey의 오류인듯 하다. 연봉인데 월급으로 생각했다던가… 1234567891011121314df21_Ea_degree = df21_Ea_DS['Q4'].value_counts().to_frame()degree = df21_Ea_degree.indexvalues = df21_Ea_degree['Q4'].tolist()fig = go.Figure(data=[ go.Bar(name='Degree', x=degree, y=values ,orientation='v')])fig.update_layout( title_text=&quot;&lt;b&gt;21년 East Asia의 Data Scientist의 학력&lt;/b&gt;&quot;, )fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.show() 1234567891011121314151617181920212223df21Ea_DS_EduSal= df21_Ea_DS.loc[:, ['Q4', 'Q25']].rename(columns={'Q4':'Edu', 'Q25':'Salary'})df21Ea_DS_EduSal['Edu'].unique()Edu_order=['~college', 'Bachelor’s degree','Master’s degree', 'Doctoral degree~', 'etc']df21Ea_DS_EduSal= (df21Ea_DS_EduSal.groupby(['Edu', 'Salary']).size().unstack().fillna(0).astype('int64'))df21Ea_DS_EduSalz = df21Ea_DS_EduSalz = z[sal_order]z = z.reindex(Edu_order)z_data = z.apply(lambda x:np.round(x/x.sum()*100, 2), axis = 1).to_numpy() # convert to correlation matrixx = sal_ordery = Edu_orderfig = ff.create_annotated_heatmap(z_data, x = x, y = y, colorscale = &quot;sunset&quot;)fig.update_layout( title_text=&quot;&lt;b&gt;Data Scientist의 경력과 연봉 &lt;/b&gt;&quot;, height=700, width=800, title_x=0.5, margin=dict(l=200, r=100, t=200, b=100))fig.show() East Asia의 ds들의 연봉은 거의 [25000-60000] 구간에 들어잇는 것 같다. 학위랑은 많이 상관 없어 보이며 심지어 200,000~$를 받는 학사학력자가 있다. 몹시 바람직하다. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455df20_Ea_DS = df20_Ea[df20_Ea['Q5'].isin(Data_Scientist)]df19_Ea_DS =df19_Ea[df19_Ea['Q5'].isin(Data_Scientist)]df19Ea_DSLag = df19_Ea_DS.loc[:, [ 'Q5', 'Q19', 'year']]df19Ea_DSLag = df19Ea_DSLag.rename(columns = {'Q19': 'Language'}, inplace = False) # To match with other datasetsdf20Ea_DSLag = df20_Ea_DS.loc[:, [ 'Q5', 'Q8', 'year']].rename(columns = {'Q8': 'Language'}, inplace = False)df21Ea_DSLag = df21_Ea_DS.loc[:, [ 'Q5', 'Q8', 'year']].rename(columns = {'Q8': 'Language'}, inplace = False)df3y_Ds_Lag = pd.concat([df19Ea_DSLag, df20Ea_DSLag, df21Ea_DSLag])df3y_Ds_Lag = df3y_Ds_Lag.groupby(['year', 'Language']).size().reset_index().rename(columns = {0:&quot;Count&quot;})df3y_Ds_Lag# 2019dfLang_Ds_19 = df3y_Ds_Lag[df3y_Ds_Lag['year'] == &quot;2019&quot;].reset_index(drop = True)dfLang_Ds_19['percentage'] = dfLang_Ds_19[&quot;Count&quot;] / dfLang_Ds_19[&quot;Count&quot;].sum()dfLang_Ds_19['%'] = np.round(dfLang_Ds_19['percentage'] * 100, 1)# 2020dfLang_Ds_20 = df3y_Ds_Lag[df3y_Ds_Lag['year'] == &quot;2020&quot;].reset_index(drop = True)dfLang_Ds_20['percentage'] = dfLang_Ds_20[&quot;Count&quot;] / dfLang_Ds_20[&quot;Count&quot;].sum()dfLang_Ds_20['%'] = np.round(dfLang_Ds_20['percentage'] * 100, 1)# 2021dfLang_Ds_21 = df3y_Ds_Lag[df3y_Ds_Lag['year'] == &quot;2021&quot;].reset_index(drop = True)dfLang_Ds_21['percentage'] = dfLang_Ds_21[&quot;Count&quot;] / dfLang_Ds_21[&quot;Count&quot;].sum()dfLang_Ds_21['%'] = np.round(dfLang_Ds_21['percentage'] * 100, 1)dfLang_Ds_19=dfLang_Ds_19.sort_values(by='%', ascending=False)dfLang_Ds_20=dfLang_Ds_20.sort_values(by='%', ascending=False)dfLang_Ds_21=dfLang_Ds_21.sort_values(by='%', ascending=False)fig = go.Figure()fig.add_trace(go.Bar(x = dfLang_Ds_19['Language'], y = dfLang_Ds_19['%'], name = &quot;2019&quot;, text = dfLang_Ds_19['%'].astype(str) + &quot;%&quot;, textposition='auto'))fig.add_trace(go.Bar(x = dfLang_Ds_20['Language'], y = dfLang_Ds_20['%'], name = &quot;2020&quot;, text = dfLang_Ds_20['%'].astype(str) + &quot;%&quot;, textposition='auto'))fig.add_trace(go.Bar(x = dfLang_Ds_21['Language'], y = dfLang_Ds_21['%'], name = &quot;2021&quot;, text = dfLang_Ds_21['%'].astype(str) + &quot;%&quot;, textposition='auto'))fig.show() 이 plot은 강사쌤의 도움을 많이 받았다. 2017, 2018년도도 넣고 싶었으나 data 찾는데 너무 시간이 많이 걸리는 것이 대회 마감이 얼마 남지 않은 이 시점에서 바람직 하지 못한 계획이라는 생각이 들어 이쯤에서 만족 하기로 했다. 비록 이 대회에서 우승 하지 못하겠지만,나에게 있어 이번 대회는 의미가 크다. 내 경력에는 큰 의미가 없을지언정 ㅎㅎ 123456789101112131415161718192021222324252627282930313233import numpy as npimport pandas as pdimport seaborn as snsimport matplotlib.pylab as pltimport plotly.io as pioimport plotly.express as pximport plotly.graph_objects as goimport plotly.figure_factory as fffrom plotly.subplots import make_subplotsfrom plotly.offline import init_notebook_mode, iplotinit_notebook_mode(connected=True)pio.templates.default = &quot;none&quot;# import plotly.offline as py# py.offline.init_notebook_mode()import osfor dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename))import warningswarnings.filterwarnings(&quot;ignore&quot;)#joypy를 쓰고 싶다면, sns에 싣어야 하는데 고민.#pip install joypy# import joypy# fig,axes = joypy.joyplot(df_duration, by='year',color=palette18, alpha=0.8)# plt.title('Time taken to complete the survey (in seconds)', size=14, fontweight='bold')# plt.show()# https://ichi.pro/ko/joypyleul-sayonghayeo-joy-plot-mandeulgi-113466494282576 123456789ds_pc=df21_Ea_DS.loc[:, ['Q5','Q25','Q6','Q4','Q8']]fig = px.parallel_categories(ds_pc, labels={'Q5':'Job', 'Q25':'Salary', 'Q6':'Experience', 'Q4':'Degree', 'Q8':'Language'})fig.update_layout(hovermode = 'x')fig.update_layout(title='&lt;b&gt; Data Scientist&lt;/b&gt;',title_font_size=20, margin = dict(t=120, l=100, r=10, b=150), height=600, width=700)fig.show()","link":"/2021/11/23/kgg/Kgg_EastAsia_DataScientist/"},{"title":"Newbie as a data scientist in East Asia! (kaggle Competition)","text":"Newbie as a data scientist in East Asia!notebook Hello, Kaggers! Nice to meet you! We are a team in East Asia that wants to be data scientists As newbies, we want to know what and/or how Kaggler is! so, let’s have a time to learn about Kaggle as a senior with us from now. If you want to support us (or feel qute) , I ask for a comment! (PLZ) ^0^ And !! Since we are not native English speakers, please ask questions if there is a context that you don’t understand because it’s not smooth. I’ll do my best to answer. 1 Introduction what is the Kagglea subsidiary of Google LLC, is an online community of data scientists and machine learning practitioners. If we use kaggle, we can take the following advantages. 1) to find and publish data sets 2) to explore and build models in a web-based data-science environment 3) to work with other data scientists and machine learning engineers 4) to enter competitions to solve data science challenges so, As data scientist beginners, we try to participate in the Kaggle competition. 21 Kaggle Machine Learning and Data Science Survey The most comprehensive dataset available for ML and data science status This is the theme of the competition we will participate in this time. To become a data scientist, we compared what kind of job Kagglers has, how much experience he has, and how much money he earns by dividing into the world and East Asia. In addition, there are detailed comparisons in East Asia, and ultimately, we will to find out what data the Kaggle competition data shows. The 2021 survey, like 2017, 2018, 2019, and 2020, launched an industry-wide survey that comprehensively presents the current status of data science and machine learning. The survey was conducted from 09/01/2021 to 10/04/2021, and after cleaning the data, Kaggle received 25,973 responses! This year, Kaggle will award $30,000 in prize money to winner in this competition. we want to receive $30,000 for winning the competition, but we just hope it will help us become a data scientist because it is difficult for a rookie. Ref. [1] Kgg_competitions [2] Kgg_definition [3] kaggle-survey-2021 1.2 Contents Introduction Contents Summary Data Import and Preprocessing Plots and Description Kaggle's transformation. (World/East_Asia) 1 user transformation 2 Gender transformation 3 Job transformation 4 Age transformation 5 Degree transformation 6 Experience transformation 7 Salary transformation 8 Language transformation Position of Data Scientist in East Asia 1 Salary 2 Salary-Experience 3 Degree 4 Salary-Degree 5 Language Discussion Close 1.3 Summary used data We used all the data for five years. (2017~2021) used Language and Library Numpy Metplotlib seaborn Plotly plotly.express : An interface where you can draw a graph easily and quickly. plotly.graph_objects : You can customize it in the way you want because you can do more detailed work than express. plotly.figure_factory : Used before express existed and remains in the module for compatibility with previous versions plotly.subplots : A module that displays multiple graphs in one figure. plotly.offline : Save locally and create HTML that opens in a web browser and make it standalone Grouping data sections East Asia and World East Asia : [‘China’,’Taiwan’, ‘South Korea’, ‘Japan’] World : all data Gender [Male, Female, Others] Job Data_Analyst =[‘Data Analyst’,’Data Miner,Information technology’,’Data Miner’, 'Predictive Modeler','Information technology, networking, or system administration', 'A business discipline (accounting, economics, finance, etc.)', 'Business Analyst', Humanities', 'Statistician', 'Mathematics or statistics', 'Medical or life sciences (biology, chemistry, medicine, etc.)', Physics or astronomy', 'Social sciences (anthropology, psychology, sociology, etc.)', 'Environmental science or geology', 'Humanities (history, literature, philosophy, etc.)'] Data_Scientist =[‘Data Scientist’, ‘Research Scientist’, ‘Researcher’,’Machine Learning Engineer’, ‘Scientist/Researcher’] Developer=[‘Developer Relations/Advocacy’,’Data Engineer’,’Engineer’,’Engineering (non-computer focused)’, ‘Programmer’,’Software Engineer’, ‘Computer Scientist’,’Computer science (software engineering, etc.)’, ‘Fine arts or performing arts’,’Product Manager’, ‘Software Developer/Software Engineer’, ‘Product/Project Manager’,’Program/Project Manager’,’DBA/Database Engineer’] Not_Employed =[‘Currently not employed’, ‘Not employed’, ‘Student’] Others = [‘I never declared a major’, ‘Other’] Age [18-21, 20s, 30s, 40s, 50s, 60s&lt;] Degree [‘college’, ‘Bachelor’s degree’,’Master’s degree’, ‘Doctoral degree‘, ‘etc’] Experience [&lt;1, 1-3, 3-5, 5-10, 10+] Salary [&lt;999, 1,000-20,000, 20,000-59,999, 60,000-99,999, 100,000-199,999, 200,000~] 2. data Import and pre-treatments 1234567891011121314151617181920import numpy as npimport pandas as pdimport seaborn as snsimport matplotlib.pylab as pltimport plotly.io as pioimport plotly.express as pximport plotly.graph_objects as goimport plotly.figure_factory as fffrom plotly.subplots import make_subplotsfrom plotly.offline import init_notebook_mode, iplotinit_notebook_mode(connected=True)pio.templates.default = &quot;none&quot;import osfor dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename))import warningswarnings.filterwarnings(&quot;ignore&quot;) 12345df17= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2017/multipleChoiceResponses.csv&quot;, encoding=&quot;ISO-8859-1&quot;)df18= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2018/multipleChoiceResponses.csv&quot;, )df19= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2019/multiple_choice_responses.csv&quot;, )df20= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2020/kaggle_survey_2020_responses.csv&quot;, )df21= pd.read_csv(&quot;/kaggle/input/kaggle-survey-2021/kaggle_survey_2021_responses.csv&quot;, ) 3. plots and description 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109#질문 제거하기, replacedf17= df17.iloc[1:, :].replace(&quot;People 's Republic of China&quot;,'China')df18= df18.iloc[1:, :].replace('Republic of Korea','South Korea')df19= df19.iloc[1:, :].replace('Republic of Korea','South Korea')df20= df20.iloc[1:, :].replace('Republic of Korea','South Korea')df21= df21.iloc[1:, :]## East Asia에는 대한민국, 일본, 중국, 타이완, 몽골, 북조선 총 6개의 국가가 속해 있다. ## 이유는 알 수 없지만, 18년도엔 타이완이 없다. EastAsia17 = ['China',&quot;People 's Republic of China&quot;, 'Taiwan', 'South Korea', 'Japan']EastAsia18= ['China', 'South Korea', 'Japan', 'Republic of Korea'] EastAsia19 = ['China','Taiwan', 'South Korea', 'Japan', 'Republic of Korea']EastAsia20 = ['China','Taiwan', 'South Korea','Republic of Korea', 'Japan']EastAsia21 = ['China','Taiwan', 'South Korea', 'Japan']EastAsia = ['Republic of Korea','China','Taiwan', 'South Korea', 'Japan', &quot;People 's Republic of China&quot; ]df21_Ea = df21[df21['Q3'].isin(EastAsia)]df21_Wo = df21[~df21['Q3'].isin(EastAsia)]df21['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df21['Q3']]df20_Ea = df20[df20['Q3'].isin(EastAsia)]df20_Wo = df20[~df20['Q3'].isin(EastAsia)]df20['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df20['Q3']]df19_Ea = df19[df19['Q3'].isin(EastAsia)]df19_Wo = df19[~df19['Q3'].isin(EastAsia)]df19['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df19['Q3']]df18_Ea = df18[df18['Q3'].isin(EastAsia)]df18_Wo = df18[~df18['Q3'].isin(EastAsia)]df18['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df18['Q3']]df17_Ea = df17[df17['Country'].isin(EastAsia)]df17_Wo = df17[~df17['Country'].isin(EastAsia)]df17['region']=[&quot;EastAsia&quot; if x in EastAsia else &quot;World&quot; for x in df17['Country']]df21['year'] = '2021'df20['year'] = '2020'df19['year'] = '2019'df18['year'] = '2018'df17['year'] = '2017'years = ['2017', '2018', '2019', '2020', '2021']df21_Ea = df21[df21['Q3'].isin(EastAsia21)]Ea21= ( df21_Ea['Q3'].value_counts().to_frame() .reset_index().rename(columns={'index':'Country', 'Q3':'21'}))df20_Ea=df20[df20['Q3'].isin(EastAsia)]Ea20= ( df20_Ea['Q3'].replace('Republic of Korea','South Korea') .value_counts().to_frame().reset_index() .rename(columns={'index':'Country', 'Q3':'20'}))df19_Ea=df19[df19['Q3'].isin(EastAsia)]Ea19= (df19_Ea['Q3'].replace('Republic of Korea','South Korea') .value_counts().to_frame().reset_index() .rename(columns={'index':'Country', 'Q3':'19'}))df18_Ea=df18[df18['Q3'].isin(EastAsia)]Ea18= (df18_Ea['Q3'].replace('Republic of Korea','South Korea') .value_counts().to_frame().reset_index() .rename(columns={'index':'Country', 'Q3':'18'}))Ea18.value_counts()#df18 열에 taiwan = 0을 추가 해야 합니다. df17_Ea = df17[df17['Country'].isin(EastAsia)]Ea17= (df17_Ea['Country'].replace(&quot;People 's Republic of China&quot;,'China') .value_counts().to_frame().reset_index() .rename(columns={'index':'Country', 'Country':'17'}))#data를 합쳐서 하나의 dataframe으로 만들어 줌.df5years = pd.merge(Ea17, Ea18, on='Country', how='outer')df5year =pd.merge(Ea19,Ea20, on='Country', how='outer')df5year=pd.merge(df5year, Ea21, on='Country', how='outer')df5years = pd.merge(df5years, df5year, on='Country', how='outer')Ea21 = len(df21_Ea)Wo21 = len(df21) - len(df21_Ea)Ea20 = len(df20_Ea)Wo20 = len(df20) - len(df20_Ea)Ea19 = len(df19_Ea)Wo19 = len(df19) - len(df19_Ea)Ea18 = len(df18_Ea)Wo18 = len(df18) - len(df18_Ea)Ea17 = len(df17_Ea)Wo17 = len(df17) - len(df17_Ea)years = ['2017','2018','2019','2020', '2021']def percent (a, b): result =a/(a+b)*100 result = np.round(result, 2) return resultdef percentR (b, a): result =a/(a+b)*100 result = np.round(result, 2) return resultpercent = [percent(Ea17, Wo17), percent(Ea18, Wo18), percent(Ea19, Wo19), percent(Ea20, Wo20), percent(Ea21, Wo21)] 3.1 Kaggle’s transformation (World/East Asia) 3.1.1 user transformation Number of respondents (bar, scatter plot : number of respondents to World and East Asia,Map plot : number of respondents to East Asia) World and East Asia: The same trend. East Asia: 15% of the total continent and 20.3% of the population (16/78.7: Ea/Wo) 2018 Issue: Significant increase in respondents-&gt;Hypothesis: Due to the rapid increase in China. 2018 Outliers Considering: 2022 Kaggle survey Respondents: Increased in both World and East Asia I wish our team the honor of becoming a respondent to the Kaggle survey in 2022…. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980fig = go.Figure()y=[len(df17_Ea),len(df18_Ea), len(df19_Ea),len(df20_Ea),len(df21_Ea)]fig.add_trace(go.Bar(x=years, y=y, base=0, marker_color='#F2D64B', yaxis = &quot;y1&quot;, name='East Asia', text= percent, texttemplate='%{text} %', textposition='outside', hovertemplate='&lt;b&gt;KaggleUser&lt;/b&gt;: %{x}&lt;br&gt;'+ '&lt;b&gt;Count&lt;/b&gt;: %{y}'))fig.add_trace(go.Scatter(name = &quot;World&quot;, x=years, y=[len(df17), len(df18), len(df19), len(df20), len(df21)], marker_color='#979DA6', mode = 'lines+markers', # please check option here yaxis = &quot;y2&quot;))fig.update_traces(hovertemplate='&lt;b&gt;Count&lt;/b&gt;: %{y}&lt;br&gt;&lt;extra&gt;&lt;/extra&gt;'+ '&lt;b&gt;Year&lt;/b&gt;: %{x}&lt;br&gt;')fig.update_layout(yaxis = dict(title = &quot;Kaggle User in East Asia&quot;,showgrid = False, range=[0, len(df21_Ea)*1.2]), yaxis2 = dict(title = &quot;Kaggle User in World&quot;, overlaying = &quot;y1&quot;, side = &quot;right&quot;, showgrid = False, zeroline = False, range=[0, len(df21)*1.2]))fig.update_layout(title='&lt;b&gt;Kaggle Users&lt;/b&gt;',title_font_size=20, margin = dict(t=200, l=100, r=50, b=200), height=700, width=700)fig.update_layout(legend=dict( orientation=&quot;h&quot;, yanchor=&quot;bottom&quot;, y=1.1, xanchor=&quot;right&quot;, x=1))fig.add_annotation(dict(font=dict(size=14), x=0.9, y=-0.25, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show()def world_map(locations,counts,title): data = [ dict( type = 'choropleth', locations = locations, z = counts, colorscale = 'Reds', locationmode = 'country names', autocolorscale = False, reversescale = False, marker = dict( line = dict(color = '#F7F7F7', width = 1.5)), colorbar = dict(autotick = True, legth = 3, len=0.75, title = 'respodents', max = 1000, min = 0))] layout = dict( title=title, titlefont={'size': 28}, width=700, height=600, paper_bgcolor='#FFFFFF', margin=dict(l=50, r=50, t=100, b=100), geo = dict( showframe = True, showcoastlines = True, fitbounds=&quot;locations&quot;)) fig = dict(data=data, layout=layout) iplot(fig, validate=False, filename='world-map')z = df21_Ea['Q3'].value_counts() world_map(locations=z.index, counts=z.values, title= '&lt;b&gt;EastAsia Countries&lt;b&gt;') 18’ : User change between United States and India. China’s markedly increase in 2018 There is no Taiwan, but only China has increased. : East Asian political situation Issue can be suspected. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384A18 = ( df18['Q3'] .replace({'Republic of Korea':'South Korea', 'I do not wish to disclose my location' : 'Other'}) .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'Q3':'2018'}) .groupby('type') .sum() .reset_index())A19 = ( df19['Q3'] .replace('Republic of Korea','South Korea') .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'Q3':'2019'}) .groupby('type') .sum() .reset_index())A17 = ( df17['Country'] .replace({'United States': 'United States of America', 'Hong Kong': 'Hong Kong (S.A.R.)', 'United Kingdom':'United Kingdom of Great Britain and Northern Ireland', }) .replace(&quot;People 's Republic of China&quot;,'China') .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'Country':'2017'}) .groupby('type') .sum() .reset_index())A18A19=pd.merge(A18,A19, how='outer',on='type').fillna(0)A18A17=pd.merge(A18,A17, how='outer',on='type').fillna(0)A18A19['minus']= A18A19['2018']-A18A19['2019']A18A17['minus']= A18A17['2018']-A18A17['2017']A18A17=A18A17.sort_values(by=&quot;minus&quot;, ascending=False)A18A19=A18A19.sort_values(by=&quot;minus&quot;, ascending=False)fig = go.Figure(data=[ go.Bar(x =A18A19['type'], y = A18A19['minus'], marker_color='#979DA6', name = '2018-2019', base=0), go.Bar(x =A18A17['type'], y = A18A17['minus'], marker_color='#F2D64B', name = '2018-2017', base=0) ])fig.update_layout(title='&lt;b&gt; Predicting outliers (2018)&lt;/b&gt;',title_font_size=20, margin = dict(t=200, l=100, r=10, b=200), height=700, width=700, xaxis_title=None, yaxis_title=None)fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.update_layout(legend=dict( orientation=&quot;h&quot;, yanchor=&quot;bottom&quot;, y=1.1, xanchor=&quot;right&quot;, x=1))fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.5, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() Total population: 1.4 billion (85%) in China, 130 million in Japan, 0.5 billion in Korea, and 0.2 billion in Taiwan. China: The number of respondents is smaller than the population. Japan: Starting in 2019, overtaking China Taiwan : 2018 data 0 =? Diplomatic issues? The growth trend is weak. Korea : Respondents at a similar level to Japan’s population. East Asia: The number of respondents will increase further. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485#data preprocessingtotal17 = ( df17['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index())total18 = ( df18['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index())total19 = ( df19['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index())total20 = ( df20['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index())total21 = ( df21['region'] .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'region':'respodents'}) .groupby('type') .sum() .reset_index())#graphcolors = ['#F2D64B','#979DA6']fig = make_subplots(rows=1, cols=5, specs=[[{'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}]], subplot_titles=(&quot;2017&quot;, &quot;2018&quot;, &quot;2019&quot;, &quot;2020&quot;, &quot;2021&quot;))fig.add_trace(go.Pie(marker=dict(colors=colors),labels=total21['type'], values=total21['respodents'], name=&quot;2021&quot;, scalegroup='one'), 1, 5)fig.add_trace(go.Pie(marker=dict(colors=colors),labels=total20['type'], values=total20['respodents'], name=&quot;2020&quot;, scalegroup='one'), 1, 4)fig.add_trace(go.Pie(marker=dict(colors=colors),labels=total19['type'], values=total19['respodents'], name=&quot;2019&quot;, scalegroup='one'), 1, 3)fig.add_trace(go.Pie(marker=dict(colors=colors),labels=total18['type'], values=total18['respodents'], name=&quot;2018&quot;, scalegroup='one'), 1, 2)fig.add_trace(go.Pie(marker=dict(colors=colors),labels=total17['type'], values=total17['respodents'], name=&quot;2017&quot;, scalegroup='one'), 1, 1)fig.update_traces(hole=.0, hoverinfo=&quot;label+percent+name&quot;, textposition='inside', textinfo='percent+label', textfont_size=12)fig.update_layout(title='&lt;b&gt;World vs EastAsia&lt;/b&gt;',title_font_size=23, margin = dict(t=300, l=0, r=0, b=200), height=700, width=700)fig.update_layout(legend=dict( orientation=&quot;h&quot;, yanchor=&quot;bottom&quot;, y=1.3, xanchor=&quot;right&quot;, x=1))fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.25, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() 123456789101112131415161718192021222324252627282930fig = go.Figure(data=[ go.Bar(name='2017', x=df5years['Country'], y=df5years['17'], marker_color='#F2798F',text=df5years['17'].tolist(), textposition='outside'), go.Bar(name='2018', x=df5years['Country'], y=df5years['18'], marker_color='#88BFBA',text=df5years['18'].fillna(0).astype(int).tolist(), textposition='outside',), go.Bar(name='2019', x=df5years['Country'], y=df5years['19'], marker_color='#CDD9A3',text=df5years['19'].tolist(), textposition='outside'), go.Bar(name='2020', x=df5years['Country'], y=df5years['20'], marker_color='#F28705',text=df5years['20'].tolist(), textposition='outside',), go.Bar(name='2021', x=df5years['Country'], y=df5years['21'], marker_color='#D9946C',text=df5years['21'].tolist(), textposition='outside')])fig.update_layout(barmode='group')fig.update_layout(title='&lt;b&gt;Kaggle User in East Asia&lt;/b&gt;',title_font_size=23, margin = dict(t=200, l=100, r=10, b=200), height=600, width=700)fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.update_traces(hovertemplate='&lt;b&gt;Count&lt;/b&gt;: %{y}')fig.update_layout(legend=dict( orientation=&quot;v&quot;, yanchor=&quot;bottom&quot;, y=1.15, xanchor=&quot;right&quot;, x=1))fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.5, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() 3.1.2 Gender transformation World: The proportion of female respondents increases (still below 20%) The number of respondents is increasing in all genders. Our team is also a team with high female members and wants to contribute as a respondent in 2022. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495#data preprocessingGender_17 = ( df17['GenderSelect'] .replace(['A different identity', 'Prefer to self-describe', 'Non-binary, genderqueer, or gender non-conforming'], 'Others') .fillna('Others') .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'GenderSelect':'Gender'}) .groupby('type') .sum() .reset_index())Gender_18 = ( df18['Q1'] .replace(['Prefer not to say', 'Prefer to self-describe'], 'Others') .fillna('Others') .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'Q1':'Gender'}) .groupby('type') .sum() .reset_index())Gender_19 = ( df19['Q2'] .replace(['Prefer not to say','Prefer to self-describe'],'Others') .fillna('Others') .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'Q2':'Gender'}) .groupby('type') .sum() .reset_index())Gender_20 = ( df20['Q2'] .replace(['Prefer not to say', 'Prefer to self-describe', 'Nonbinary'], 'Others') .replace(['Man', 'Woman'], ['Male', 'Female']) .fillna('Others') .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'Q2':'Gender'}) .groupby('type') .sum() .reset_index())Gender_21 = ( df21['Q2'] .replace(['Prefer not to say', 'Prefer to self-describe', 'Nonbinary'], 'Others') .replace(['Man', 'Woman'], ['Male', 'Female']) .fillna('Others') .value_counts() .to_frame() .reset_index() .rename(columns={'index':'type', 'Q2':'Gender'}) .groupby('type') .sum() .reset_index())colors = ['#D9946C','#88BFBA', '#CDD9A3']fig = make_subplots(rows=1, cols=5, specs=[[{'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}]],)fig.add_trace(go.Pie(marker=dict(colors=colors), labels=Gender_21['type'], values=Gender_21['Gender'], name=&quot;2021&quot;, scalegroup='one', text=np.array(Gender_21['Gender'].sum()), title=&quot;2021&quot;, titleposition='bottom center'), 1, 5)fig.add_trace(go.Pie(marker=dict(colors=colors), labels=Gender_20['type'], values=Gender_20['Gender'], name=&quot;2020&quot;, scalegroup='one', text=np.array(Gender_20['Gender'].sum()), title=&quot;2020&quot;, titleposition='bottom center'), 1, 4)fig.add_trace(go.Pie(marker=dict(colors=colors), labels=Gender_19['type'], values=Gender_19['Gender'], name=&quot;2019&quot;, scalegroup='one', text=np.array(Gender_19['Gender'].sum()), title=&quot;2019&quot;, titleposition='bottom center'), 1, 3)fig.add_trace(go.Pie(marker=dict(colors=colors), labels=Gender_18['type'], values=Gender_18['Gender'], name=&quot;2018&quot;, scalegroup='one', text=np.array(Gender_18['Gender'].sum()), title=&quot;2018&quot;, titleposition='bottom center'), 1, 2)fig.add_trace(go.Pie(marker=dict(colors=colors), labels=Gender_17['type'], values=Gender_17['Gender'], name=&quot;2017&quot;, scalegroup='one', text=np.array(Gender_17['Gender'].sum()), title=&quot;2017&quot;, titleposition='bottom center'), 1, 1)fig.update_traces(hole=.0, hoverinfo=&quot;label+percent+name&quot;, textinfo='label+percent+value')fig.update_layout(title='&lt;b&gt;World Gender&lt;/b&gt;',title_font_size=23, margin = dict(t=300, l=100, r=0, b=200), height=700, width=1000)fig.update_layout(legend=dict( orientation=&quot;v&quot;, yanchor=&quot;bottom&quot;, y=1.3, xanchor=&quot;right&quot;, x=1))fig.add_annotation(dict(font=dict(size=14), x=0.85, y=-0.5, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() - Male (1004-&gt;2037 : 2017-&gt;2021) double increase - Female 183-&gt;327 : 2017-&gt;2021 increased 1.8 times - Others (8-&gt;64 : 2017-&gt;2021) 8x increase [Compare the high and low points] It can be seen that the number of female respondents and the ratio of male respondents hardly change, which is a difference compared to World data. It can be seen that the degree of gender freedom in East Asia has increased relatively. Compared to World data, it can be seen that in 2021 (1.87: 2.6= Wo: Ea), compared to 2017 (1.96: 0.7 = Ea), which was relatively conservative. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#data preprocessinggender21= df21_Ea.loc[:, ['Q3', 'Q2', 'year']].rename(columns={'Q3':'Country', 'Q2':'Gender'})gender20= df20_Ea.loc[:, ['Q3', 'Q2', 'year']].rename(columns={'Q3':'Country', 'Q2':'Gender'})gender19= df19_Ea.loc[:, ['Q3', 'Q2', 'year']].rename(columns={'Q3':'Country', 'Q2':'Gender'})gender18= df18_Ea.loc[:, ['Q3', 'Q1', 'year']].rename(columns={'Q3':'Country', 'Q1':'Gender'})gender17= df17_Ea.loc[:, ['Country', 'GenderSelect', 'year']].rename(columns={'index':'type', 'GenderSelect':'Gender'})Gender5y= pd.concat([gender17, gender18, gender19, gender20, gender21])Gender5y= (Gender5y.replace(['Prefer not to say', 'Prefer to self-describe', 'Nonbinary', 'A different identity'], 'Others') .replace(['Man', 'Woman'], ['Male', 'Female']) .groupby(['year', 'Gender']) .size() .reset_index() .rename(columns = {0:&quot;Count&quot;}))gen17_5y = Gender5y[Gender5y['year'] == &quot;2017&quot;].reset_index(drop = True)gen18_5y = Gender5y[Gender5y['year'] == &quot;2018&quot;].reset_index(drop = True)gen19_5y = Gender5y[Gender5y['year'] == &quot;2019&quot;].reset_index(drop = True)gen20_5y = Gender5y[Gender5y['year'] == &quot;2020&quot;].reset_index(drop = True)gen21_5y = Gender5y[Gender5y['year'] == &quot;2021&quot;].reset_index(drop = True)Gen5y_ = pd.concat([gen17_5y, gen18_5y, gen19_5y, gen20_5y, gen21_5y], ignore_index = True)Gen5y_= pd.pivot(Gen5y_, index = &quot;year&quot;, columns = &quot;Gender&quot;, values = &quot;Count&quot;).reset_index()Gen5y_Gen5y_['year'].unique()#graphfig = go.Figure()fig.add_trace(go.Bar( x = Gen5y_['year'], y = Gen5y_['Male'].tolist(), name = 'Male',marker_color='#88BFBA', text=Gen5y_['Male'].tolist(), textposition='outside'))fig.add_trace(go.Bar( x = Gen5y_['year'], y = Gen5y_['Female'].tolist(), name = 'Female',marker_color='#D9946C', text=Gen5y_['Female'].tolist(), textposition='outside'))fig.add_trace(go.Bar( x = Gen5y_['year'], y = Gen5y_['Others'].tolist(), name = 'Others',marker_color='#CDD9A3', text=Gen5y_['Others'].tolist(), textposition='outside'))fig.update_layout(barmode=&quot;group&quot;) fig.update_layout(title='&lt;b&gt;Gender by year&lt;/b&gt;',title_font_size=22, margin = dict(t=200, l=100, r=10, b=200), height=700, width=700, xaxis_title=None, yaxis_title=None)fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.5, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() 3.1.3 Job transformation 21' World Vs East Asia Age Ratio: Bar plot Not Employed : More than 30% in both East Asia and the world, the highest. Because “Students” is included. Data Scientist : High percentage in the world and East Asia. Relatively low proportion in East Asia. = Absolute lack of numbers We would like to move forward by selecting a **data scientist** with insufficient numbers in East Asia. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132#data preprocessingData_Analyst =['Data Analyst','Data Miner,Information technology','Data Miner', 'Predictive Modeler','Information technology, networking, or system administration', 'A business discipline (accounting, economics, finance, etc.)', 'Business Analyst', 'Humanities', 'Statistician', 'Mathematics or statistics', 'Medical or life sciences (biology, chemistry, medicine, etc.)', 'Physics or astronomy', 'Social sciences (anthropology, psychology, sociology, etc.)', 'Environmental science or geology', 'Humanities (history, literature, philosophy, etc.)']Data_Scientist =['Data Scientist', 'Research Scientist', 'Researcher', 'Machine Learning Engineer', 'Scientist/Researcher']Developer=['Developer Relations/Advocacy','Data Engineer','Engineer','Engineering (non-computer focused)', 'Programmer','Software Engineer', 'Computer Scientist','Computer science (software engineering, etc.)', 'Fine arts or performing arts','Product Manager', 'Software Developer/Software Engineer', 'Product/Project Manager','Program/Project Manager','DBA/Database Engineer']Not_Employed =['Currently not employed', 'Not employed', 'Student']Others = ['I never declared a major', 'Other']df21job_Ea = df21_Ea.loc[:,['Q3','Q5']].rename(columns={'Q5':'2021'}).fillna('Other')df20job_Ea = df20_Ea.loc[:,['Q3','Q5']].rename(columns={'Q5':'2020'}).fillna('Other')df19job_Ea = df19_Ea.loc[:,['Q3','Q5']].rename(columns={'Q5':'2019'}).fillna('Other')df18job_Ea = df18_Ea.loc[:,['Q3','Q5']].rename(columns={ 'Q5':'2018'}).fillna('Other')df17job_Ea = df17_Ea.loc[:,['Country','CurrentJobTitleSelect']].rename(columns={'CurrentJobTitleSelect':'2017'}).fillna('Other')df21job_Ea.value_counts('2021')df21job_Ea['JOB']=[&quot;Data Analyst&quot; if x in Data_Analyst else &quot;Data Scientist&quot; if x in Data_Scientist # Data Scientist else &quot;Developer&quot; if x in Developer else &quot;NotEmployed&quot; if x in Not_Employed else &quot;Others&quot; for x in df21job_Ea['2021']]df21job_Ea.value_counts('JOB')df20job_Ea.value_counts('2020')df20job_Ea['JOB']=[&quot;Data Analyst&quot; if x in Data_Analyst else &quot;Data Scientist&quot; if x in Data_Scientist else &quot;Developer&quot; if x in Developer else &quot;NotEmployed&quot; if x in Not_Employed else &quot;Other&quot; for x in df20job_Ea['2020']]df20job_Ea[['2020','JOB']]df19job_Ea.value_counts('2019')df19job_Ea['JOB']=[&quot;Data Analyst&quot; if x in Data_Analyst else &quot;Data Scientist&quot; if x in Data_Scientist else &quot;Developer&quot; if x in Developer else &quot;NotEmployed&quot; if x in Not_Employed else &quot;Other&quot; for x in df19job_Ea['2019']]df19jobTest = df19job_Ea.loc[df19job_Ea.JOB == 'Other']df19jobTest['2019'].value_counts()df18job_Ea.value_counts('2018')df18job_Ea['JOB']=[&quot;Data Analyst&quot; if x in Data_Analyst else &quot;Data Scientist&quot; if x in Data_Scientist else &quot;Developer&quot; if x in Developer else &quot;NotEmployed&quot; if x in Not_Employed else &quot;Other&quot; for x in df18job_Ea['2018']]df18jobTest = df18job_Ea.loc[df18job_Ea.JOB == 'Other']df18jobTest['2018'].value_counts()df17job_Ea.value_counts('2017')df17job_Ea['JOB']=[&quot;Data Analyst&quot; if x in Data_Analyst else &quot;Data Scientist&quot; if x in Data_Scientist else &quot;Developer&quot; if x in Developer else &quot;NotEmployed&quot; if x in Not_Employed else &quot;Other&quot; for x in df17job_Ea['2017']]df17jobTest = df17job_Ea.loc[df17job_Ea.JOB == 'Other']df17jobTest['2017'].value_counts()df21jobTest = df21job_Ea.loc[df21job_Ea.JOB == 'Other']df21jobTest['2021'].head()df21job_Ea.value_counts('JOB')dfjob21 =df21job_Ea.groupby(['Q3','JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;}).rename(columns={'Q3':'country'})dfjob20 =df20job_Ea.groupby(['Q3','JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;}).rename(columns={'Q3':'country'})dfjob19 =df19job_Ea.groupby(['Q3','JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;}).rename(columns={'Q3':'country'})dfjob18 =df18job_Ea.groupby(['Q3','JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;}).rename(columns={'Q3':'country'})dfjob17 =df17job_Ea.groupby(['Country','JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;}).rename(columns={'Country':'country'})df21_Ea_job =df21job_Ea.groupby(['JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;})df20_Ea_job =df20job_Ea.groupby(['JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;})df19_Ea_job =df19job_Ea.groupby(['JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;})df18_Ea_job =df18job_Ea.groupby(['JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;})df17_Ea_job =df17job_Ea.groupby(['JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;})df21_DA=df21[df21['Q5'].isin(Data_Analyst)]df21_DS=df21[df21['Q5'].isin(Data_Scientist)]df21_D=df21[df21['Q5'].isin(Developer)]df21_N=df21[df21['Q5'].isin(Not_Employed)]df21_O=df21[df21['Q5'].isin(Others)]World_ = np.array([df21_DA['Q5'].count(), df21_DS['Q5'].count(), df21_D['Q5'].count(), df21_N['Q5'].count(), df21_O['Q5'].count()]) East_Asia_ = df21_Ea_job['Count'].to_numpy()World =((World_/World_.sum())*100).round(1)East_Asia =((East_Asia_/East_Asia_.sum())*100).round(1)y = df21_Ea_job.JOB.to_numpy()fig = go.Figure(data=[ go.Bar(y=y, x=World, orientation='h', name=&quot;World&quot;, base=0, hovertemplate='&lt;b&gt;World&lt;/b&gt;: %{x}%&lt;br&gt;', marker_color='#979DA6', text=World, textposition='outside'), go.Bar(y=y, x=-East_Asia, orientation='h', name=&quot;East Asia&quot;, base=0, hovertemplate='&lt;b&gt;East Asia&lt;/b&gt;: %{x}%&lt;br&gt;', marker_color='#F2D64B', text=East_Asia, textposition='outside')])fig.update_layout(barmode='stack')fig.update_layout(title='&lt;b&gt;World vs EastAsia&lt;/b&gt;',title_font_size=22, margin = dict(t=200, l=100, r=50, b=200), height=700, width=750, xaxis_title=None, yaxis_title=None)fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.update_layout(legend=dict( orientation=&quot;h&quot;, yanchor=&quot;bottom&quot;, y=1.1, xanchor=&quot;right&quot;, x=1))fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.5, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() World Job Ratio: Heat Map The trend of increasing each job except Others. Data Scientist has a high proportion, and the trend is to increase further in 2022. East Asia Job Ratio: Heat Map East Asia : Increasing the ratio of data scientist. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113#data preprocessingdf21job= df21.loc[:,['region','Q5']].rename(columns={'Q5':'2021'}).fillna('Others')df20job= df20.loc[:,['region','Q5']].rename(columns={'Q5':'2020'}).fillna('Others')df19job= df19.loc[:,['region','Q5']].rename(columns={'Q5':'2019'}).fillna('Others')df18job= df18.loc[:,['region','Q6']].rename(columns={ 'Q6':'2018'}).fillna('Others')df17job= df17.loc[:,['region','CurrentJobTitleSelect']].rename(columns={'CurrentJobTitleSelect':'2017'}).fillna('Others')df21job['JOB']=[&quot;Data Analyst&quot; if x in Data_Analyst else &quot;Data Scientist&quot; if x in Data_Scientist # Data Scientist else &quot;Developer&quot; if x in Developer else &quot;NotEmployed&quot; if x in Not_Employed else &quot;Others&quot; for x in df21job['2021']]df20job['JOB']=[&quot;Data Analyst&quot; if x in Data_Analyst else &quot;Data Scientist&quot; if x in Data_Scientist else &quot;Developer&quot; if x in Developer else &quot;NotEmployed&quot; if x in Not_Employed else &quot;Others&quot; for x in df20job['2020']]df19job['JOB']=[&quot;Data Analyst&quot; if x in Data_Analyst else &quot;Data Scientist&quot; if x in Data_Scientist else &quot;Developer&quot; if x in Developer else &quot;NotEmployed&quot; if x in Not_Employed else &quot;Others&quot; for x in df19job['2019']]df18job['JOB']=[&quot;Data Analyst&quot; if x in Data_Analyst else &quot;Data Scientist&quot; if x in Data_Scientist else &quot;Developer&quot; if x in Developer else &quot;NotEmployed&quot; if x in Not_Employed else &quot;Others&quot; for x in df18job['2018']]df17job['JOB']=[&quot;Data Analyst&quot; if x in Data_Analyst else &quot;Data Scientist&quot; if x in Data_Scientist else &quot;Developer&quot; if x in Developer else &quot;NotEmployed&quot; if x in Not_Employed else &quot;Others&quot; for x in df17job['2017']]df21_job =df21job.groupby(['JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;})df20_job =df20job.groupby(['JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;})df19_job =df19job.groupby(['JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;})df18_job =df18job.groupby(['JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;})df17_job =df17job.groupby(['JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;})merge11=pd.merge(df21_job,df20_job, how='outer',on='JOB')merge21=pd.merge(df19_job,df18_job, how='outer',on='JOB')merge31=pd.merge(merge11,merge21, how='outer',on='JOB')merge_Wo=(pd.merge(merge31,df17_job, how='outer',on='JOB') .rename(columns = {'Count_x_x':'2021','Count_y_x':'2020','Count_x_y':'2019','Count_y_y':'2018','Count':'2017'}).fillna(0) .reindex(columns = ['JOB','2017','2018','2019','2020','2021' ]))df21job_Ea = df21job[df21job['region'] == 'EastAsia'].loc[:,['region','JOB']].rename(columns={'region':'EastAsia'})df20job_Ea = df20job[df20job['region'] == 'EastAsia'].loc[:,['region','JOB']].rename(columns={'region':'EastAsia'})df19job_Ea = df19job[df19job['region'] == 'EastAsia'].loc[:,['region','JOB']].rename(columns={'region':'EastAsia'})df18job_Ea = df18job[df18job['region'] == 'EastAsia'].loc[:,['region','JOB']].rename(columns={'region':'EastAsia'})df17job_Ea = df17job[df17job['region'] == 'EastAsia'].loc[:,['region','JOB']].rename(columns={'region':'EastAsia'})df21job_Ea =df21job_Ea.groupby(['JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;})df20job_Ea =df20job_Ea.groupby(['JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;})df19job_Ea =df19job_Ea.groupby(['JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;})df18job_Ea =df18job_Ea.groupby(['JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;})df17job_Ea =df17job_Ea.groupby(['JOB']).size().reset_index().rename(columns = {0:&quot;Count&quot;})merge1=pd.merge(df21job_Ea,df20job_Ea, how='outer',on='JOB')merge2=pd.merge(df19job_Ea,df18job_Ea, how='outer',on='JOB')merge3=pd.merge(merge1,merge2, how='outer',on='JOB')merge=(pd.merge(merge3,df17job_Ea, how='outer',on='JOB') .rename(columns = {'Count_x_x':'2021','Count_y_x':'2020','Count_x_y':'2019','Count_y_y':'2018','Count':'2017'}).fillna(0) .reindex(columns = ['JOB','2017','2018','2019','2020','2021' ]))#graphz1=((merge_Wo.iloc[:,[1,2,3,4,5]].to_numpy()/merge_Wo.iloc[:,[1,2,3,4,5]].to_numpy().sum())*100).round(1)z2=((merge.iloc[:,[1,2,3,4,5]].to_numpy()/merge.iloc[:,[1,2,3,4,5]].to_numpy().sum())*100).round(1)x=['2017-year','2018-year','2019-year','2020-year','2021-year']y1=merge_Wo['JOB'].tolist()y2=merge['JOB'].tolist()fig1 = ff.create_annotated_heatmap(z1, x = x, y = y1, colorscale='sunset')fig2 = ff.create_annotated_heatmap(z2, x = x, y = y2, colorscale='sunset')for annot in fig2['layout']['annotations']: annot['xref'] = 'x2' fig = make_subplots(rows=1, cols=2)fig.add_trace(fig1.data[0], row=1, col=1)fig.add_trace(fig2.data[0], row=1, col=2)fig.update_layout(fig1.layout, title='&lt;b&gt; World vs EastAsia&lt;/b&gt;',title_font_size=22, margin = dict(t=200, l=100, r=10, b=200), height=700, width=1150, coloraxis=dict(showscale=True, colorscale='sunset'))fig.update_traces(hovertemplate='&lt;b&gt;Job&lt;/b&gt;: %{y}&lt;br&gt;'+ '&lt;b&gt;Year&lt;/b&gt;: %{x}&lt;br&gt;'+ '&lt;b&gt;Percent&lt;/b&gt;: %{z}%')fig.layout.annotations += fig2.layout.annotationsfig.add_annotation(dict(font=dict(size=14), x=0.9, y=-0.25, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() 3.1.4 Age transformation > Age change in World and East Asia by year: Stacked scatter plot In the case of Age data, there is no 2017 data. 70% of the World respondents said 20s to 30s. 70% of East Asia respondents said 20s to 30s. The number of respondents increases, but the ratio seems to have stabilized. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224#data preprocessing#WorldAge21_W = df21.loc[:,['Q3','Q1', 'year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'age'}).fillna('etc')Age20_W = df20.loc[:,['Q3','Q1','year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'age'}).fillna('etc')Age19_W = df19.loc[:,['Q3','Q1','year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'age'}).fillna('etc')Age18_W = df18.loc[:,['Q3','Q2','year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q2':'age'}).fillna('etc')Age5y_W= pd.concat([Age21_W, Age20_W, Age19_W, Age18_W])Age5y_W= (Age5y_W.replace(['60-69', '70+', '70-79', '80+'], '60+') .replace(['22-24', '25-29'], '22-29') .replace(['30-34', '35-39'], '30-39') .replace(['40-44', '45-49'], '40-49') .replace(['50-54', '55-59'], '50-59') .groupby(['year', 'age']) .size() .reset_index() .rename(columns = {0:&quot;Count&quot;}))Age21_percent_W = Age5y_W[Age5y_W['year'] == &quot;2021&quot;].reset_index(drop = True)Age21_percent_W['percentage'] = Age21_percent_W[&quot;Count&quot;] / Age21_percent_W[&quot;Count&quot;].sum()Age21_percent_W['%'] = np.round(Age21_percent_W['percentage'] * 100, 1)Age20_percent_W = Age5y_W[Age5y_W['year'] == &quot;2020&quot;].reset_index(drop = True)Age20_percent_W['percentage'] = Age20_percent_W[&quot;Count&quot;] / Age20_percent_W[&quot;Count&quot;].sum()Age20_percent_W['%'] = np.round(Age20_percent_W['percentage'] * 100, 1)Age19_percent_W = Age5y_W[Age5y_W['year'] == &quot;2019&quot;].reset_index(drop = True)Age19_percent_W['percentage'] = Age19_percent_W[&quot;Count&quot;] / Age19_percent_W[&quot;Count&quot;].sum()Age19_percent_W['%'] = np.round(Age19_percent_W['percentage'] * 100, 1)Age18_percent_W = Age5y_W[Age5y_W['year'] == &quot;2018&quot;].reset_index(drop = True)Age18_percent_W['percentage'] = Age18_percent_W[&quot;Count&quot;] / Age18_percent_W[&quot;Count&quot;].sum()Age18_percent_W['%'] = np.round(Age18_percent_W['percentage'] * 100, 1)Age5y_percent_W = pd.concat([Age18_percent_W, Age19_percent_W, Age20_percent_W, Age21_percent_W], ignore_index = True)Age5y_percent_W= pd.pivot(Age5y_percent_W, index = &quot;year&quot;, columns = 'age', values = &quot;%&quot;).reset_index()Age5y_percent_WAge21 = df21_Ea.loc[:,['Q3','Q1', 'year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'age'}).fillna('etc')Age20 = df20_Ea.loc[:,['Q3','Q1','year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'age'}).fillna('etc')Age19 = df19_Ea.loc[:,['Q3','Q1','year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'age'}).fillna('etc')Age18 = df18_Ea.loc[:,['Q3','Q2','year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q2':'age'}).fillna('etc')Age5y= pd.concat([Age21, Age20, Age19, Age18])Age5y= (Age5y.replace(['60-69', '70+', '70-79', '80+'], '60+') .replace(['22-24', '25-29'], '22-29') .replace(['30-34', '35-39'], '30-39') .replace(['40-44', '45-49'], '40-49') .replace(['50-54', '55-59'], '50-59') .groupby(['year', 'age']) .size() .reset_index() .rename(columns = {0:&quot;Count&quot;}))#EastAsiaAge21_percent = Age5y[Age5y['year'] == &quot;2021&quot;].reset_index(drop = True)Age21_percent['percentage'] = Age21_percent[&quot;Count&quot;] / Age21_percent[&quot;Count&quot;].sum()Age21_percent['%'] = np.round(Age21_percent['percentage'] * 100, 1)Age21_percentAge20_percent = Age5y[Age5y['year'] == &quot;2020&quot;].reset_index(drop = True)Age20_percent['percentage'] = Age20_percent[&quot;Count&quot;] / Age20_percent[&quot;Count&quot;].sum()Age20_percent['%'] = np.round(Age20_percent['percentage'] * 100, 1)Age20_percentAge19_percent = Age5y[Age5y['year'] == &quot;2019&quot;].reset_index(drop = True)Age19_percent['percentage'] = Age19_percent[&quot;Count&quot;] / Age19_percent[&quot;Count&quot;].sum()Age19_percent['%'] = np.round(Age19_percent['percentage'] * 100, 1)Age19_percentAge18_percent = Age5y[Age5y['year'] == &quot;2018&quot;].reset_index(drop = True)Age18_percent['percentage'] = Age18_percent[&quot;Count&quot;] / Age18_percent[&quot;Count&quot;].sum()Age18_percent['%'] = np.round(Age18_percent['percentage'] * 100, 1)Age18_percentAge5y_percent = pd.concat([Age18_percent, Age19_percent, Age20_percent, Age21_percent], ignore_index = True)Age5y_percent= pd.pivot(Age5y_percent, index = &quot;year&quot;, columns = 'age', values = &quot;%&quot;).reset_index()Age5y_percentAge5y_percent_order = Age5y_percent_W['year'].tolist()Age5y_order = Age5y_W['age'].unique().tolist()#graph1fig = go.Figure()fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent_W['18-21'].tolist(), mode = &quot;lines&quot;, name = '18-21', line = dict(width = 1), stackgroup = &quot;one&quot;, marker_color='#F2798F'))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent_W['22-29'].tolist(), mode = &quot;lines&quot;, name = &quot;20s&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;, marker_color='#88BFBA'))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent_W['30-39'].tolist(), mode = &quot;lines&quot;, name = &quot;30s&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;, marker_color='#CDD9A3'))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent_W['40-49'].tolist(), mode = &quot;lines&quot;, name = &quot;40s&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;, marker_color='#F28705'))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent_W['50-59'].tolist(), mode = &quot;lines&quot;, name = &quot;50s&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;, marker_color='#D9946C'))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent_W['60+'].tolist(), mode = &quot;lines&quot;, name = &quot;60s&lt;&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;, marker_color='#F2D64B'))fig.update_traces(hovertemplate='&lt;b&gt;Percent&lt;/b&gt;: %{y}%&lt;br&gt;'+ '&lt;b&gt;Year&lt;/b&gt;: %{x}&lt;br&gt;')fig.update_layout(yaxis_range = (0, 100), height=500, width=700, title_text=&quot;&lt;b&gt;World&lt;/b&gt;&quot;, title_font_size=20, title_x=0.5)fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.2, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show()#graph2Age5y_percent_order = Age5y_percent['year'].tolist()Age5y_order = Age5y['age'].unique().tolist()fig = go.Figure()fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent['18-21'].tolist(), mode = &quot;lines&quot;, name = '18-21', line = dict(width = 1), stackgroup = &quot;one&quot;, marker_color='#F2798F'))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent['22-29'].tolist(), mode = &quot;lines&quot;, name = &quot;20s&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;, marker_color='#88BFBA'))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent['30-39'].tolist(), mode = &quot;lines&quot;, name = &quot;30s&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;, marker_color='#CDD9A3'))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent['40-49'].tolist(), mode = &quot;lines&quot;, name = &quot;40s&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;, marker_color='#F28705'))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent['50-59'].tolist(), mode = &quot;lines&quot;, name = &quot;50s&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;, marker_color='#D9946C'))fig.add_trace(go.Scatter( x = Age5y_percent_order, y = Age5y_percent['60+'].tolist(), mode = &quot;lines&quot;, name = &quot;60s&lt;&quot;, line = dict(width = 1), stackgroup = &quot;one&quot;, marker_color='#F2D64B'))fig.update_traces(hovertemplate='&lt;b&gt;Percent&lt;/b&gt;: %{y}%&lt;br&gt;'+ '&lt;b&gt;Year&lt;/b&gt;: %{x}&lt;br&gt;')fig.update_layout(yaxis_range = (0, 100), height=500, width=700, title_text=&quot;&lt;b&gt;East Asia&lt;/b&gt;&quot;, title_font_size=20, title_x=0.5)fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.2, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() 17'East Asia Age Ratio: Heat Map East Asia : 50% or more. Those in their 20s and 30s. Korea: Those in their 20s are the highest. The number of respondents in their 50s and older is also large. Taiwan : The number of respondents in their 30s and older is relatively small. China: 70% or more of respondents in their 30s or younger. Related to life expectancy? Japan: Like an aging country, all ages are evenly distributed. Even if you’re older, there are many respondents to Kaggle. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#data processingdf21Age_Ea = df21_Ea.loc[:,['Q3','Q1']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'2021'}).fillna('etc')df21Age_Ea=(df21Age_Ea.replace(['60-69', '70+', '70-79', '80+'], '60+') .replace(['22-24', '25-29'], '22-29') .replace(['30-34', '35-39'], '30-39') .replace(['40-44', '45-49'], '40-49') .replace(['50-54', '55-59'], '50-59'))# 연령-지역 %dfKo_Age21= df21Age_Ea[df21Age_Ea['East_Asia']=='South Korea']dfKo_Age21_per=dfKo_Age21['2021'].value_counts().to_frame().reset_index()dfKo_Age21_per['South Korea']=((dfKo_Age21_per['2021'] / len(dfKo_Age21))*100).round(2)dfTw_Age21= df21Age_Ea[df21Age_Ea['East_Asia']=='Taiwan']dfTw_Age21_per=dfTw_Age21['2021'].value_counts().to_frame().reset_index()dfTw_Age21_per['Taiwan']=((dfTw_Age21_per['2021'] / len(dfTw_Age21))*100).round(2)dfTw_Age21_perdfCh_Age21= df21Age_Ea[df21Age_Ea['East_Asia']=='China']dfCh_Age21_per=dfCh_Age21['2021'].value_counts().to_frame().reset_index()dfCh_Age21_per['China']=((dfCh_Age21_per['2021'] / len(dfCh_Age21))*100).round(2)dfCh_Age21_perdf21Age_Ea.head()dfJp_Age21= df21Age_Ea[df21Age_Ea['East_Asia']=='Japan']dfJp_Age21_per=dfJp_Age21['2021'].value_counts().to_frame().reset_index()dfJp_Age21_per['Japan']=((dfJp_Age21_per['2021'] / len(dfJp_Age21))*100).round(2)dfJp_Age21_permerge1= pd.merge(dfKo_Age21_per,dfTw_Age21_per, on='index', how='outer')merge2= pd.merge(dfCh_Age21_per,dfJp_Age21_per, on='index', how='outer')merge= pd.merge(merge1,merge2, on='index', how='outer').fillna(0).sort_values(by=['index'],ascending=True)#graphx1=['South Korea','Taiwan','China','Japan']y1=merge.sort_values(by=['index'], ascending=True)['index'].tolist()z1=merge.iloc[:,[2,4,6,8]].to_numpy()fig = go.Figure(data=go.Heatmap( z=z1, x=x1, y=y1, hoverongaps = True, opacity=1.0, xgap=2.5, ygap=2.5))fig = ff.create_annotated_heatmap(z1, x = x1, y = y1, colorscale='sunset')fig.update_layout(height=500, width=600, title_text=&quot;&lt;b&gt;East Asia Age (2021)&lt;/b&gt;&quot;, title_font_size=20, title_x=0.5)fig.update_traces(hovertemplate='&lt;b&gt;Age&lt;/b&gt;: %{y}&lt;br&gt;'+ '&lt;b&gt;Country&lt;/b&gt;: %{x}&lt;br&gt;'+ '&lt;b&gt;Percent&lt;/b&gt;: %{z}%')fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.2, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() 17’East Asia’s age ratio: Box plot 2017: Data is not a section but an individual number. If you divide the interval, you can add it to the previous graph. It was data that I could draw a bar plot, so I drew it. You can see a 100-year-old in China, but they don’t remove missing values on purpose. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# 연도별 나이 df21Age_Ea = df21_Ea.loc[:,['Q3','Q1']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'2021'}).fillna('etc')df20Age_Ea = df20_Ea.loc[:,['Q3','Q1']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'2020'}).fillna('etc')df19Age_Ea = df19_Ea.loc[:,['Q3','Q1']].reset_index().rename(columns={'Q3':'East_Asia', 'Q1':'2019'}).fillna('etc')df18Age_Ea = df18_Ea.loc[:,['Q3','Q2']].reset_index().rename(columns={'Q3':'East_Asia', 'Q2':'2018'}).fillna('etc')df17Age_Ea = df17_Ea.loc[:,['Country','Age']].reset_index().rename(columns={'Country':'East_Asia', 'Age':'2017'}).fillna('etc')#data frame 정리dfAge21 =df21Age_Ea.groupby(['East_Asia','2021']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfAge20 =df20Age_Ea.groupby(['East_Asia','2020']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfAge19 =df19Age_Ea.groupby(['East_Asia','2019']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfAge18 =df18Age_Ea.groupby(['East_Asia','2018']).size().reset_index().rename(columns = {0:&quot;Count&quot;})dfAge17 =(df17Age_Ea.groupby(['East_Asia','2017']) .size().reset_index().rename(columns = {0:&quot;Count&quot;}))#graphfig = go.Figure()x = ['China','Japan','South Korea','Taiwan']fig.add_trace(go.Box( y=dfAge17['2017'][dfAge17['East_Asia']==&quot;Japan&quot;].to_numpy(), name='Japan', marker=dict(color='#CDD9A3')))fig.add_trace(go.Box(y=dfAge17['2017'][dfAge17['East_Asia']==&quot;China&quot;].to_numpy(), name='China', marker=dict(color='#88BFBA')))fig.add_trace(go.Box(y=dfAge17['2017'][dfAge17['East_Asia']==&quot;South Korea&quot;].to_numpy(), name='South Korea', marker=dict(color='#F2798F')))fig.add_trace(go.Box(y=dfAge17['2017'][dfAge17['East_Asia']==&quot;Taiwan&quot;].to_numpy(), name='Taiwan', marker=dict(color='#F28705' ),))fig.update_layout(yaxis = dict(range=[0, 120]))fig.update_layout(yaxis_range = (0, 110), height=600, width=700, title_text=&quot;&lt;b&gt;Age in East Asia (2017)&lt;/b&gt;&quot;, title_font_size=20, margin = dict(t=100, l=50, r=50, b=100), title_x=0.5)fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.update_layout(legend=dict( orientation=&quot;v&quot;, yanchor=&quot;bottom&quot;, y=0.8, xanchor=&quot;right&quot;, x=1))fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.2, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() 3.1.5 Degree transformation World job ratio in each country: pie plot World: 90% or higher Bachelor’s degree East Asia: 85% bachelor’s degree or higher 12345678910111213141516171819202122232425262728293031323334353637383940414243#data preprocessingdegree_wo = (df21['Q4'] .replace(['No formal education past high school', 'Some college/university study without earning a bachelor’s degree'],'~college') .replace(['Doctoral degree', 'Professional doctorate'],'Doctoral degree~') .value_counts().to_frame())degree_ea = (df21_Ea['Q4'] .replace(['No formal education past high school', 'Some college/university study without earning a bachelor’s degree'],'~college') .replace(['Doctoral degree', 'Professional doctorate'],'Doctoral degree~') .value_counts().to_frame())#graphcolors = ['#F2798F','#88BFBA', '#CDD9A3', '#F28705', '#D9946C']fig = make_subplots(rows=1, cols=2, specs=[[{'type':'pie'}, {'type':'pie'}]], subplot_titles=(&quot;World&quot;, &quot;East Asia&quot;))fig.add_trace(go.Pie(marker=dict(colors=colors), labels=degree_wo.index, values=degree_wo['Q4'].to_numpy(), name=&quot;World&quot;), 1, 1)fig.add_trace(go.Pie(marker=dict(colors=colors), labels=degree_ea.index, values=degree_ea['Q4'].to_numpy(), name=&quot;East Asia&quot;), 1, 2)fig.update_traces(hole=.0, hoverinfo=&quot;label+percent+name&quot;)fig.update_layout(title='&lt;b&gt;World vs East Asia&lt;/b&gt;',title_font_size=22, margin = dict(t=200, l=30, r=0, b=200), height=700, width=700)fig.update_layout(legend=dict( orientation=&quot;h&quot;, yanchor=&quot;bottom&quot;, y=1.1, xanchor=&quot;right&quot;, x=1.0))fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.5, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() Percentage of East Asia degrees by year: sunburst plot The highest percentage of respondents with master’s degrees per year 12345678910111213141516171819202122232425262728293031323334353637383940414243444546#data preprocessingdf21_Ea_degree=(df21_Ea['Q4'].replace(['No formal education past high school', 'Some college/university study without earning a bachelor’s degree'],'~college') .replace(['Doctoral degree','Professional doctorate'],'Doctoral degree~') .value_counts().to_frame().rename(columns={'Q4':'2021'}))df20_Ea_degree=(df20_Ea['Q4'].replace(['No formal education past high school', 'Some college/university study without earning a bachelor’s degree'],'~college') .replace(['Doctoral degree', 'Professional degree'],'Doctoral degree~') .value_counts().to_frame().rename(columns={'Q4':'2020'}))df19_Ea_degree=(df19_Ea['Q4'].replace(['No formal education past high school','Some college/university study without earning a bachelor’s degree'],'~college') .replace(['Doctoral degree', 'Professional degree'],'Doctoral degree~') .value_counts().to_frame().rename(columns={'Q4':'2019'}))df18_Ea_degree=(df18_Ea['Q4'].replace(['No formal education past high school', 'Some college/university study without earning a bachelor’s degree'],'~college') .replace(['Doctoral degree', 'Professional degree'],'Doctoral degree~') .value_counts().to_frame().rename(columns={'Q4':'2018'}))df17_Ea_degree=(df17_Ea['FormalEducation'] .replace(['No formal education past high school', 'Some college/university study without earning a bachelor’s degree'],'~college') .replace(['Doctoral degree', 'Professional degree'],'Doctoral degree~') .value_counts().to_frame() .rename(columns={'FormalEducation':'2017'} ,index = {'I did not complete any formal education past high school':'No formal education past high school','Master\\'s degree':'Master’s degree','Bachelor\\'s degree':'Bachelor’s degree','Some college/university study without earning a bachelor\\'s degree':'Some college/university study without earning a bachelor’s degree'}) ) concat1 = pd.concat([df21_Ea_degree,df20_Ea_degree],axis=1, join='outer') concat2 = pd.concat([df19_Ea_degree,df18_Ea_degree],axis=1, join='outer') concat3 = pd.concat([concat1,concat2],axis=1, join='outer') df21_Ea_degree_yearly_=concat3.join(df17_Ea_degree).fillna(0).transpose() #.transpose() 행 열 바꾸기df21_Ea_degree_yearly=df21_Ea_degree_yearly_.stack().to_frame().reset_index().rename(columns={'level_0':'year','level_1':'degree',0:'value'})df21_Ea_degree_yearly#graphfig = px.sunburst(df21_Ea_degree_yearly, path=['year','degree'], values=df21_Ea_degree_yearly['value'].tolist())fig.update_layout( margin = dict(t=10, l=10, r=10, b=10),colorway=(&quot;#F2798F&quot;,&quot;#88BFBA&quot;,&quot;#CDD9A3&quot;,'#F28705','#D9946C'))fig.update_layout(title='&lt;b&gt; Degree&lt;/b&gt;',title_font_size=25, margin = dict(t=100, l=100, r=50, b=100), height=700, width=700)fig.update_traces(hovertemplate='&lt;b&gt;Name&lt;/b&gt;: %{id}&lt;br&gt;'+ '&lt;b&gt;Count&lt;/b&gt;: %{value}&lt;br&gt;'+ '&lt;b&gt;Parent&lt;/b&gt;: %{parent}') fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.2, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() Plus we could see the advantages of Plotly in this graph. Matplotlib draws a static graph, but Plotly can dynamically click and move, and it supports zooming out, zooming in, and downloading graphs. Because all of our graphs are made of plotly, the viewer can represent or remove items in the graph if desired. With a click East Asia Degree Ratio: Bar plot 40% of master’s degrees or higher, and respondents have a high educational background. China and Japan have similar trends to East Asia and the World. The number of people itself is large, so a representative trend seems to appear here. However, it is noteworthy that the two countries have the same tendency. Korea: It is the only country among the four countries with a high degree of education below Ph.D., bachelor’s degree, and junior college. Only masters are low. (Polarization of education?) Taiwan: 1st place in master’s ratio (55%), 2nd place in Ph.D. or higher (13.8%). = The highest level of education. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091#data preprocessingdf21Edu_Ea = df21_Ea.loc[:,['Q3','Q4']].reset_index().rename(columns={'Q3':'East_Asia', 'Q4':'Dgree'}).fillna('etc')df21Edu_Ea =(df21Edu_Ea.replace({'I prefer not to answer':'etc'}).replace(['No formal education past high school', 'Some college/university study without earning a bachelor’s degree'],'~college') .replace(['Doctoral degree', 'Professional doctorate'],'Doctoral degree~'))df21Edu_Ea= (df21Edu_Ea .groupby(['East_Asia', 'Dgree']) .size() .reset_index() .rename(columns = {0:&quot;Count&quot;}))# 연령-지역 %dfKo_Edu21= df21Edu_Ea[df21Edu_Ea['East_Asia']=='South Korea']dfKo_Edu21['%']=((dfKo_Edu21['Count'] / dfKo_Edu21['Count'].sum()*100)).round(2)dfKo_Edu21=dfKo_Edu21.sort_values(by='%', ascending=False)dfTw_Edu21= df21Edu_Ea[df21Edu_Ea['East_Asia']=='Taiwan']dfTw_Edu21['%']=((dfTw_Edu21['Count'] / dfTw_Edu21['Count'].sum())*100).round(2)dfTw_Edu21=dfTw_Edu21.sort_values(by='%', ascending=False)dfCh_Edu21= df21Edu_Ea[df21Edu_Ea['East_Asia']=='China']dfCh_Edu21['%']=((dfCh_Edu21['Count'] / dfCh_Edu21['Count'].sum())*100).round(2)dfCh_Edu21=dfCh_Edu21.sort_values(by='%', ascending=False)dfJp_Edu21= df21Edu_Ea[df21Edu_Ea['East_Asia']=='Japan']dfJp_Edu21['%']=((dfJp_Edu21['Count'] / dfJp_Edu21['Count'].sum())*100).round(2)dfJp_Edu21=dfJp_Edu21.sort_values(by='%', ascending=False)# #data 완성# dfEdu_21_per = pd.concat([dfKo_Edu21, dfTw_Edu21, dfCh_Edu21, dfJp_Edu21], ignore_index = True)# dfEdu_21_per= pd.pivot(dfEdu_21_per, index = &quot;Dgree&quot;, columns = 'East_Asia', values = &quot;%&quot;).reset_index()# dfEdu_21_per#graphfig = make_subplots(rows = 1, cols = 4, shared_yaxes=True, vertical_spacing = 0.05)fig.add_trace(go.Bar(x = dfCh_Edu21['Dgree'], y = dfCh_Edu21['%'], text = dfCh_Edu21['%'].astype(str) + &quot;%&quot;, textposition='outside', name='China', marker_color='#88BFBA'), row = 1, col = 1)fig.add_trace(go.Bar(x = dfJp_Edu21['Dgree'], y = dfJp_Edu21['%'], text = dfJp_Edu21['%'].astype(str) + &quot;%&quot;, textposition='outside', name='Japan', marker_color='#CDD9A3'), row = 1, col = 2)fig.add_trace(go.Bar(x = dfKo_Edu21['Dgree'], y = dfKo_Edu21['%'], text = dfKo_Edu21['%'].astype(str) + &quot;%&quot;, textposition='outside', name='South Korea', marker_color='#F28705'), row = 1, col = 3)fig.add_trace(go.Bar(x = dfTw_Edu21['Dgree'], y = dfTw_Edu21['%'], text = dfTw_Edu21['%'].astype(str) + &quot;%&quot;, textposition='outside', name='Taiwan', marker_color='#D9946C'), row = 1, col = 4)fig.update_layout(showlegend=True,title='&lt;b&gt;Degree in East Asia&lt;/b&gt;',title_font_size=22, margin = dict(t=200, l=100, r=50, b=200), height=700, width=700)fig.update_traces(hovertemplate='&lt;b&gt;Percent&lt;/b&gt;: %{y}%&lt;br&gt;'+ '&lt;b&gt;Degree&lt;/b&gt;: %{x}&lt;br&gt;')fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.update_layout(legend=dict( orientation=&quot;h&quot;, yanchor=&quot;bottom&quot;, y=1.1, xanchor=&quot;right&quot;, x=1))fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.5, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() 3.1.6 Experience transformation Trends in World & East Asia Career: Stacked Scatter plot - < 2 years: 50% of the total. - 3-5 years: Decrease in the world, maintain East Asia ratio - 2021 'etc data' disappeared. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120#Exp data 전처리# Exp 뽑아오기Exp21_Wo = df21.loc[:,['Q3','Q6', 'year']].reset_index().rename(columns={'Q3':'Country', 'Q6':'Exp'}).fillna('etc')Exp20_Wo = df20.loc[:,['Q3','Q6','year']].reset_index().rename(columns={'Q3':'Country', 'Q6':'Exp'}).fillna('etc')Exp19_Wo = df19.loc[:,['Q3','Q15','year']].reset_index().rename(columns={'Q3':'Country', 'Q15':'Exp'}).fillna('etc')Exp18_Wo = df18.loc[:,['Q3','Q8','year']].reset_index().rename(columns={'Q3':'Country', 'Q8':'Exp'}).fillna('etc')Exp17_Wo = df17.loc[:,['Country','Tenure', 'year']].reset_index().rename(columns={'Country':'Country', 'Tenure':'Exp'}).fillna('etc')Exp21_Wo= Exp21_Wo.replace({'I have never written code': '&lt; 1 years', '1-3 years': '1-2 years'}).replace(['10-20 years', '20+ years'], '10+ years' )Exp20_Wo= Exp20_Wo.replace({'I have never written code': '&lt; 1 years'}).replace(['10-20 years', '20+ years'], '10+ years' )Exp19_Wo= Exp19_Wo.replace({'I have never written code': '&lt; 1 years'}).replace(['10-20 years', '20+ years'], '10+ years' )Exp18_Wo= (Exp18_Wo.replace({'0-1': '&lt; 1 years', '1-2': '1-2 years', '5-10':'5-10 years'}) .replace(['2-3', '3-4', '4-5'],'3-5 years') .replace(['10-15', '15-20','20-25', '30 +','25-30'],'10+ years'))Exp17_Wo=(Exp17_Wo.replace({'More than 10 years':'10+ years', '1 to 2 years':'1-2 years', 'Less than a year':'&lt; 1 years', '3 to 5 years':'3-5 years', &quot;I don't write code to analyze data&quot;:'&lt; 1 years', '6 to 10 years':'5-10 years'})) #data 정제(한꺼번에 이름바꾸기)Exp5y_Wo= pd.concat([Exp17_Wo, Exp18_Wo, Exp19_Wo, Exp20_Wo, Exp21_Wo]).reset_index()Exp5y_Wo=(Exp5y_Wo.groupby(['year', 'Exp']) .size() .reset_index() .rename(columns = {0:&quot;Count&quot;}))#percent data 넣기Exp21_per_W= Exp5y_Wo[Exp5y_Wo['year'] == &quot;2021&quot;].reset_index(drop = True)Exp21_per_W['percentage'] = Exp21_per_W[&quot;Count&quot;] / Exp21_per_W[&quot;Count&quot;].sum()Exp21_per_W['%'] = np.round(Exp21_per_W['percentage'] * 100, 1)Exp20_per_W = Exp5y_Wo[Exp5y_Wo['year'] == &quot;2020&quot;].reset_index(drop = True)Exp20_per_W['percentage'] = Exp20_per_W[&quot;Count&quot;] / Exp20_per_W[&quot;Count&quot;].sum()Exp20_per_W['%'] = np.round(Exp20_per_W['percentage'] * 100, 1)Exp19_per_W = Exp5y_Wo[Exp5y_Wo['year'] == &quot;2019&quot;].reset_index(drop = True)Exp19_per_W['percentage'] = Exp19_per_W[&quot;Count&quot;] / Exp19_per_W[&quot;Count&quot;].sum()Exp19_per_W['%'] = np.round(Exp19_per_W['percentage'] * 100, 1)Exp18_per_W = Exp5y_Wo[Exp5y_Wo['year'] == &quot;2018&quot;].reset_index(drop = True)Exp18_per_W['percentage'] = Exp18_per_W[&quot;Count&quot;] / Exp18_per_W[&quot;Count&quot;].sum()Exp18_per_W['%'] = np.round(Exp18_per_W['percentage'] * 100, 1)Exp17_per_W = Exp5y_Wo[Exp5y_Wo['year'] == &quot;2017&quot;].reset_index(drop = True)Exp17_per_W['percentage'] = Exp17_per_W[&quot;Count&quot;] / Exp17_per_W[&quot;Count&quot;].sum()Exp17_per_W['%'] = np.round(Exp17_per_W['percentage'] * 100, 1)#data 완성Exp5y_per_W = pd.concat([Exp17_per_W, Exp18_per_W, Exp19_per_W, Exp20_per_W, Exp21_per_W], ignore_index = True)Exp5y_per_W= pd.pivot(Exp5y_per_W, index = &quot;year&quot;, columns = 'Exp', values = &quot;%&quot;).reset_index()Exp5y_per_W.fillna('0')Exp5y_percent_order = Exp5y_per_W['year'].tolist()fig = go.Figure()fig.add_trace(go.Scatter( x = Exp5y_percent_order, y = Exp5y_per_W['&lt; 1 years'].tolist(), mode = &quot;lines&quot;, name = '&lt; 1 years', line = dict(width = 0.5), stackgroup = &quot;one&quot;, marker_color='#F2798F'))fig.add_trace(go.Scatter( x = Exp5y_percent_order, y = Exp5y_per_W['1-2 years'].tolist(), mode = &quot;lines&quot;, name = '1-2 years', line = dict(width = 0.5), stackgroup = &quot;one&quot;, marker_color='#88BFBA'))fig.add_trace(go.Scatter( x = Exp5y_percent_order, y = Exp5y_per_W['3-5 years'].tolist(), mode = &quot;lines&quot;, name = '3-5 years', line = dict(width = 0.5), stackgroup = &quot;one&quot;, marker_color='#CDD9A3'))fig.add_trace(go.Scatter( x = Exp5y_percent_order, y = Exp5y_per_W['5-10 years'].tolist(), mode = &quot;lines&quot;, name = '5-10 years', line = dict(width = 0.5), stackgroup = &quot;one&quot;, marker_color='#F28705'))fig.add_trace(go.Scatter( x = Exp5y_percent_order, y = Exp5y_per_W['10+ years'].tolist(), mode = &quot;lines&quot;, name = '10+ years', line = dict(width = 0.5), stackgroup = &quot;one&quot;, marker_color='#D9946C'))fig.add_trace(go.Scatter( x = Exp5y_percent_order, y = Exp5y_per_W['etc'].tolist(), mode = &quot;lines&quot;, name = 'etc', line = dict(width = 1), stackgroup = &quot;one&quot;, marker_color='#F2D64B'))fig.update_traces(hovertemplate='&lt;b&gt;Percent&lt;/b&gt;: %{y}%&lt;br&gt;')fig.update_layout(yaxis_range = (0, 100), title_font_size=20, title_text=&quot;&lt;b&gt;Experience in world&lt;/b&gt;&quot;, height=500, width=700, title_x=0.5)fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.2, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135#data preprocessingExp21 = df21_Ea.loc[:,['Q3','Q6', 'year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q6':'Exp'}).fillna('etc')Exp20 = df20_Ea.loc[:,['Q3','Q6','year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q6':'Exp'}).fillna('etc')Exp19 = df19_Ea.loc[:,['Q3','Q15','year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q15':'Exp'}).fillna('etc')Exp18 = df18_Ea.loc[:,['Q3','Q8','year']].reset_index().rename(columns={'Q3':'East_Asia', 'Q8':'Exp'}).fillna('etc')Exp17 = df17_Ea.loc[:,['Country','Tenure', 'year']].reset_index().rename(columns={'Country':'East_Asia', 'Tenure':'Exp'}).fillna('etc')Exp21Uni=['3-5 years', '&lt; 1 years', '1-3 years', '10-20 years', 'I have never written code', '5-10 years', '20+ years']Exp20Uni= ['3-5 years', '&lt; 1 years', '5-10 years', '1-2 years', 'etc', '20+ years', '10-20 years', 'I have never written code']Exp19Uni=['1-2 years', '5-10 years', '&lt; 1 years', 'I have never written code', '3-5 years', '10-20 years', '20+ years', 'etc']Exp18Uni=['0-1', '2-3', '1-2', '5-10', '3-4', '10-15', '15-20', '4-5', '20-25', '30 +', 'etc', '25-30']Exp17Uni=['More than 10 years', '1 to 2 years', 'etc', 'Less than a year', '3 to 5 years', &quot;I don't write code to analyze data&quot;, '6 to 10 years']Exp21= Exp21.replace({'I have never written code': '&lt; 1 years', '1-3 years': '1-2 years'}).replace(['10-20 years', '20+ years'], '10+ years' )Exp20= Exp20.replace({'I have never written code': '&lt; 1 years'}).replace(['10-20 years', '20+ years'], '10+ years' )Exp19= Exp19.replace({'I have never written code': '&lt; 1 years'}).replace(['10-20 years', '20+ years'], '10+ years' )Exp18= (Exp18.replace({'0-1': '&lt; 1 years', '1-2': '1-2 years', '5-10':'5-10 years'}) .replace(['2-3', '3-4', '4-5'],'3-5 years') .replace(['10-15', '15-20','20-25', '30 +','25-30'],'10+ years'))Exp17=(Exp17.replace({'More than 10 years':'10+ years', '1 to 2 years':'1-2 years', 'Less than a year':'&lt; 1 years', '3 to 5 years':'3-5 years', &quot;I don't write code to analyze data&quot;:'&lt; 1 years', '6 to 10 years':'5-10 years'})) Exp5y= pd.concat([Exp17, Exp18, Exp19, Exp20, Exp21]).reset_index()Exp5y=(Exp5y.groupby(['year', 'Exp']) .size() .reset_index() .rename(columns = {0:&quot;Count&quot;}))Exp21_percent = Exp5y[Exp5y['year'] == &quot;2021&quot;].reset_index(drop = True)Exp21_percent['percentage'] = Exp21_percent[&quot;Count&quot;] / Exp21_percent[&quot;Count&quot;].sum()Exp21_percent['%'] = np.round(Exp21_percent['percentage'] * 100, 1)Exp21_percentExp20_percent = Exp5y[Exp5y['year'] == &quot;2020&quot;].reset_index(drop = True)Exp20_percent['percentage'] = Exp20_percent[&quot;Count&quot;] / Exp20_percent[&quot;Count&quot;].sum()Exp20_percent['%'] = np.round(Exp20_percent['percentage'] * 100, 1)Exp20_percentExp19_percent = Exp5y[Exp5y['year'] == &quot;2019&quot;].reset_index(drop = True)Exp19_percent['percentage'] = Exp19_percent[&quot;Count&quot;] / Exp19_percent[&quot;Count&quot;].sum()Exp19_percent['%'] = np.round(Exp19_percent['percentage'] * 100, 1)Exp19_percentExp18_percent = Exp5y[Exp5y['year'] == &quot;2018&quot;].reset_index(drop = True)Exp18_percent['percentage'] = Exp18_percent[&quot;Count&quot;] / Exp18_percent[&quot;Count&quot;].sum()Exp18_percent['%'] = np.round(Exp18_percent['percentage'] * 100, 1)Exp18_percentExp17_percent = Exp5y[Exp5y['year'] == &quot;2017&quot;].reset_index(drop = True)Exp17_percent['percentage'] = Exp17_percent[&quot;Count&quot;] / Exp17_percent[&quot;Count&quot;].sum()Exp17_percent['%'] = np.round(Exp17_percent['percentage'] * 100, 1)Exp17_percent#graphExp5y_percent = pd.concat([Exp17_percent, Exp18_percent, Exp19_percent, Exp20_percent, Exp21_percent], ignore_index = True)Exp5y_percent= pd.pivot(Exp5y_percent, index = &quot;year&quot;, columns = 'Exp', values = &quot;%&quot;).reset_index()Exp5y_percent.fillna('0')Exp5y_percent_order = Exp5y_percent['year'].tolist()fig = go.Figure()fig.add_trace(go.Scatter( x = Exp5y_percent_order, y = Exp5y_percent['&lt; 1 years'].tolist(), mode = &quot;lines&quot;, name = '&lt; 1 years', line = dict(width = 0.5), stackgroup = &quot;one&quot;, marker_color='#F2798F'))fig.add_trace(go.Scatter( x = Exp5y_percent_order, y = Exp5y_percent['1-2 years'].tolist(), mode = &quot;lines&quot;, name = '1-2 years', line = dict(width = 0.5), stackgroup = &quot;one&quot;, marker_color='#88BFBA'))fig.add_trace(go.Scatter( x = Exp5y_percent_order, y = Exp5y_percent['3-5 years'].tolist(), mode = &quot;lines&quot;, name = '3-5 years', line = dict(width = 0.5), stackgroup = &quot;one&quot;, marker_color='#CDD9A3'))fig.add_trace(go.Scatter( x = Exp5y_percent_order, y = Exp5y_percent['5-10 years'].tolist(), mode = &quot;lines&quot;, name = '5-10 years', line = dict(width = 0.5), stackgroup = &quot;one&quot;, marker_color='#F28705'))fig.add_trace(go.Scatter( x = Exp5y_percent_order, y = Exp5y_percent['10+ years'].tolist(), mode = &quot;lines&quot;, name = '10+ years', line = dict(width = 0.5), stackgroup = &quot;one&quot;, marker_color='#D9946C'))fig.add_trace(go.Scatter( x = Exp5y_percent_order, y = Exp5y_percent['etc'].tolist(), mode = &quot;lines&quot;, name = 'etc', line = dict(width = 0.5), stackgroup = &quot;one&quot;, marker_color='#F2D64B'))fig.update_traces(hovertemplate='&lt;b&gt;Percent&lt;/b&gt;: %{y}%&lt;br&gt;')fig.update_layout(yaxis_range = (0, 100), title_text=&quot;&lt;b&gt;Experience in East Asia&lt;/b&gt;&quot;, height=500, width=700, title_font_size=20, title_x=0.5)fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.2, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() 3.1.7 Salary transformation World & East Asia Annual salary: Bar-H plot $ 200,000 ~ : World (2.9%) is more than 50% compared to East Asia (1.3%) $ ~250,000 : World (59.2%) is less than East Asia (50.3%) = East Asia’s annual salary gap between rich and poor is less. $ 25,000~60,000: The highest section in East Asia at 24%. = The annual salary section that we aim for. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103#data preprocessingdf21_salary_=df21['Q25'].value_counts().to_frame().rename(index={'$0-999':'&lt;999','&gt;$1,000,000':'1,000,000~','$500,000-999,999':'500,000-999,999'}).fillna(0)df21_Ea_salary_=df21_Ea['Q25'].value_counts().to_frame().rename(index={'$0-999':'&lt;999','&gt;$1,000,000':'1,000,000~','$500,000-999,999':'500,000-999,999'}).fillna(0)#퍼센트df21_salary__=(df21_salary_['Q25']/(df21_salary_['Q25'].sum())*100).round(1).to_frame().rename(columns={'Q25':'World'})df21_Ea_salary__=(df21_Ea_salary_['Q25']/(df21_Ea_salary_['Q25'].sum())*100).round(1).to_frame().rename(columns={'Q25':'EA'})#그룹화df21_salary=(df21_salary__.rename(index= {'1,000-1,999':'1,000-7,499', '2,000-2,999':'1,000-7,499', '3,000-3,999':'1,000-7,499', '4,000-4,999':'1,000-7,499', '5,000-7,499':'1,000-7,499'}) .rename(index={'7,500-9,999':'7,500-24,999', '10,000-14,999':'7,500-24,999', '15,000-19,999':'7,500-24,999', '20,000-24,999':'7,500-24,999' }) .rename(index={'25,000-29,999':'25,000-59,999', '30,000-39,999':'25,000-59,999', '40,000-49,999':'25,000-59,999', '50,000-59,999':'25,000-59,999'}) .rename(index={'60,000-69,999':'60,000-99,999', '70,000-79,999':'60,000-99,999', '80,000-89,999':'60,000-99,999', '90,000-99,999':'60,000-99,999'}) .rename(index={'100,000-124,999':'100,000-199,999', '125,000-149,999':'100,000-199,999', '150,000-199,999':'100,000-199,999'}) .rename(index={'200,000-249,999':'200,000-1,000,000~', '250,000-299,999':'200,000-1,000,000~', '300,000-499,999':'200,000-1,000,000~', '500,000-999,999':'200,000-1,000,000~', '1,000,000~':'200,000-1,000,000~'}) .reset_index().groupby('index').sum() .reindex(index = ['&lt;999', '1,000-7,499', '7,500-24,999', '25,000-59,999', '60,000-99,999', '100,000-199,999', '200,000-1,000,000~']))df21_Ea_salary=(df21_Ea_salary__.rename(index= {'1,000-1,999':'1,000-7,499', '2,000-2,999':'1,000-7,499', '3,000-3,999':'1,000-7,499', '4,000-4,999':'1,000-7,499', '5,000-7,499':'1,000-7,499'}) .rename(index={'7,500-9,999':'7,500-24,999', '10,000-14,999':'7,500-24,999', '15,000-19,999':'7,500-24,999', '20,000-24,999':'7,500-24,999'}) .rename(index={'25,000-29,999':'25,000-59,999', '30,000-39,999':'25,000-59,999', '40,000-49,999':'25,000-59,999', '50,000-59,999':'25,000-59,999'}) .rename(index={'60,000-69,999':'60,000-99,999', '70,000-79,999':'60,000-99,999', '80,000-89,999':'60,000-99,999', '90,000-99,999':'60,000-99,999'}) .rename(index={'100,000-124,999':'100,000-199,999', '125,000-149,999':'100,000-199,999', '150,000-199,999':'100,000-199,999'}) .rename(index={'200,000-249,999':'200,000-1,000,000~', '250,000-299,999':'200,000-1,000,000~', '300,000-499,999 ':'200,000-1,000,000~', '500,000-999,999':'200,000-1,000,000~', '1,000,000~':'200,000-1,000,000~'}) .reset_index().groupby('index').sum() .reindex(index = ['&lt;999', '1,000-7,499', '7,500-24,999', '25,000-59,999', '60,000-99,999', '100,000-199,999', '200,000-1,000,000~']))#graphWorld = df21_salary['World'].valuesEast_Asia = df21_Ea_salary['EA'].valuesy = df21_salary.indexfig = go.Figure(data=[ go.Bar(y=y, x=World, orientation='h', name=&quot;World&quot;, base=0, hovertemplate='&lt;b&gt;World&lt;/b&gt;: %{x}%&lt;br&gt;', marker_color='#979DA6'), go.Bar(y=y, x=-East_Asia, orientation='h', name=&quot;East Asia&quot;, base=0, hovertemplate='&lt;b&gt;East Asia&lt;/b&gt;: %{x}%&lt;br&gt;', marker_color='#F2D64B') ])fig.update_layout(barmode='stack')fig.update_layout( margin=dict(l=200, r=0, t=200, b=100), autosize=False, title_text=&quot;&lt;b&gt; Salary in East Asia vs World&lt;/b&gt;&quot;, height=600, width=700, title_font_size=20, title_x=0.5)fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.update_layout(legend=dict( orientation=&quot;h&quot;, yanchor=&quot;bottom&quot;, y=1.1, xanchor=&quot;right&quot;, x=1))fig.show() World experience and annual salary: Heat Map Relatively **positive correlation.** Even with 5-10 years of experience, more than 45% has an annual salary of less than $20,000 With more than 10 years of experience, more than 30% receive an annual salary of $100,000. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990#data preprocessingSalExp21= df21.loc[:, ['region', 'Q25', 'Q6']].rename(columns={'Q6':'Exp', 'Q25':'Salary'})SalExp21=(SalExp21 .replace(['0-999','$0-999','0'], '&lt; 999') .replace({'&gt;$1,000,000':'200,000~'}) .replace(['1,000-1,999','2,000-2,999','3,000-3,999', '4,000-4,999','5,000-7,499','7,500-9,999','10,000-14,999', '15,000-19,999'],'1,000-20,000') .replace(['20,000-24,999''25,000-29,999','30,000-39,999', '40,000-49,999', '50,000-59,999'],'20,000-59,999') .replace(['60,000-69,999', '70,000-79,999', '80,000-89,999', '90,000-99,999'], '60,000-99,999') .replace(['100,000-124,999', '300,000-499,999', '125,000-149,999', '125,000-149,999', '150,000-199,999'],'100,000-199,999') .replace(['200,000-249,999', '250,000-299,999', '1,000,000','$500,000-999,999'], '200,000~') .replace({'I have never written code': '&lt; 1 years'}) .replace(['10-20 years', '20+ years'], '10+ years' ) )sal_order=['&lt; 999', '1,000-20,000', '20,000-59,999', '60,000-99,999','100,000-199,999', '200,000~']Exp21_order=['&lt; 1 years', '1-3 years','3-5 years', '5-10 years', '10+ years' ]SalExp21_Ea = SalExp21[SalExp21['region'] == &quot;EastAsia&quot;].reset_index(drop = True)SalExp21_Ea=(SalExp21_Ea.groupby(['Exp', 'Salary']) .size() .unstack().fillna(0).astype('int64'))SalExp21_Wo = SalExp21[SalExp21['region'] == &quot;World&quot;].reset_index(drop = True)SalExp21_Wo=(SalExp21_Wo.groupby(['Exp', 'Salary']) .size() .unstack().fillna(0).astype('int64'))SalExp21_Wo#graph#Worldz = SalExp21_Woz = z[sal_order]z = z.reindex(Exp21_order)z_data = z.apply(lambda x:np.round(x/x.sum()*100, 2), axis = 1).to_numpy() # convert to correlation matrixx = sal_ordery = Exp21_orderfig = ff.create_annotated_heatmap(z_data, x = x, y = y, colorscale = &quot;sunset&quot;)fig.update_layout( title_text=&quot;&lt;b&gt;Experience and salary in World&lt;/b&gt;&quot;, height=700, width=700, title_font_size=20, title_x=0.5, margin=dict(l=100, r=100, t=200, b=100))fig.add_annotation(dict(font=dict(size=14), x=0.85, y=-0.1, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show()#East Asiaz = SalExp21_Eaz = z[sal_order]z = z.reindex(Exp21_order)z_data = z.apply(lambda x:np.round(x/x.sum(), 2), axis = 1).to_numpy() # convert to correlation matrixx = sal_ordery = Exp21_orderfig = ff.create_annotated_heatmap(z_data, x = x, y = y, colorscale = &quot;sunset&quot;)fig.update_layout(title_text=&quot;&lt;b&gt;Experience and salary in East Asia&lt;/b&gt;&quot;, height=700, width=700, title_font_size=20, title_x=0.5, margin=dict(l=100, r=100, t=200, b=100))fig.add_annotation(dict(font=dict(size=14), x=0.85, y=-0.1, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() World & East Asia Degree/Annual salary: Heat Map \\$ ~20,000 : Regardless of degree, about 40% of the annual salary is $ 20,000 or less. Guess it’s the ratio that comes from a student. $ 25,000-100,000 : Earned more than 40% with a bachelor’s degree alone in East Asia (World: less than 20%) $ 200,000~ : Even with a doctorate or higher, it is difficult to obtain it from East Asia. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798#data preprocessingSalary21= df21.loc[:, ['region', 'Q25', 'year']].rename(columns={'Q3':'Country', 'Q25':'Salary'})salary21_Index=['&lt; 999', '1,000-20,000', '20,000-59,999', '60,000-99,999','100,000-199,999', '200,000~']Salary21=(Salary21 .replace(['0-999','$0-999','0'], '&lt; 999') .replace({'&gt;$1,000,000':'200,000~'}) .replace(['1,000-1,999','2,000-2,999','3,000-3,999', '4,000-4,999','5,000-7,499','7,500-9,999','10,000-14,999', '15,000-19,999'],'1,000-20,000') .replace(['20,000-24,999''25,000-29,999','30,000-39,999', '40,000-49,999', '50,000-59,999'],'20,000-59,999') .replace(['60,000-69,999', '70,000-79,999', '80,000-89,999','90,000-99,999'], '60,000-99,999') .replace(['100,000-124,999', '300,000-499,999', '125,000-149,999', '125,000-149,999', '150,000-199,999'],'100,000-199,999') .replace(['200,000-249,999', '250,000-299,999','1,000,000','$500,000-999,999'], '200,000~')).fillna('0')sal_order=['&lt; 999', '1,000-20,000', '20,000-59,999', '60,000-99,999','100,000-199,999', '200,000~']Salary21=(Salary21.groupby(['region', 'Salary']) .size() .reset_index() .rename(columns = {0:&quot;Count&quot;}))Salary21_Ea = Salary21[Salary21['region'] == &quot;EastAsia&quot;].reset_index(drop = True)Salary21_Ea['%']=((Salary21_Ea['Count'] / Salary21_Ea['Count'].sum())*100).round(2)Salary21_Wo = Salary21[Salary21['region'] == &quot;World&quot;].reset_index(drop = True)Salary21_Wo['%']=((Salary21_Wo['Count'] / Salary21_Wo['Count'].sum())*100).round(2)Dgr_Sal_21= df21.loc[:, ['region', 'Q25', 'Q4']].rename(columns={'Q4':'Dgree', 'Q25':'Salary'})Dgr_Sal_21 = (Dgr_Sal_21.replace(['0-999','$0-999','0'], '&lt; 999') .replace({'&gt;$1,000,000':'200,000~'}) .replace(['1,000-1,999','2,000-2,999','3,000-3,999', '4,000-4,999','5,000-7,499','7,500-9,999','10,000-14,999', '15,000-19,999'],'1,000-20,000') .replace(['20,000-24,999''25,000-29,999','30,000-39,999', '40,000-49,999', '50,000-59,999'],'20,000-59,999') .replace(['60,000-69,999', '70,000-79,999', '80,000-89,999', '90,000-99,999'], '60,000-99,999') .replace(['100,000-124,999', '300,000-499,999', '125,000-149,999', '125,000-149,999','150,000-199,999'],'100,000-199,999') .replace(['200,000-249,999', '250,000-299,999','1,000,000','$500,000-999,999'], '200,000~') .replace({'I prefer not to answer':'etc'}) .replace(['No formal education past high school', 'Some college/university study without earning a bachelor’s degree'],'~college') .replace(['Doctoral degree', 'Professional doctorate'],'Doctoral degree~'))#EastAsia 뽑기Dgr_Sal_21_Ea= Dgr_Sal_21[Dgr_Sal_21['region'] == &quot;EastAsia&quot;].reset_index(drop = True)Dgr_Sal_21_Ea = Dgr_Sal_21_Ea.groupby(['Dgree', 'Salary']).size().unstack().fillna(0).astype('int64')dgree_order=[ '~college','Bachelor’s degree', 'Master’s degree', 'Doctoral degree~', 'etc']#graph#Worldz = Dgr_Sal_21.groupby(['Dgree', 'Salary']).size().unstack().fillna(0).astype('int64')z = z[sal_order]z = z.reindex(dgree_order)z_data = z.apply(lambda x:np.round(x/x.sum()*100, 2), axis = 1).to_numpy() # convert to correlation matrixx = sal_ordery = dgree_orderfig = ff.create_annotated_heatmap(z_data, x = x, y = y, colorscale = &quot;sunset&quot;)fig.update_layout( title_text=&quot;&lt;b&gt; Degree-Salary in World&lt;/b&gt;&quot;, height=700, width=700, title_font_size=20, title_x=0.5, margin=dict(l=150, r=100, t=200, b=50))fig.update_traces(hovertemplate='&lt;b&gt;Degree&lt;/b&gt;: %{y}&lt;br&gt;'+ '&lt;b&gt;Salary&lt;/b&gt;: %{x}&lt;br&gt;'+ '&lt;b&gt;Percent&lt;/b&gt;: %{z}%')fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.1, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show()#East Asiaz = Dgr_Sal_21_Eaz = z[sal_order]z = z.reindex(dgree_order)z_data = z.apply(lambda x:np.round(x/x.sum()*100, 2), axis = 1).to_numpy() # convert to correlation matrixx = sal_ordery = dgree_orderfig = ff.create_annotated_heatmap(z_data, x = x, y = y, colorscale = &quot;sunset&quot;)fig.update_layout(title_text=&quot;&lt;b&gt; Degree-Salary in East Asia&lt;/b&gt;&quot;, height=700, width=700, title_font_size=20, title_x=0.5, margin=dict(l=150, r=100, t=200, b=50))fig.update_traces(hovertemplate='&lt;b&gt;Degree&lt;/b&gt;: %{y}&lt;br&gt;'+ '&lt;b&gt;Salary&lt;/b&gt;: %{x}&lt;br&gt;'+ '&lt;b&gt;Percent&lt;/b&gt;: %{z}%')fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.1, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() 3.1.8 Language transformation World & East Asia Programming Language: Bar plot - Python: 80% of the world and 85% of East Asia use it. We've been working on the project as python, so I hope we can continue to learn python and become experienced Data Scientists! 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081#data preprocessing#worldprogramming_list = [&quot;Python&quot;, &quot;R&quot;, &quot;SQL&quot;, &quot;Java&quot;, &quot;C&quot;, &quot;Bash&quot;, &quot;Javascript&quot;, &quot;C++&quot;]programming_df = pd.Series(programming_list)df_2019 = df19[df19['Q19'].isin(programming_df)]df_2020 = df20[df20['Q8'].isin(programming_df)]df_2021 = df21[df21['Q8'].isin(programming_df)]df19Lag = df_2019.loc[:, ['region', 'Q5', 'Q19', 'year']]df19Lag = df19Lag.rename(columns = {'Q19': 'Language'}, inplace = False) # To match with other datasetsdf20Lag = df_2020.loc[:, ['region', 'Q5', 'Q8', 'year']].rename(columns = {'Q8': 'Language'}, inplace = False)df21Lag = df_2021.loc[:, ['region', 'Q5', 'Q8', 'year']].rename(columns = {'Q8': 'Language'}, inplace = False)df3y_Lag = pd.concat([df19Lag, df20Lag, df21Lag])df3y_Lag = df3y_Lag.groupby(['year', 'Language']).size().reset_index().rename(columns = {0:&quot;Count&quot;})df3y_Lag# 2019dfLang_19 = df3y_Lag[df3y_Lag['year'] == &quot;2019&quot;].reset_index(drop = True)dfLang_19['percentage'] = dfLang_19[&quot;Count&quot;] / dfLang_19[&quot;Count&quot;].sum()dfLang_19['%'] = np.round(dfLang_19['percentage'] * 100, 1)# 2020dfLang_20 = df3y_Lag[df3y_Lag['year'] == &quot;2020&quot;].reset_index(drop = True)dfLang_20['percentage'] = dfLang_20[&quot;Count&quot;] / dfLang_20[&quot;Count&quot;].sum()dfLang_20['%'] = np.round(dfLang_20['percentage'] * 100, 1)# 2021dfLang_21 = df3y_Lag[df3y_Lag['year'] == &quot;2021&quot;].reset_index(drop = True)dfLang_21['percentage'] = dfLang_21[&quot;Count&quot;] / dfLang_21[&quot;Count&quot;].sum()dfLang_21['%'] = np.round(dfLang_21['percentage'] * 100, 1)dfLang_19=dfLang_19.sort_values(by='%', ascending=False)dfLang_20=dfLang_20.sort_values(by='%', ascending=False)dfLang_21=dfLang_21.sort_values(by='%', ascending=False)#graphfig = go.Figure()fig.add_trace(go.Bar(x = dfLang_19['Language'], y = dfLang_19['%'], name = &quot;2019&quot;, text = dfLang_19['%'].astype(str) + &quot;%&quot;, textposition='auto', marker_color='#CDD9A3'))fig.add_trace(go.Bar(x = dfLang_20['Language'], y = dfLang_20['%'], name = &quot;2020&quot;, text = dfLang_20['%'].astype(str) + &quot;%&quot;, textposition='auto', marker_color='#F28705'))fig.add_trace(go.Bar(x = dfLang_21['Language'], y = dfLang_21['%'], name = &quot;2021&quot;, text = dfLang_21['%'].astype(str) + &quot;%&quot;, textposition='auto', marker_color='#88BFBA'))fig.update_layout(title='&lt;b&gt;Language in World&lt;/b&gt;',title_font_size=20, margin = dict(t=100, l=100, r=50, b=100), height=600, width=700, xaxis_title=None, yaxis_title=None)fig.update_traces(hovertemplate='&lt;b&gt;Percent&lt;/b&gt;: %{y}%&lt;br&gt;'+ '&lt;b&gt;Language&lt;/b&gt;: %{x}&lt;br&gt;')fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.update_layout(legend=dict( orientation=&quot;v&quot;, yanchor=&quot;bottom&quot;, y=0.8, xanchor=&quot;right&quot;, x=1))fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.2, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#data prprocessing#Eadf_2019 = df19_Ea[df19_Ea['Q19'].isin(programming_df)]df_2020 = df20_Ea[df20_Ea['Q8'].isin(programming_df)]df_2021 = df21_Ea[df21_Ea['Q8'].isin(programming_df)]df19Lag = df_2019.loc[:, ['region', 'Q5', 'Q19', 'year']]df19Lag = df19Lag.rename(columns = {'Q19': 'Language'}, inplace = False) # To match with other datasetsdf20Lag = df_2020.loc[:, ['region', 'Q5', 'Q8', 'year']].rename(columns = {'Q8': 'Language'}, inplace = False)df21Lag = df_2021.loc[:, ['region', 'Q5', 'Q8', 'year']].rename(columns = {'Q8': 'Language'}, inplace = False)df3y_Lag = pd.concat([df19Lag, df20Lag, df21Lag])df3y_Lag = df3y_Lag.groupby(['year', 'Language']).size().reset_index().rename(columns = {0:&quot;Count&quot;})df3y_Lag# 2019dfLang_19 = df3y_Lag[df3y_Lag['year'] == &quot;2019&quot;].reset_index(drop = True)dfLang_19['percentage'] = dfLang_19[&quot;Count&quot;] / dfLang_19[&quot;Count&quot;].sum()dfLang_19['%'] = np.round(dfLang_19['percentage'] * 100, 1)# 2020dfLang_20 = df3y_Lag[df3y_Lag['year'] == &quot;2020&quot;].reset_index(drop = True)dfLang_20['percentage'] = dfLang_20[&quot;Count&quot;] / dfLang_20[&quot;Count&quot;].sum()dfLang_20['%'] = np.round(dfLang_20['percentage'] * 100, 1)# 2021dfLang_21 = df3y_Lag[df3y_Lag['year'] == &quot;2021&quot;].reset_index(drop = True)dfLang_21['percentage'] = dfLang_21[&quot;Count&quot;] / dfLang_21[&quot;Count&quot;].sum()dfLang_21['%'] = np.round(dfLang_21['percentage'] * 100, 1)dfLang_19=dfLang_19.sort_values(by='%', ascending=False)dfLang_20=dfLang_20.sort_values(by='%', ascending=False)dfLang_21=dfLang_21.sort_values(by='%', ascending=False)#graphfig = go.Figure()fig.add_trace(go.Bar(x = dfLang_19['Language'], y = dfLang_19['%'], name = &quot;2019&quot;, text = dfLang_19['%'].astype(str) + &quot;%&quot;, textposition='auto', marker_color='#CDD9A3'))fig.add_trace(go.Bar(x = dfLang_20['Language'], y = dfLang_20['%'], name = &quot;2020&quot;, text = dfLang_20['%'].astype(str) + &quot;%&quot;, textposition='auto', marker_color='#F28705'))fig.add_trace(go.Bar(x = dfLang_21['Language'], y = dfLang_21['%'], name = &quot;2021&quot;, text = dfLang_21['%'].astype(str) + &quot;%&quot;, textposition='auto', marker_color='#88BFBA'))fig.update_layout(title='&lt;b&gt;Language in EastAsia&lt;/b&gt;',title_font_size=20, margin = dict(t=100, l=100, r=50, b=100), height=600, width=700, xaxis_title=None, yaxis_title=None)fig.update_traces(hovertemplate='&lt;b&gt;Percent&lt;/b&gt;: %{text}')fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.2, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() 3.2 Position of Data Scientist in East Asia 123456789101112131415161718192021222324# data preprocessingdf21_Ea_DS = df21_Ea[df21_Ea['Q5'].isin(Data_Scientist)].fillna(0)salary_order= ['&lt;999', '1,000-19,999', '20,000-59,999', '60,000-99,999','100,000-199,999', '200,000~']dgree_order=[ '~college','Bachelor’s degree', 'Master’s degree', 'Doctoral degree~', 'etc']df21_Ea_DS=(df21_Ea_DS #salary .replace({'$0-999':'&lt;999','&gt;$1,000,000':'1,000,000~','$500,000-999,999':'500,000-999,999'}) .replace(['1,000-1,999','2,000-2,999','3,000-3,999', '4,000-4,999','5,000-7,499','7,500-9,999','10,000-14,999', '15,000-19,999'],'1,000-19,999') .replace(['20,000-24,999','25,000-29,999','30,000-39,999', '40,000-49,999', '50,000-59,999'],'20,000-59,999') .replace(['60,000-69,999', '70,000-79,999', '80,000-89,999', '90,000-99,999'], '60,000-99,999') .replace(['100,000-124,999','125,000-149,999','150,000-199,999'],'100,000-199,999') .replace(['200,000-249,999', '250,000-299,999', '300,000-499,999','500,000-999,999', '1,000,000~'], '200,000~') #degree .replace({'I prefer not to answer':'etc'}) .replace(['No formal education past high school','Some college/university study without earning a bachelor’s degree'],'~college') .replace(['Doctoral degree', 'Professional doctorate'],'Doctoral degree~') )sal_order= ['&lt;999', '1,000-19,999', '20,000-59,999', '60,000-99,999','100,000-199,999', '200,000~']dgree_order=[ '~college','Bachelor’s degree', 'Master’s degree', 'Doctoral degree~', 'etc'] 3.2.1 Salary 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162df21_Ea_DS_= df21_Ea_DS.loc[:,['Q5','Q25']].reset_index().rename(columns={'Q5':'Data_Scientist', 'Q25':'Salary'}).fillna('etc')df21_Ea_DS_= (df21_Ea_DS_.groupby(['Data_Scientist', 'Salary']).size() .reset_index() .rename(columns = {0:&quot;Count&quot;}))#Data Scientistdf21_Ea_DS_Ds = df21_Ea_DS_[df21_Ea_DS_['Data_Scientist'] == &quot;Data Scientist&quot;].reset_index(drop = True)df21_Ea_DS_Ds['%']=((df21_Ea_DS_Ds['Count'] / df21_Ea_DS_Ds['Count'].sum())*100).round(2)#Machine Learning Engineerdf21_Ea_DS_Mle = df21_Ea_DS_[df21_Ea_DS_['Data_Scientist'] == &quot;Machine Learning Engineer&quot;].reset_index(drop = True)df21_Ea_DS_Mle['%']=((df21_Ea_DS_Mle['Count'] / df21_Ea_DS_Mle['Count'].sum())*100).round(2)#Research Scientistdf21_Ea_DS_Rs = df21_Ea_DS_[df21_Ea_DS_['Data_Scientist'] == &quot;Research Scientist&quot;].reset_index(drop = True)df21_Ea_DS_Rs['%']=((df21_Ea_DS_Rs['Count'] / df21_Ea_DS_Rs['Count'].sum())*100).round(2)df21_Ea_DS_Rsdf21_Ea_DS_salary = pd.concat([df21_Ea_DS_Ds, df21_Ea_DS_Mle, df21_Ea_DS_Rs], ignore_index = True)df21_Ea_DS_salary= pd.pivot(df21_Ea_DS_salary, index = &quot;Salary&quot;, columns = 'Data_Scientist', values = &quot;%&quot;).reset_index().fillna('0')df21_Ea_DS_salary= df21_Ea_DS_salary.set_index(&quot;Salary&quot;).reindex(sal_order)#graphfig = go.Figure()fig.add_trace(go.Bar(x = df21_Ea_DS_salary.index, y = df21_Ea_DS_salary['Data Scientist'], name = &quot;Data Scientist&quot;, text = df21_Ea_DS_salary['Data Scientist'].astype(str) + &quot;%&quot;, textposition='auto', marker_color='#F2798F'))fig.add_trace(go.Bar(x = df21_Ea_DS_salary.index, y = df21_Ea_DS_salary['Machine Learning Engineer'], name = &quot;Machine Learning Engineer&quot;, text = df21_Ea_DS_salary['Machine Learning Engineer'].astype(str) + &quot;%&quot;, textposition='auto', marker_color='#CDD9A3'))fig.add_trace(go.Bar(x = df21_Ea_DS_salary.index, y = df21_Ea_DS_salary['Research Scientist'], name = &quot;Research Scientist&quot;, text = df21_Ea_DS_salary['Research Scientist'].astype(str) + &quot;%&quot;, textposition='auto', marker_color='#88BFBA'))fig.update_layout(barmode='stack', showlegend=True, height=600, width=700, title_text=&quot;&lt;b&gt;Data Scientist's Salary in East Asia&lt;/b&gt;&quot;, title_x=0.5, title_font_size=20, margin=dict(l=100, r=100, t=100, b=100))fig.update_traces(hovertemplate='&lt;b&gt;Percent&lt;/b&gt;: %{y}%&lt;br&gt;'+ '&lt;b&gt;Salary&lt;/b&gt;: %{x}$&lt;br&gt;')fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.update_layout(legend=dict( orientation=&quot;v&quot;, yanchor=&quot;bottom&quot;, y=0.8, xanchor=&quot;right&quot;, x=1.2))fig.show() 3.2.2 Salary-Experience 12345678910111213141516171819202122232425df21Ea_DS_ExSal = df21_Ea_DS.loc[:,['Q6','Q25']].reset_index().rename(columns={'Q25':'Salary', 'Q6':'Exp'}).fillna('etc')df21Ea_DS_ExSal= (df21Ea_DS_ExSal.groupby(['Exp', 'Salary']).size().unstack().fillna(0).astype('int64'))Exp_order=['&lt; 1 years','1-3 years','3-5 years', '5-10 years', '10-20 years', '20+ years', 'I have never written code']df21Ea_DS_ExSalz = df21Ea_DS_ExSalz = z[sal_order]z = z.reindex(Exp_order)z_data = z.apply(lambda x:np.round(x/x.sum()*100, 2), axis = 1).to_numpy() # convert to correlation matrixx = sal_ordery = Exp_orderfig = ff.create_annotated_heatmap(z_data, x = x, y = y, colorscale = &quot;sunset&quot;)fig.update_layout(title_text=&quot;&lt;b&gt; Data Scientist's Experience &amp; Salary &lt;/b&gt;&quot;,title_font_size=20, height=700, width=700, title_x=0.5, margin=dict(l=100, r=100, t=200, b=100))fig.update_traces(hovertemplate='&lt;b&gt;Salary&lt;/b&gt;: %{y}&lt;br&gt;'+ '&lt;b&gt;Experience&lt;/b&gt;: %{x}&lt;br&gt;'+ '&lt;b&gt;Percent&lt;/b&gt;: %{z}%')fig.show() 3.2.3 Degree 1234567891011121314151617181920212223df21_Ea_degree = df21_Ea_DS['Q4'].value_counts().to_frame()degree = df21_Ea_degree.indexvalues = df21_Ea_degree['Q4'].tolist()colors = ['#F2798F','#88BFBA', '#CDD9A3', '#F28705', '#D9946C']fig = go.Figure(data=[go.Bar(name='Degree', x=degree, y=values ,orientation='v', marker_color=colors, text=values, textposition='outside')])fig.update_layout(title_text=&quot;&lt;b&gt;Data Scientist's Degree (2021)&lt;/b&gt;&quot;, title_font_size=20, height=600, width=700, title_x=0.5, margin=dict(l=100, r=100, t=200, b=100))fig.update_traces(hovertemplate='&lt;b&gt;Count&lt;/b&gt;: %{y}&lt;br&gt;'+ '&lt;b&gt;Degree&lt;/b&gt;: %{x}&lt;br&gt;')fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.2, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() 3.2.4 Salary-Degree 1234567891011121314151617181920212223242526272829303132df21Ea_DS_EduSal= df21_Ea_DS.loc[:, ['Q4', 'Q25']].rename(columns={'Q4':'Edu', 'Q25':'Salary'})df21Ea_DS_EduSal['Edu'].unique()Edu_order=['~college', 'Bachelor’s degree','Master’s degree', 'Doctoral degree~', 'etc']df21Ea_DS_EduSal= (df21Ea_DS_EduSal.groupby(['Edu', 'Salary']).size().unstack().fillna(0).astype('int64'))df21Ea_DS_EduSalz = df21Ea_DS_EduSalz = z[sal_order]z = z.reindex(Edu_order)z_data = z.apply(lambda x:np.round(x/x.sum()*100, 2), axis = 1).to_numpy() # convert to correlation matrixx = sal_ordery = Edu_orderfig = ff.create_annotated_heatmap(z_data, x = x, y = y, colorscale = &quot;sunset&quot;)fig.update_layout(title_text=&quot;&lt;b&gt; Data Scientist's Degree &amp; Salary &lt;/b&gt;&quot;, title_font_size=20, height=700, width=700, title_x=0.5, margin=dict(l=150, r=100, t=200, b=50))fig.update_traces(hovertemplate='&lt;b&gt;Degree&lt;/b&gt;: %{y}&lt;br&gt;'+ '&lt;b&gt;Salary&lt;/b&gt;: %{x}&lt;br&gt;'+ '&lt;b&gt;Percent&lt;/b&gt;: %{z}%')fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.1, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() 3.2.5 Language 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677#data preprocessingdf20_Ea_DS = df20_Ea[df20_Ea['Q5'].isin(Data_Scientist)]df19_Ea_DS =df19_Ea[df19_Ea['Q5'].isin(Data_Scientist)]df19Ea_DSLag = df19_Ea_DS.loc[:, [ 'Q5', 'Q19', 'year']]df19Ea_DSLag = df19Ea_DSLag.rename(columns = {'Q19': 'Language'}, inplace = False) # To match with other datasetsdf20Ea_DSLag = df20_Ea_DS.loc[:, [ 'Q5', 'Q8', 'year']].rename(columns = {'Q8': 'Language'}, inplace = False)df21Ea_DSLag = df21_Ea_DS.loc[:, [ 'Q5', 'Q8', 'year']].rename(columns = {'Q8': 'Language'}, inplace = False)df3y_Ds_Lag = pd.concat([df19Ea_DSLag, df20Ea_DSLag, df21Ea_DSLag])df3y_Ds_Lag = df3y_Ds_Lag.groupby(['year', 'Language']).size().reset_index().rename(columns = {0:&quot;Count&quot;})df3y_Ds_Lag# 2019dfLang_Ds_19 = df3y_Ds_Lag[df3y_Ds_Lag['year'] == &quot;2019&quot;].reset_index(drop = True)dfLang_Ds_19['percentage'] = dfLang_Ds_19[&quot;Count&quot;] / dfLang_Ds_19[&quot;Count&quot;].sum()dfLang_Ds_19['%'] = np.round(dfLang_Ds_19['percentage'] * 100, 1)# 2020dfLang_Ds_20 = df3y_Ds_Lag[df3y_Ds_Lag['year'] == &quot;2020&quot;].reset_index(drop = True)dfLang_Ds_20['percentage'] = dfLang_Ds_20[&quot;Count&quot;] / dfLang_Ds_20[&quot;Count&quot;].sum()dfLang_Ds_20['%'] = np.round(dfLang_Ds_20['percentage'] * 100, 1)# 2021dfLang_Ds_21 = df3y_Ds_Lag[df3y_Ds_Lag['year'] == &quot;2021&quot;].reset_index(drop = True)dfLang_Ds_21['percentage'] = dfLang_Ds_21[&quot;Count&quot;] / dfLang_Ds_21[&quot;Count&quot;].sum()dfLang_Ds_21['%'] = np.round(dfLang_Ds_21['percentage'] * 100, 1)dfLang_Ds_19=dfLang_Ds_19.sort_values(by='%', ascending=False)dfLang_Ds_20=dfLang_Ds_20.sort_values(by='%', ascending=False)dfLang_Ds_21=dfLang_Ds_21.sort_values(by='%', ascending=False)#graphfig = go.Figure()fig.add_trace(go.Bar(x = dfLang_Ds_19['Language'], y = dfLang_Ds_19['%'], name = &quot;2019&quot;, text = dfLang_Ds_19['%'].astype(str) + &quot;%&quot;, textposition='auto', marker_color='#CDD9A3'))fig.add_trace(go.Bar(x = dfLang_Ds_20['Language'], y = dfLang_Ds_20['%'], name = &quot;2020&quot;, text = dfLang_Ds_20['%'].astype(str) + &quot;%&quot;, textposition='auto', marker_color='#F28705'))fig.add_trace(go.Bar(x = dfLang_Ds_21['Language'], y = dfLang_Ds_21['%'], name = &quot;2021&quot;, text = dfLang_Ds_21['%'].astype(str) + &quot;%&quot;, textposition='auto', marker_color='#88BFBA'))fig.update_layout(title='&lt;b&gt; The language used by the data scientist&lt;/b&gt;',title_font_size=22, margin = dict(t=120, l=100, r=10, b=150), height=600, width=700)fig.update_traces(hovertemplate='&lt;b&gt;Percent&lt;/b&gt;: %{y}%&lt;br&gt;'+ '&lt;b&gt;Language&lt;/b&gt;: %{x}&lt;br&gt;')fig.update_xaxes(showgrid=False)fig.update_yaxes(showgrid=False)fig.update_layout(legend=dict( orientation=&quot;h&quot;, yanchor=&quot;bottom&quot;, y=0.8, xanchor=&quot;right&quot;, x=1))fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.2, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() 1234567891011121314151617181920ds_pc=(df21_Ea_DS.loc[:, ['Q5','Q25','Q6','Q4','Q8']] .replace({'I have never written code': '&lt; 1 years', '1-3 years': '1-2 years'}) .replace(['10-20 years', '20+ years'], '10+ years' ) .replace([0,'&lt;999']) )fig = px.parallel_categories(ds_pc, labels={'Q5':'Job', 'Q25':'Salary', 'Q6':'Experience', 'Q4':'Degree', 'Q8':'Language'})fig.update_layout(hovermode = 'x')fig.update_layout(title='&lt;b&gt; Data Scientist&lt;/b&gt;',title_font_size=20, margin = dict(t=120, l=100, r=10, b=150), height=600, width=700)fig.add_annotation(dict(font=dict(size=14), x=0.8, y=-0.2, showarrow=False, text=&quot;@green_yhjw&quot;, xanchor='left', xref=&quot;paper&quot;, yref=&quot;paper&quot;))fig.show() 4. Ref. Ref. 동아시아 지역 https://ko.wikipedia.org/wiki/%EB%8F%99%EC%95%84%EC%8B%9C%EC%95%84 동아시아 인구 https://ko.wikipedia.org/wiki/%EC%95%84%EC%8B%9C%EC%95%84%EC%9D%98_%EC%9D%B8%EA%B5%AC 세계 인구 https://ko.wikipedia.org/wiki/%EC%84%B8%EA%B3%84_%EC%9D%B8%EA%B5%AC https://ko.wikipedia.org/wiki/%EC%9D%B8%EA%B0%84_%EA%B0%9C%EB%B0%9C_%EC%A7%80%EC%88%98#2020%EB%85%84 동아시아 인간개발지수 https://namu.wiki/w/%EB%8F%99%EC%95%84%EC%8B%9C%EC%95%84 Data Scientist란 https://dataprofessional.tistory.com/126 https://terms.naver.com/entry.naver?docId=1691563&amp;cid=42171&amp;categoryId=42183 Kaggle이란 https://ko.wikipedia.org/wiki/%EC%BA%90%EA%B8%80 Python이란 https://ko.wikipedia.org/wiki/%ED%8C%8C%EC%9D%B4%EC%8D%AC Kaggle competition Ref. https://www.kaggle.com/miguelfzzz/the-typical-kaggle-data-scientist-in-2021 https://www.kaggle.com/desalegngeb/how-popular-is-kaggle-in-africa flaricon: Icons made by Freepik from www.flaticon.com 5. close 안녕하세요 한국에 사는 YH입니다. python을 배운지 한달이 채 안되서 명이 한 팀이 되어 이번 대회에 참가 하게 되었습니다. 많이 부족하지만 여기까지 읽어 주셔서 감사합니다. 아직은 너무너무 부족한 제출물 이지만, 앞으로 열심히 해서 케글 대회에서 1등하는 그 날까지 지켜봐 주세요 ^^! 혹시 코멘트로 다 전하지 못하셨던 말이 있으시다면, 저의 github blog에 방문하여 도움을 주세요! 별거 없지만 놀러오세요 ;-) Hello, I’m YH and I live in Korea.Less than a month after learning python, people became a team and participated in this competition. It’s not enough, but thank you for reading it up to here. It’s still not enough, but please watch until the day we win first place at the Kaggle competition ^^! If there’s anything you haven’t said in the comments, please visit my github blog and help me! It’s nothing special, but come and play. ;-)","link":"/2021/11/28/kgg/KaggleCompetition_ver33byYH_Finall/"},{"title":"R for DS_03 ggplot2","text":"Welcome 저작권 : “R for DataScience by Hadley Wickham and Garrett Grolemund(O’Reilly). Copyright 2017 Garrett Grolemund, Hadley Wickham, 978-1-491-91039-9 Introduction how to visualise your data using ggplot2. R ggplot2는 그래프를 그려주는 프로그램ggplot2 이론배경 3.1.1 Prerequisites 12345678910111213install.packages(&quot;tidyverse&quot;)library(tidyverse)#&gt; ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──#&gt; ✔ ggplot2 3.3.2 ✔ purrr 0.3.4#&gt; ✔ tibble 3.0.3 ✔ dplyr 1.0.2#&gt; ✔ tidyr 1.1.2 ✔ stringr 1.4.0#&gt; ✔ readr 1.4.0 ✔ forcats 0.5.0#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──#&gt; ✖ dplyr::filter() masks stats::filter()#&gt; ✖ dplyr::lag() masks stats::lag()install.packages(c(&quot;nycflights13&quot;, &quot;gapminder&quot;, &quot;Lahman&quot;)) 3.2 First steps 3.2.1 The mpg data frame US Environmental Protection Agency on 38 models of car A data frame is a rectangular 1mpg displ = car’s engine size, in litres hwy = fuel efficiency in miles per gallon (mpg) 3.2.2 Creating a ggplot12ggplot(data = mpg) + geom_point(mapping = aes(x= displ, y = hwy)) ggplot(data = mpg) : 비어있는 Graph를 만들어 준다. geom_point() : Layers 추가 scatterplot mapping = aes(x= displ, y = hwy) : x와 y를 mapping 해 준다. 3.2.3 A graphing templateggplot(data = &lt;DATA&gt;) + &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;)) 이런 식으로 쓰면 된다고 함. (모형) 3.3 Aesthetic mappings aesthetic : 래전드 모양, 색 크기 value : data level : aesthetic properties size : 크기 color = colour, aesthetic의 색 alpha = shape , aesthetic의 모양 12ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, color = class)) 12ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, size = class)) colour , color : 모두 써도 됨. 12ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y= hwy, alpha = class)) 12ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, shape = class)) (수동으로 색 설정) 래전드를 생성 하지 않으면서 color만 바꿀 수 있다. 12ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy), color = &quot;blue&quot;) 12ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, color = &quot;blue&quot;)) mpg 내의 data가 color이라는 column이 있다. 그 data가 “blue”인 data들의 displ과 hwy의 Graph 아직 덜 했다 ! rmarkdown_Book R publishingRggplot","link":"/2021/11/29/R/R%20for%20Ds_03%20ggplot2/"},{"title":"R for DS_01 welcome &amp; Introduction","text":"Welcome 저작권 : “R for DataScience by Hadley Wickham and Garrett Grolemund(O’Reilly). Copyright 2017 Garrett Grolemund, Hadley Wickham, 978-1-491-91039-9 Introduction Data science is an exciting discipline that allows you to turn raw data into understanding, insight, and knowledge. The goal of “R for Data Science” is to help you learn the most important tools in R that will allow you to do data science. After reading this book, you’ll have the tools to tackle a wide variety of data science challenges, using the best parts of R. R을 이용한 data science를 해 봅시다. ggplot2를 더 잘 쓰고 싶다면A Layered Grammar of Graphics을 읽어보세요. 1.1 What you will learn 1.2 How this book is organised1.3 What you won’t learn1.4 Prerequisites1.4.1 R https://cloud.r-project.org,1.4.2 RStudio http://www.rstudio.com/download.1.4.3 The tidyverse 1install.packages(&quot;tidyverse&quot;) https://cloud.r-project.org/ 123456789library(tidyverse)#&gt; ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──#&gt; ✔ ggplot2 3.3.2 ✔ purrr 0.3.4#&gt; ✔ tibble 3.0.3 ✔ dplyr 1.0.2#&gt; ✔ tidyr 1.1.2 ✔ stringr 1.4.0#&gt; ✔ readr 1.4.0 ✔ forcats 0.5.0#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──#&gt; ✖ dplyr::filter() masks stats::filter()#&gt; ✖ dplyr::lag() masks stats::lag() tidyverse : ggplot2, tibble, tidyr, readr, purrr, and dplyr packages1.4.4 Other packages1install.packages(c(&quot;nycflights13&quot;, &quot;gapminder&quot;, &quot;Lahman&quot;)) 1.5 Running R code1.6 Getting help and learning more1.7 Acknowledgements1.8 Colophon ref. introduction Part 3 more","link":"/2021/11/29/R/R_R4ds_Welcom/"},{"title":"loan_classification","text":"1. 병렬처리를 위한 패키지 불러오기1library(caret) # 머신러닝을 위한 패키지 1## Loading required package: ggplot2 1## Loading required package: lattice 1library(tidyverse) # 데이터 핸들링 및 시각화를 위한 패키지 1## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── 1234## ✓ tibble 3.1.6 ✓ dplyr 1.0.7## ✓ tidyr 1.1.4 ✓ stringr 1.4.0## ✓ readr 2.1.0 ✓ forcats 0.5.1## ✓ purrr 0.3.4 1234## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──## x dplyr::filter() masks stats::filter()## x dplyr::lag() masks stats::lag()## x purrr::lift() masks caret::lift() 1library(doParallel) # 병렬처리를 위한 패키지 1## Loading required package: foreach 12## ## Attaching package: 'foreach' 123## The following objects are masked from 'package:purrr':## ## accumulate, when 1## Loading required package: iterators 1## Loading required package: parallel 병렬처리 주 목적: 속도 때문에 씀 원리 및 기타 설명은 다음 링크를 참고한다. https://freshrimpsushi.tistory.com/1266 1detectCores() # 현재 자기 컴퓨터의 코어 개수를 반환한다 1## [1] 8 병렬처리에 쓸 코어를 등록한다. 보통 50% 쓰는 것을 추천한다. (이유: 모형이 개발되는 동안 다른 간단한 작업도 해야 함) 12cl &lt;- parallel::makeCluster(6, setup_timeout = 0.5)registerDoParallel(cl) 2. 데이터 가져오기 경로를 확인한 뒤 데이터를 가져온다. 12loan_data &lt;- read.csv(&quot;data/cleaned_loan_data.csv&quot;, stringsAsFactors = FALSE)dim(loan_data) 1## [1] 29091 8 3. 데이터 전처리 경로를 확인한 뒤 데이터를 가져온다. 먼저 중복값을 확인한다. 1sapply(loan_data, function(x) sum(is.na(x))) 1234## loan_status loan_amnt grade home_ownership annual_inc ## 0 0 0 0 0 ## age emp_cat ir_cat ## 0 0 0 데이터 타입을 확인한다. 1loan_data %&gt;% duplicated() %&gt;% sum() # 374개 확인 1## [1] 374 1loan_data2 &lt;- loan_data %&gt;% distinct() 데이터 타입을 확인한다. 1glimpse(loan_data2) 12345678910## Rows: 28,717## Columns: 8## $ loan_status &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0…## $ loan_amnt &lt;int&gt; 5000, 2400, 10000, 5000, 3000, 12000, 9000, 3000, 10000…## $ grade &lt;chr&gt; &quot;B&quot;, &quot;C&quot;, &quot;C&quot;, &quot;A&quot;, &quot;E&quot;, &quot;B&quot;, &quot;C&quot;, &quot;B&quot;, &quot;B&quot;, &quot;D&quot;, &quot;C&quot;, …## $ home_ownership &lt;chr&gt; &quot;RENT&quot;, &quot;RENT&quot;, &quot;RENT&quot;, &quot;RENT&quot;, &quot;RENT&quot;, &quot;OWN&quot;, &quot;RENT&quot;, …## $ annual_inc &lt;dbl&gt; 24000.00, 12252.00, 49200.00, 36000.00, 48000.00, 75000…## $ age &lt;int&gt; 33, 31, 24, 39, 24, 28, 22, 22, 28, 22, 23, 27, 30, 24,…## $ emp_cat &lt;chr&gt; &quot;0-15&quot;, &quot;15-30&quot;, &quot;0-15&quot;, &quot;0-15&quot;, &quot;0-15&quot;, &quot;0-15&quot;, &quot;0-15&quot;…## $ ir_cat &lt;chr&gt; &quot;8-11&quot;, &quot;Missing&quot;, &quot;11-13.5&quot;, &quot;Missing&quot;, &quot;Missing&quot;, &quot;11… 우선 타겟 데이터는 영어로 표현한다. 123loan_data2$loan_status &lt;- factor(loan_data2$loan_status, levels = c(0, 1), labels = c(&quot;non_default&quot;, &quot;default&quot;))loan_data2$grade &lt;- as.factor(loan_data2$grade)loan_data2$home_ownership &lt;- as.factor(loan_data2$home_ownership) 만약 한꺼번에 하고 싶다면 다음과 같이 할 수 있다. 12loan_data2 &lt;- loan_data2 %&gt;% mutate_if(is.character, as.factor) chr 데이터가 모두 factor로 바뀌었는지 확인한다. 1glimpse(loan_data2) 12345678910## Rows: 28,717## Columns: 8## $ loan_status &lt;fct&gt; non_default, non_default, non_default, non_default, non…## $ loan_amnt &lt;int&gt; 5000, 2400, 10000, 5000, 3000, 12000, 9000, 3000, 10000…## $ grade &lt;fct&gt; B, C, C, A, E, B, C, B, B, D, C, A, B, A, B, B, B, B, B…## $ home_ownership &lt;fct&gt; RENT, RENT, RENT, RENT, RENT, OWN, RENT, RENT, RENT, RE…## $ annual_inc &lt;dbl&gt; 24000.00, 12252.00, 49200.00, 36000.00, 48000.00, 75000…## $ age &lt;int&gt; 33, 31, 24, 39, 24, 28, 22, 22, 28, 22, 23, 27, 30, 24,…## $ emp_cat &lt;fct&gt; 0-15, 15-30, 0-15, 0-15, 0-15, 0-15, 0-15, 0-15, 0-15, …## $ ir_cat &lt;fct&gt; 8-11, Missing, 11-13.5, Missing, Missing, 11-13.5, 11-1… 4. 데이터 분리 훈련 데이터와 테스트 데이터로 분리한다. 1234set.seed(2021)inx &lt;- createDataPartition(loan_data2$loan_status, p = 0.6, list = F)train &lt;- loan_data2[ inx, ]test &lt;- loan_data2[-inx, ] 5. 모형 개발 준비 caret 패키지에서의 모형 개발 관련해서는 다음 웹사이트에서 확인 하기를 바란다. Ref. http://appliedpredictivemodeling.com/ (1) Controller trainControl 함수를 활용하여 기본 세팅을 진행한다. 123456control &lt;- trainControl( method = &quot;repeatedcv&quot;, number = 10, # 10겹 repeats = 3, # 3번 search = &quot;grid&quot;, classProbs = TRUE) (2) Feature Engineering 통계처리를 진행한다. 123456preProc &lt;- c(&quot;BoxCox&quot;, &quot;center&quot;, &quot;scale&quot;, &quot;spatialSign&quot;, &quot;corr&quot;, &quot;zv&quot;) (3) 독립 변수와 종속 변수의 정의 독립변수와 종속 변수를 정의한다. 1frml &lt;- loan_status ~ loan_amnt + grade + home_ownership + annual_inc + age + emp_cat + ir_cat 6. 모형개발 개발준비가 끝났다면, 다양한 모델을 개발하도록 한다. (1) 로지스틱회귀분석12345678910logis &lt;- train( frml, data = train, method = &quot;glm&quot;, metric = &quot;Accuracy&quot;, trControl = control, preProcess = preProc)logis 1234567891011121314## Generalized Linear Model ## ## 17231 samples## 7 predictor## 2 classes: 'non_default', 'default' ## ## Pre-processing: Box-Cox transformation (3), centered (20), scaled (20),## spatial sign transformation (20) ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 15509, 15508, 15508, 15507, 15508, 15507, ... ## Resampling results:## ## Accuracy Kappa ## 0.8878377 -0.0004200657 (2) 의사결정나무 의사결정 나무에서 하이퍼파라미터를 정의한다. 12rpartGrid &lt;- expand.grid(cp = c(0.001, 0.005, 0.01))modelLookup(&quot;rpart&quot;) 12## model parameter label forReg forClass probModel## 1 rpart cp Complexity Parameter TRUE TRUE TRUE 이제 모형을 개발한다. 1234567891011set.seed(2021)rpt &lt;- train( frml, data = train, method = &quot;rpart&quot;, metric = &quot;Accuracy&quot;, trControl = control, preProcess = preProc, tuneGrid = rpartGrid)rpt 12345678910111213141516171819## CART ## ## 17231 samples## 7 predictor## 2 classes: 'non_default', 'default' ## ## Pre-processing: Box-Cox transformation (3), centered (20), scaled (20),## spatial sign transformation (20) ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 15508, 15507, 15508, 15508, 15508, 15508, ... ## Resampling results across tuning parameters:## ## cp Accuracy Kappa ## 0.001 0.8872189 0.008546392## 0.005 0.8880506 0.000000000## 0.010 0.8880506 0.000000000## ## Accuracy was used to select the optimal model using the largest value.## The final value used for the model was cp = 0.01. 1ggplot(rpt) (3) 랜덤포레스트 이번에는 랜덤포레스트를 사용하기 위한 하이퍼파라미터를 정의한다. 12rfGrid &lt;- expand.grid(mtry = c(3, 4, 5))modelLookup(&quot;rf&quot;) 12## model parameter label forReg forClass probModel## 1 rf mtry #Randomly Selected Predictors TRUE TRUE TRUE 랜덤포레스트 모델을 개발한다. 1234567891011rf &lt;- train( frml, data = train, method = &quot;rf&quot;, metric = &quot;Accuracy&quot;, trControl = control, preProcess = preProc, tuneGrid = rfGrid)rf 12345678910111213141516171819## Random Forest ## ## 17231 samples## 7 predictor## 2 classes: 'non_default', 'default' ## ## Pre-processing: Box-Cox transformation (3), centered (20), scaled (20),## spatial sign transformation (20) ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 15508, 15508, 15507, 15508, 15509, 15508, ... ## Resampling results across tuning parameters:## ## mtry Accuracy Kappa ## 3 0.8768692 0.005206944## 4 0.8774109 0.005965678## 5 0.8777978 0.008642398## ## Accuracy was used to select the optimal model using the largest value.## The final value used for the model was mtry = 5. 1ggplot(rf) 7. 모형 Resampling 3개의 모형을 비교하도록 한다. 123456resamps &lt;- resamples( list(glm = logis, rpt = rpt, rf = rf))summary(resamps) 12345678910111213141516171819202122## ## Call:## summary.resamples(object = resamps)## ## Models: glm, rpt, rf ## Number of resamples: 30 ## ## Accuracy ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA's## glm 0.8861789 0.8879861 0.8879861 0.8878377 0.8879861 0.8885017 0## rpt 0.8879861 0.8879861 0.8879861 0.8880506 0.8879861 0.8885665 0## rf 0.8723157 0.8758701 0.8772134 0.8777978 0.8797156 0.8839234 0## ## Kappa ## Min. 1st Qu. Median Mean 3rd Qu. Max.## glm -0.004571755 0.000000000 0.0000000 -0.0004200657 0.00000000 0.00000000## rpt 0.000000000 0.000000000 0.0000000 0.0000000000 0.00000000 0.00000000## rf -0.024423256 -0.003628695 0.0129503 0.0086423981 0.01672265 0.03835002## NA's## glm 0## rpt 0## rf 0 1bwplot(resamps, layout = c(2, 1)) 8. 최종모형 선정 및 모형평가(1) Confusion Matrix1234pred_rpt &lt;- predict(rf, test, type = &quot;prob&quot;)pred_rpt$loan_status &lt;- ifelse(pred_rpt$non_default &gt; 0.85, 0, 1) # cut-off를 조정하며 맞춰보자pred_rpt$loan_status &lt;- factor(pred_rpt$loan_status, levels = c(0, 1), labels = c(&quot;non_default&quot;, &quot;default&quot;))confusionMatrix(pred_rpt$loan_status, test$loan_status, positive = &quot;non_default&quot;) 123456789101112131415161718192021222324252627## Confusion Matrix and Statistics## ## Reference## Prediction non_default default## non_default 7428 787## default 2772 499## ## Accuracy : 0.6901 ## 95% CI : (0.6816, 0.6986)## No Information Rate : 0.888 ## P-Value [Acc &gt; NIR] : 1 ## ## Kappa : 0.0694 ## ## Mcnemar's Test P-Value : &lt;2e-16 ## ## Sensitivity : 0.7282 ## Specificity : 0.3880 ## Pos Pred Value : 0.9042 ## Neg Pred Value : 0.1526 ## Prevalence : 0.8880 ## Detection Rate : 0.6467 ## Detection Prevalence : 0.7152 ## Balanced Accuracy : 0.5581 ## ## 'Positive' Class : non_default ## (2) ROC Curve &amp; AUC 이번에는 ROC Curve와 AUC를 계산하도록 한다. 12345library(ROCR)pr &lt;- prediction(as.numeric(pred_rpt$loan_status) - 1, as.numeric(test$loan_status) - 1)prf &lt;- performance(pr, measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;)plot(prf, col = &quot;blue&quot;)abline(a = 0, b = 1) 1234# AUC = Area Under Curve의 뜻으로auc &lt;- performance(pr, measure = &quot;auc&quot;)auc &lt;- auc@y.values[[1]]; auc 1## [1] 0.5581301","link":"/2021/11/29/R/loan_classification/"},{"title":"name &amp; namespace","text":"##Name name : identifier, variable, reference object의 구분을 위해 사용 object object는 value, type, 주소 의 3원소를 가지고 있다. Ex) 12345a = 10b = 0.12print(a + b) Memory에 저장된 형태 object(10, int, ref_cnt=1)0x100 = 100번째 저장된(주소) object는 10(value), int(type)의 속성을 갖는다. ** 하나의 object는 여러개의 이름(name or ref.)를 가질 수 있다. ** ref_cnt는 reference count를 의미한다. name binding- assignment - import - class 정의 - function 정의 ... assignment dictionary (key : value) name binding (name : address) 객체(object)에 이름과 주소를 할당 이름 작성 규칙 하나의 단어로 작성, 문자(한글), 숫자, _ 가능(띄워쓰기 불가능) Keyword(+ 다른곳의 함수 이름)은 안쓰는 것이 좋고 쓰지만자 숫자로 시작할 수 없으며, 대소문자 구분가능 _로 시작하는 이름은 class에서 쓰이므로 지양 name space namespace는 이름 관리를 위한 dict container이다. 모든 class, instance, fuction은 자신의 namespace를 가지고 있다. built-in namespacepython에서 제공하는 함수, class, instance 등이 들어있는 곳. Built-in Functions global name space내가 만든 함수, 인수, class들이 들어 있는 곳 12345a = 100b = ac = aa = 101print(a, b, c) ◎ namespace (global) a : 0x100 b : 0x100 c : 0x100 ◎ in memory object(100, int ref_cnt=3) 0x100 object(101, int ref_cnt=1) 0x200 그렇다면, 왜 ref_cnt 를 하는 것일까 ?? 사용되지 않는 객체를 삭제 하기 위해 하나의 객체가 여러개의 이름을 가질 수있다. 하지만, 하나의 이름은 하나의 객체만 가질 수 있다. In 1234567891011import sysa= &quot;Python&quot;b= ac= a a= &quot;python&quot;print(f'a=[a], b=[b], c=[c]')sys.getrefcount(a)sys.getrefcount(b)sys.getrefcount(c) Out a=[a], b=[b], c=[c] 4 5 5 키워드pprint 안의 함수 pprint를 사용하여 키워드 리스트를 출력해 본다. in 1234import keywordimport pprint as pppp.pprint(keyword.kwlist) out [‘False’, ‘None’, ‘True’, ‘_peg_parser_‘, ‘and’, ‘as’, ‘assert’, ‘async’, ‘await’, ‘break’, ‘class’, ‘continue’, ‘def’, ‘del’, ‘elif’, ‘else’, ‘except’, ‘finally’, ‘for’, ‘from’, ‘global’, ‘if’, ‘import’, ‘in’, ‘is’, ‘lambda’, ‘nonlocal’, ‘not’, ‘or’, ‘pass’, ‘raise’, ‘return’, ‘try’, ‘while’, ‘with’, ‘yield’] assignment assignment는 Expression이 아닌 statment이다. Expression은 한개의 객체로 Evaluate될 수 있는것 이름에 binding할 수 있다. (syntax에서 사용 할 수 있는 위치가 다르기 때문) ver 3.8~ assignment expression (:=)이 추가되어 제공 assignment의 종류 종류 기호 ex exp assignment = a = 10 10에 a 를 바인딩 assignmented assignment **=, +=, -=, *=, //=, %=. &lt;&lt;=, &gt;&gt;=, &amp;=, |=, ^=, @= a+= 10 a에 10을 더한 결과 객체에 a를 바인딩 덧셈의 ref_cnt123456print(&quot;**********&quot;)i = 10print(id(i))i += 1print(id(i))print(&quot;**********&quot;) i가 10이때, i += 1 을 하면 11이라는 것이 만들어 진다. 이 11은 새로운 객체이다. 새로운 객체가 생성 되기 때문에 id가 달라진다. (memory의 adress) pack &amp; unPack pack : (,) 콤마를 이용하여 Tuple 객체 하나 생성 unpack : 1개의 묶음에 있는 여러개의 객체 아이템이 분리되어 각각의 이름에 바인딩 됨. 이와 같은 pack과 unpack을 이용 하면, 여러 이름에 여러 값을 부여하기 쉽다. in 123456789101112131415data_int = 1data_Tuple = 1,data = 10, 20, 30first, second, third = dataprint(first, second, third)def function(): a = 10 b = 20 print(locals()) del b print(locals())function() Out 10 20 30 {‘a’: 10, ‘b’: 20} {‘a’: 10} del (변수) : python에 있는 좋은 기능 변수를 삭제 할 수 있다. locals() : local안에 있는 변수를 확인 할 수 있다. type() : 변수의 data type을 확인 할 수 있다. Ref. youtube, 1hr","link":"/2021/12/06/python/nameNnamespace/"},{"title":"Python Class and function","text":"day 1 Lecture 1. 새로운 Project 시작 python project를 새로 시작 해 보자. pycham을 실행하여 project file을 만들어도 되지만, 원하는 directory에 file 을 만들고, 그 file에서 pycharm을 실행 해도 된다. 2. main.py :stop point 12345678910111213# This is a sample Python script.# Press Shift+F10 to execute it or replace it with your code.# Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.# 인공지능 / 머신러닝 --&gt; 리서치 관점 (논문리뷰, 정리, End User) / Engineer 관점def print_hi(name): # Use a breakpoint in the code line below to debug your script. print(f'Hi, {name}') # Press Ctrl+F8 to toggle the breakpoint.# f string# Press the green button in the gutter to run the script. BreakPoint def name(name): 함수 정의 하기1234567def print_hi(name): print(f'Hi, {name}')if __name__ == '__main__': print_hi('PyCharm')print(&quot;hi&quot;, name) Ctrl + shift + F10 : Run def 로 함수 정의 이름(print_hi), 인수(name)을 설정 : 원하는 동작(print(f’Hi, {name})) 넣기 if 3. Import.123456numpy == 1.21.4pandas ==1.3.4seaborn==0.11.2matplotlib==3.5.0scipy ==1.7.3scikit-learn == 1.0.1","link":"/2021/12/06/python/pythonLectureD1/"},{"title":"MarkDown Table, hr, &#96;code&#96;","text":"MarkDown Table마크다운 문법에서 테이블 그리기 pipe로 table을 쉽게 생성 할 수 있다. 아래 표는table ref.에서 가져온 것. 123456| 종류 | 기호 | ex | exp ||:-----------:|:-------------:|:-------------:|:------:|| assignment | = | a = 10 | 10에 a 를 바인딩 || assignmented assignment | **=, +=, -=, *=, //=, %=. &lt;&lt;=, &gt;&gt;=, &amp;=, &amp;#124;=, ^=, @= | a+= 10 |a에 10을 더한 결과 객체에 a를 바인딩 | 대충 칸에 맞게 순서만 맞으면아래와 같이 예쁜 표를 만들 수 있다. 종류 기호 ex exp assignment = a = 10 10에 a 를 바인딩 assignmented assignment **=, +=, -=, *=, //=, %=. &lt;&lt;=, &gt;&gt;=, &amp;=, |=, ^=, @= a+= 10 a에 10을 더한 결과 객체에 a를 바인딩 table 안에 정렬1234&lt;!-- 하이픈 갯수로 크기 조절 --&gt;|왼쪽정렬 | 중간정렬|오른쪽정렬||:---------|:---------------:|---------:| 왼쪽정렬 중간정렬 오른쪽정렬 왼쪽정렬 중간정렬 오른쪽정렬 pipe 보이기pipe는 Enrer Key 위에 있다. 1&amp;#124 하지만, Markdown형식에서 보이지 않으닌까 \\ | 만약 사용 하고 싶으면, &amp;#124 를 사용 하도록 한다. ^0^ table만드는 것이 귀찮다면 아래 링크를 타고 가보쟈 table 쉽게 만드는 곳 code 강조하기inline code 강조하는 방법 1234&lt;!--Tab key 위쪽, 1 왼쪽에 위치한 Grave 키 이용--&gt;`code` code 이것이 가능하다니, 이제야 알았다. !!! 수평선12345678---(hyphen X 3)***(asterisk X 3)___(Underscore X 3) (hyphen X 3) (asterisk X 3) (Underscore X 3) Ref.","link":"/2021/12/06/Markdown/CreateTable/"}],"tags":[{"name":"python","slug":"python","link":"/tags/python/"},{"name":"bioinformatics","slug":"bioinformatics","link":"/tags/bioinformatics/"},{"name":"BioDataScientist","slug":"BioDataScientist","link":"/tags/BioDataScientist/"},{"name":"BioData_Science","slug":"BioData-Science","link":"/tags/BioData-Science/"},{"name":"google, colaboratory, github, upload","slug":"google-colaboratory-github-upload","link":"/tags/google-colaboratory-github-upload/"},{"name":"machineLearning","slug":"machineLearning","link":"/tags/machineLearning/"},{"name":"decisionTree","slug":"decisionTree","link":"/tags/decisionTree/"},{"name":"predictionModel","slug":"predictionModel","link":"/tags/predictionModel/"},{"name":"Classifier","slug":"Classifier","link":"/tags/Classifier/"},{"name":"DataScience","slug":"DataScience","link":"/tags/DataScience/"},{"name":"github","slug":"github","link":"/tags/github/"},{"name":"kaggle","slug":"kaggle","link":"/tags/kaggle/"},{"name":"summary","slug":"summary","link":"/tags/summary/"},{"name":"kaggle_dictation","slug":"kaggle-dictation","link":"/tags/kaggle-dictation/"},{"name":"Plotly","slug":"Plotly","link":"/tags/Plotly/"},{"name":"plot","slug":"plot","link":"/tags/plot/"},{"name":"BarGraph","slug":"BarGraph","link":"/tags/BarGraph/"},{"name":"HorizontalBar","slug":"HorizontalBar","link":"/tags/HorizontalBar/"},{"name":"Scatter","slug":"Scatter","link":"/tags/Scatter/"},{"name":"ScatterLine","slug":"ScatterLine","link":"/tags/ScatterLine/"},{"name":"Bargraph","slug":"Bargraph","link":"/tags/Bargraph/"},{"name":"Donut_Chart","slug":"Donut-Chart","link":"/tags/Donut-Chart/"},{"name":"makeBlog, makegithub","slug":"makeBlog-makegithub","link":"/tags/makeBlog-makegithub/"},{"name":"Study","slug":"Study","link":"/tags/Study/"},{"name":"Numpy","slug":"Numpy","link":"/tags/Numpy/"},{"name":"Treemap","slug":"Treemap","link":"/tags/Treemap/"},{"name":"Pandas","slug":"Pandas","link":"/tags/Pandas/"},{"name":"List","slug":"List","link":"/tags/List/"},{"name":"studyPython","slug":"studyPython","link":"/tags/studyPython/"},{"name":"Tuple","slug":"Tuple","link":"/tags/Tuple/"},{"name":"kaggleNote","slug":"kaggleNote","link":"/tags/kaggleNote/"},{"name":"googleColab","slug":"googleColab","link":"/tags/googleColab/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"Matplotlib","slug":"Matplotlib","link":"/tags/Matplotlib/"},{"name":"Seaborn","slug":"Seaborn","link":"/tags/Seaborn/"},{"name":"paperReview","slug":"paperReview","link":"/tags/paperReview/"},{"name":"title","slug":"title","link":"/tags/title/"},{"name":"coding","slug":"coding","link":"/tags/coding/"},{"name":"DataFrame","slug":"DataFrame","link":"/tags/DataFrame/"},{"name":"python_basic","slug":"python-basic","link":"/tags/python-basic/"},{"name":"method","slug":"method","link":"/tags/method/"},{"name":"repository","slug":"repository","link":"/tags/repository/"},{"name":"make","slug":"make","link":"/tags/make/"},{"name":"makeBlog","slug":"makeBlog","link":"/tags/makeBlog/"},{"name":"panel","slug":"panel","link":"/tags/panel/"},{"name":"Series","slug":"Series","link":"/tags/Series/"},{"name":"makegithub","slug":"makegithub","link":"/tags/makegithub/"},{"name":"draft","slug":"draft","link":"/tags/draft/"},{"name":"Subplots","slug":"Subplots","link":"/tags/Subplots/"},{"name":"Stacked_Bar","slug":"Stacked-Bar","link":"/tags/Stacked-Bar/"},{"name":"kaggle_Competition","slug":"kaggle-Competition","link":"/tags/kaggle-Competition/"},{"name":"R4ds","slug":"R4ds","link":"/tags/R4ds/"},{"name":"dataScience","slug":"dataScience","link":"/tags/dataScience/"},{"name":"ggplot2","slug":"ggplot2","link":"/tags/ggplot2/"},{"name":"pythonFuction","slug":"pythonFuction","link":"/tags/pythonFuction/"},{"name":"namespace","slug":"namespace","link":"/tags/namespace/"},{"name":"markdown","slug":"markdown","link":"/tags/markdown/"},{"name":"table","slug":"table","link":"/tags/table/"},{"name":"table arrange","slug":"table-arrange","link":"/tags/table-arrange/"},{"name":"pipe","slug":"pipe","link":"/tags/pipe/"}],"categories":[{"name":"BDS","slug":"BDS","link":"/categories/BDS/"},{"name":"DecisionTree","slug":"DecisionTree","link":"/categories/DecisionTree/"},{"name":"index","slug":"index","link":"/categories/index/"},{"name":"python - plotly","slug":"python-plotly","link":"/categories/python-plotly/"},{"name":"python","slug":"python","link":"/categories/python/"},{"name":"kaggle","slug":"kaggle","link":"/categories/kaggle/"},{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"init","slug":"init","link":"/categories/init/"},{"name":"Matplotlib","slug":"Python/Matplotlib","link":"/categories/Python/Matplotlib/"},{"name":"Seaborn","slug":"Python/Matplotlib/Seaborn","link":"/categories/Python/Matplotlib/Seaborn/"},{"name":"R","slug":"R","link":"/categories/R/"},{"name":"data_science","slug":"R/data-science","link":"/categories/R/data-science/"},{"name":"Markdown","slug":"Markdown","link":"/categories/Markdown/"}]}